\pdfoutput=1

\documentclass{l4proj}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{microtype}
\usepackage{enumerate}
\usepackage{url}
\usepackage{breakurl}
\usepackage[breaklinks]{hyperref}
\usepackage{fancyvrb}
\def\UrlBreaks{\do\/\do-}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage[dvipsnames]{xcolor}
\usepackage{listings}
\hypersetup{
colorlinks,
linkcolor={red!50!black},
citecolor={blue!50!black},
urlcolor={blue!80!black}
}

\newcommand\textttd[1]{\texttt{\detokenize{#1}}}
\newenvironment{halfmini}[1]
{
\begin{minipage}[t]{0.5\textwidth}
\noindent\textbf{\textit{#1}}\\
}
{
\end{minipage}
}

% Source listings
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{10} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{10}  % for normal
\definecolor{keyword}{HTML}{8C0D40}
\definecolor{name}{HTML}{000F55}
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},
keywordstyle=\ttb\color{keyword},
emph={__init__,ThingDoer,accept_foo_bar,THING_QUOTA,do_thing,Discombobulator,discombobulate,
Discombobulated,func},
emphstyle=\ttb\color{name},
stringstyle=\color{PineGreen},
rulecolor=\color{gray},
showstringspaces=false
}}
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}
\newcommand\javastyle{\lstset{
language=Java,
basicstyle=\ttm,
otherkeywords={},
keywordstyle=\ttb\color{keyword},
emph={method},
emphstyle=\ttb\color{name},
stringstyle=\color{PineGreen},
rulecolor=\color{gray},
showstringspaces=false
}}
\lstnewenvironment{java}[1][]
{
\javastyle
\lstset{#1}
}
{}
\newcommand\javainline[1]{{\javastyle\lstinline!#1!}}
\newcommand\clojurestyle{\lstset{
language=Lisp,
basicstyle=\ttm,
otherkeywords={defn,defn-,macroexpand,macroexpand-all},
keywordstyle=\ttb\color{keyword},
emph={lib.core,constant,func,get-nil},
emphstyle=\ttb\color{name},
stringstyle=\color{PineGreen},
rulecolor=\color{gray},
showstringspaces=false
}}
\lstnewenvironment{clojure}[1][]
{
\clojurestyle
\lstset{#1}
}
{}
\newcommand\clojureinline[1]{{\clojurestyle\lstinline!#1!}}
\newcommand\clojureinlinedef{{\clojurestyle\lstset{otherkeywords=def}\lstinline!def!}}
\newcommand\xmlstyle{\lstset{
language=XML,
basicstyle=\ttm,
otherkeywords={},
keywordstyle=\ttb\color{keyword},
emph={},
emphstyle=\ttb\color{name},
stringstyle=\color{PineGreen},
rulecolor=\color{gray},
showstringspaces=false
}}
\lstnewenvironment{xml}[1][]
{
\xmlstyle
\lstset{#1}
}
{}
\newcommand\xmlinline[1]{{\xmlstyle\lstinline!#1!}}

% Body

% TODO: inconsistent british/american spelling

\begin{document}
\title{Automatically proposing next semantic version of a project by \\
inspecting changes to the codebase}
\author{Christian Shtarkov}
\date{\today}
\maketitle

\begin{abstract}
In software development, assigning a version number
to each release is crucial. It allows customers and other developers
to identify that a software package has changed from a previous
iteration. It is especially important when considering libraries and
frameworks, where changes can greatly affect applications built on top
of them. \\ Different policies exist dictating how to assign version
numbers, one of the most popular being \textit{Semantic
Versioning}\cite{SemanticVersioning}, where the version gives
information about how exactly the software package has changed. This
allows for better estimation whether a new version of a library or a
framework would result in incompatibilities.
\\\\
Regardless of
policy, assigning version numbers is typically done manually and
leaves room for human mistakes or deviations from the established
scheme. We propose the tool \textit{Autobump} that automatically
inspects changes to a codebase and proposes the next version number
according to semantic versioning. This reduces friction when releasing
packages, and encourages more frequent, even fully automated,
releases.
\end{abstract}

\educationalconsent

\tableofcontents

%% Contents
\chapter{Introduction}
\pagenumbering{arabic}

In this chapter, we discuss the background of software versioning and
more specifically, semantic versioning of software. We also briefly
statement our overall aim for this project and outline the rest of the
document.

\section{Background}

\subsection{Software versioning}
Software versioning is the practice of assigning nominal or numerical
identifiers to subsequent iterations of a software package. Every
variant of a piece of software is expected to have a different
(however small the difference) set of functionality, and versioning
allows for finding out how exactly a piece of software is expected to
behave. In practice, all non-trivial software is versioned in one way
or another.

Versioning can have different levels of granularity. Revision control
systems, such as Git or Subversion, allow developers to introduce
small incremental changes to a codebase. After every change, the new
state of the codebase is given a unique identifier and made
recoverable. For example, Subversion assigns a single number to every
revision starting at 1 and increasing in chronological order. This
allows for many productivity-boosting features, such as reverting code
to a previous state, proving that a particular changeset is
responsible for the introduction of a bug, or identifying differences
between divergent branches of the same initial piece of code.

On a macro scale, releases of software packages are also versioned.
Because releases happen much less often than small changes, versions
also change much less often and follow different policies. The de
facto standard are sequence-based schemes where each version
identifier is a tuple consisting of numbers or letters separated by
dots -- e.g. \textit{1.3.2} or \textit{10.6-rc1}. The vast majority of
computer software follows a scheme of this type. It is usually implied
that the further to the left an element of the tuple is, the more
significant it is. What ``significant'' means is not well-defined and
depends on the kind of software that is being versioned. Comparing
versions \textit{5.0} and \textit{6.0} of a Web browser may imply that
the latter is more advanced and has new features, whereas comparing
the same versions of a library may imply that client code using the
former is incompatible with the latter. In general, incrementing
versions in this format follows these rules:
\begin{enumerate}[(i)]
\item Incrementing any non-final element ``resets'' all elements to
the right, i.e. bumping the middle element of \textit{3.5.4} results
in \textit{3.6.0}.
\item Incrementing a version which has a label attached at the end
necessitates either changing the label or droping it, i.e.
\textit{3.5.4-beta} $\rightarrow$ \textit{3.5.4} or \textit{3.5.4-rc1}
$\rightarrow $ \textit{3.5.4-rc2}.
\end{enumerate}

Note that software can be versioned by more than one scheme -- it is a
typical scenario that revision numbers are used by developers
internally and a sequence-based scheme is used for public releases.
Large consumer-facing software with long release cycles, like
operating systems, may use a codename or date-based scheme for
versioning public releases -- e.g. \textit{Ubuntu 14.04 (Trusty Tahr)}
which was released in April of 2014 happens to use both. Those
typically serve a marketing purpose rather than provide any technical
benefit.

\subsection{Semantic versioning}

In software development, it is a widespread practice to leverage
pre-existing code in the form of libraries or
frameworks\footnote{Those are the most common use cases for one piece
of software interfacing with another. The same argument also applies
to philosophically similar use cases, like invoking command-line tools
such as \textit{sed} or \textit{awk} from a script.} to avoid
duplication of effort. Given that client applications and the
libraries they depend on have separate development cycles, a problem
arises in determining whether a client application is compatible with
a different version of the same library. A common solution is to take
a sequence-based scheme and attach meaning to the positions in order
to measure degree of compatibility.

Semantic versioning\cite{SemanticVersioning} (Semver) is a strict
formalization of such a scheme, where the format and significance of
each position are well-defined:

\begin{center}
$\mathrm{Major.Minor.Patch}(-\mathrm{Prerelease})^?(+\mathrm{Build})^?$
\end{center}

\begin{itemize}
\item \textit{Major} increments when incompatible changes to the
public API have been made and it is possible for interfacing software
to no longer work.
\item \textit{Minor} increments when new features have been added in a
backwards-compatible manner.
\item \textit{Patch} increments when bugs have been fixed in a
backwards-compatible manner.
\item \textit{Prerelease} is an optional label indicating that the
version does not yet meet the compatibility requirements imposed by
the first three numbers.
\item \textit{Build} is another optional label indicating the
corresponding internal version numbers, e.g. the one used by the
revision control system.
\end{itemize}

\subsubsection{Benefits}

Semver is especially useful for software meant for use by other
software, such as libraries and frameworks. When a new version of a
library is released, someone developing an application based on that
library can quickly judge how the update affects their application.

If only the minor or patch numbers have changed, then the application
should work fine with the new version. That is because incrementing
just those numbers implies that there are no breaking changes. If,
however, the major number has increased, then further work on part of
the application developer may be needed to make it compatible again
with the library.

This scheme also helps with automatic dependency management systems,
like ones found in package managers. A developer can specify that
their application relies on version \textit{2.4} of a particular
library. Semver guarantees that versions of the form \textit{(2.x | x
> 4)} will contain all features found in \textit{2.4} and be
backwards-compatible. If later on \textit{2.4.1} or \textit{2.5} come
along, the library can be automatically upgraded without breaking the
application, with the added benefit of fixing bugs.

In summary, the most important benefit provided by Semver is the
alleviation of dependency hell through unambiguous identification of
breaking changes. The major version number is increased if and only if
backwards compatibility is deprecated.

\subsubsection{Application}

Semver only provides these benefits when it is applied consistently.
If applied haphazardly, then two versions cannot be compared
reliably and decisions made by simply examining version numbers are
risky. A good example of that is the programming language PHP.
Although minor releases are usually meant for feature additions, it
can happen that they contain breaking changes along with that.
One the many examples are the changes introduced in PHP 5.3
compared to PHP 5.2\cite{PHPChangelog} where the signature of an array
of functions changed -- constituting a breaking change. The PHP
changelogs routinely document what backwards-incompatible changes are
in a release, but using an inconsistent versioning scheme negates
most benefits.

In practice, semantic versioning is applied poorly. A
study\cite{SemverMaven} of more than 22,000 libraries in the Maven
repository found that there was virtually no difference in the
frequency of breaking changes in major and minor version increments.
It also found that this has a significant impact on client
applications, mostly by causing compilation errors.

The study does not go into determining what the reasons are for these
inconsistencies, but some possibilities are:
\begin{itemize}
\item Some libraries never claimed to follow Semver, instead opting
for a similar, yet not identical, scheme.
\item Semver is very strict about what's considered a breaking change.
This is great for robots, but not so much for humans. Small changes to
the API may not be considered ``major'' by the developers even if they
are technically breaking. Developers who introduce breaking changes
frequently into their code may be reluctant to bump the major version
number, as it would skyrocket. This in itself is not necessarily a
problem, but it's important to realise that this will happen not
because of the versioning scheme, but because of that development
methodology.
\item Misinterpretation of Semver about what constitutes a breaking
change. Some developers may be inclined to believe that an API change
that affects none of their users (to their knowledge) is not breaking.
\end{itemize}

\section{Aim}

We have identified the reluctance or inconsisteny of applying a strict
versioning scheme like Semver as a problem in software engineering
that costs development time and disallows automated systems like
package managers from making decisions about software upgrades.

In this project we evaluate the automated assignment of version
numbers as a possible solution. Our goal is to develop a tool that
given a previous iteration of a project and its version, can
automatically propose the version of the current iteration.

\clearpage
% TODO: remove this if adding text above
\section{Outline}

The rest of this dissertation is comprised of the following chapters:

% TODO: chapters go here
\noindent Chapter \ref{PreviousWork} - \textbf{Previous work} \\
Goes into existing projects that to some extent solve the problem
of automatically assigning version numbers and the approaches that
they use.

\noindent Chapter \ref{Requirements} - \textbf{Requirements} \\
Lays out the approach that Autobump is going to use for
determining version numbers and the set of features that would make
the tool useful.

\noindent Chapter \ref{Implementation} - \textbf{Implementation} \\
Discusses how Autobump works in great detail, both internally and
in terms of being exposed to the end user.
% TODO: rewrite?

\noindent Chapter \ref{Testing} - \textbf{Testing} \\
Goes into the techniques used to show that Autobump indeed works
as advertised.

\noindent Chapter \ref{Evaluation} - \textbf{Evaluation} \\
Evaluates Autobump against real-world libraries written in
supported languages. Discrepancies between the version that the tool
proposes and one set manually by the developers are examined closely,
to evaluate both the tool itself and the culture of that project or language.

\noindent Chapter \ref{Conclusion} - \textbf{Conclusion} \\
Summarizes what was accomplished by this project.

\chapter{Previous work}
\label{PreviousWork}

Automatically assigning version numbers is not a new idea and projects
exist that already do that to some extent. In this chapter we discuss
existing tools that claim to do that. We also examine the various
ways they approach it, why they are not as successful as we would
expect and draw ideas for how we may go about implementing our own tool.
% TODO: do we discuss why they aren't successful?

\section{semantic-release}

\textit{semantic-release}\cite{SemanticRelease} is a Javascript tool
for ``fully automated package publishing''. It can do a range of
functions related to automated publishing of packages in the
Javascript ecosystem through the Node Package Manager, but of interest
are the two ways it can deal with proposing a new version number.

One way is that it tries to interpret Git commit messages and from
that determines what the next version number should be. A minor
(feature) release for example would be indicated by a commit message
of the following format:

\begin{center}
\texttt{feat(pencil): add 'graphiteWidth' option}
\end{center}

This approach suffers from the same drawback as incrementing the
version number manually does -- it has to be done consistently.

Another way is by employing an additional tool called
\textit{cracks}\cite{Cracks}. According to the official description
``this module can automatically detect breaking changes by running the
test suite of your last-release against the current codebase. This
shouldn't fail''. This is generally a good approach -- by definition
the test suite of the previous version should pass if the new one is
not meant to be a major release. However, it raises some issues.

\begin{itemize}
\item It could be that the library in question has whitebox unit
tests. If they start failing, that does not necessarily constitute a
breaking change, because while the internal implementation of those
methods may have changed, the public API is still the same. Cracks has
no way of differentiating blackbox from whitebox tests unless manually
specified.
\item The unit tests may not cover the public API in its entirety.
Modifying the signature of a public method, for example, that's not
covered by the tests may go completely unnoticed.
\item The library may be in a stage of development such that it has no
unit tests, or they cover a very small portion of the public API, yet
the authors still want to use semantic versioning.
\end{itemize}

\section{GitVersion}

\textit{GitVersion}\cite{GitVersion} ``looks at your Git history and
works out the semantic version of the commit being built''.  By
knowing in advance what workflow contributors to the repository are
using, it can determine when breaking changes occur, when new features
are being introduced and when it's only patches that are going
through. Previous versions of GitVersion have implicit knowledge of
the workflows \textit{GitFlow}\cite{GitFlow} and
\textit{GitHubFlow}\cite{GitHubFlow}, but the current one (3.0) at the
time of writing can be configured to work with more custom workflows.

Consider an example usage of GitVersion on a history of commits.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/dot/gitversionexample}
\caption{Example Git commit history}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\textbf{Commit} & \textbf{Version} & \textbf{Explanation}                              \\
0      & 1.0.0                 & First commit that has a version.                      \\
1      & 1.0.1+1               & Patch was introduced, and one commit since then.      \\
2      & 1.0.1-feature-foo.1+1 & Feature was introduced, and one commit since then.    \\
3      & 1.0.1+3               & Three commits since the last patch.                   \\
4      & 1.1.0-rc.1            & First release candidate for the new feature.          \\
5      & 1.1.0                 & Feature branch was merged.                            \\
6      & 1.1.1+1               & Patch was introduced, and one commit since then.      \\
7      & 1.1.1+2               & Patch was introduced, and two commits since then.     \\
8      & 2.0.0-rc.1            & First release candidate for the new breaking changes. \\
9      & 2.0.0                 & Breaking release branch was merged.
\end{tabular}
\caption{Proposed semantic version numbers by GitVersion, if every
commit were to be released}
\end{table}

Notice that similarly to semantic-release, GitVersion relies on the
naming of the branches. This results in the same set of drawbacks. It
has to be applied consistently, and forces contributors to use one
particular naming scheme. The latter one is somewhat alleviated in
this case, as the workflow GitVersion expects is highly
configurable\cite{GitVersionConfiguration}.

\section{Endjin.Assembly.ChangeDetection}

\textit{Endjin.Assembly.ChangeDetection}\cite{Endjin} is a
proof-of-concept project that takes a very different and less
error-prone approach. It can identify breaking changes in .NET
assemblies by looking at the assembly itself. It does this
by examining any changes made to public types in the assembly, such as
removing them or changing their signature.

This way of proposing a version number truly reflects changes to
the code and imposes no additonal requirements on contributors to
the repository. However, Endjin.Assembly.ChangeDetection is not really
a tool, but an experiment, and is tied to the .NET ecosystem.

\section{elm-package}

\section{SpiseMesu.SemanticVersioning}

% TODO: This should be called requirements

\chapter{Requirements}
\label{Requirements}

In this chapter we formalise the approach that the tool for this
project, Autobump, should use to propose version numbers, and
formulate the set of features that should be expected. We also briefly
discuss some non-functional requirements.

\section{Determining next semantic version through code analysis}

Much like Endjin.Assembly.ChangeDetection, Autobump to look at how the
code itself has changed to estimate what the next version should be.
This requires no manual intervention from the user, no special naming
schemes, and is as reliable as this kind of software can be.

% TODO: Determining vs Estimation?

\subsection{Breaking changes}
\label{BreakingChanges}

For the definition of breaking change to be useful, it ought to be
strict -- and that is the way Semver approaches it. It defines
\textbf{any} change to the API which makes it incompatible\footnote{By
``incompatible'', we mean changes that make the new code technically
incompatible with the old, i.e. it doesn't compile in the case of
static languages, or throws unexpected runtime errors with dynamic
languages.} with the last version as breaking. It does not take into
account number of affected users, difficulty of transitioning to the
new version, or changes in behaviour, because those things cannot be
possibly known reliably.

In the spirit of Semver, for this project we adopted a technical and
deterministic list of changes to entities that are considered
to be breaking. We make the assumption that all public-facing entities
(i.e. those accessible externally in a normal for the language way)
are part of the API.

\begin{table}[H]
\centering
\begin{tabular}{|p{0.45\linewidth}|p{0.45\linewidth}|}
\hline
\textbf{Change}                                                                               & \textbf{Rationalisation}                                                 \\
\hline
Removing an entity.                                                                           & User applications may be using it.                                       \\
\hline
Changing the type of an entity to an incompatible type.                                       & User applications may be expecting a different type.                     \\
\hline
Adding a parameter with no default value to a function signature.                             & Calls to that function will immediately fail due to a missing parameter. \\
\hline
Removing a parameter from a function signature, regardless of whether it had a default value. & Calls to that function \textit{may} fail due to a missing parameter.     \\
\hline
Removing the default value of a parameter.                                                    & Calls to that function may be relying on the now missing default value.  \\
\hline
\end{tabular}
\caption{List of changes that are considered breaking}
\end{table}

Note that the list is sufficiently abstract to accommodate for
differences in widely used languages. It makes only three assumptions
about the structure of the language.

\begin{enumerate}
\item The language provides some mechanism of grouping functionality
into entities, be it object-orientation, namespacing, modules and so
on. Note that groupings are absolutely allowed to be nested --
consider a Java library which defines a constant inside a class inside
a class. In this case the constant and both classes are considered to
be ``entities''.
\item The language has some notion of parameterized callable pieces of
code, which for simplicity's sake we refer to only as ``functions'',
even though in reality they may be object methods, macros etc.
Additionally, some of those functions are callable from outside
of the program where they are defined, effectively being an API.
\label{SecondAssumption}
\item The language has a type system which has the notion of binary
compatibility:
\begin{equation}
\forall(T,Y) \in S, \mathrm{compatible}(T,Y) \in \{\mathrm{True},\mathrm{False}\}
\end{equation}
where $S$ is the set of all admissable types in the language and
program. \\

We define compatibility as $T$ being able to replace $Y$
without violating the rules of the language -- e.g. the Liskov
substitution principle\cite{Liskov} in a typical object-oriented
language.
\end{enumerate}

There are languages that do not conform at all to these assumptions
(e.g. minimalistic ones such as Brainfuck\cite{Brainfuck}), but these
are also languages where a tool like Autobump would be utterly
unuseful. There are also languages that partially conform. In those
cases, as long as assumption \ref{SecondAssumption} is valid (i.e.
there is an API), we can ignore the rest -- of course making obsolete
some functionality of Autobump\footnote{Namely identifying in which
parts of the program the changes have occurred and checking type
compatibility.}. For this project, we examined the radically
different languages Python, Java and Clojure\ref{LanguageHandlers} and
the assumptions were found to be reasonable.

\subsection{Feature additions}

Semver defines feature additions as backwards-compatible changes to
the API. This means that changes which colloquially may be thought of
new features, such as making the library run faster or handling more
edge cases are considered patch changes by Semver.

Going by the list of breaking changes and replacing its destructive
aspect with a constructive one, we get the following list:

\begin{itemize}
\item Adding a new entity.
\item Changing the type of an entity into a compatible type.
\item Adding a parameter with a default value to a function signature.
\end{itemize}

\section{Feature set and usage}

% TODO: check tense usage

Autobump ought to be a a tool which given two revisions in a version
controlled project reliably determines what the version of the later
revision should be\footnote{In some contexts, ``version'' and
``revision'' may be identical. In this dissertation, we refer to the
semantic version of a release as ``version'', and the commit
identifier in a VCS as ``revision''}.

Although technically Autobump could be used with any project, we are
going to be focused at libraries, frameworks and otherwise projects
whose main usefulness comes from the ability to be integrated in
client applications by exposing an API. It is with those types of
projects that semantic versioning is most practical.

Because identifying the version delta requires first identifying a
list of changes to the public API, a secondary function of Autobump
can be the generation of a changelog in preparation for a release.

Autobump can be part of a manual or automated workflow of bumping the
version number with every release, generating a changelog, or both. It
is a command-line application with behaviour driven by passing
options, so it can be easily integrated in an automated deployment
system. Another possibility is using it standalone as a sanity check.
Developers can run it to find out if they have unintentionally broken
backwards compatibility with a recent commit.

If we rely on code analysis to find differences between iterations,
then it may seem necessary that Autobump itself is written in the same
language that we are trying to analyze. This is not necessarily so; we
propose an approach which none of the preexisting tools take --
program the tool itself and the comparison logic in a convenient
middleware language (for this project we use Python) and have handlers
for different languages as seperate modules.
% TODO: spellcheck

\chapter{Implementation}
\label{Implementation}

In this chapter, we look at how Autobump works internally in terms of
its architecture and how it exposes its functionality to the end
user. Furthermore, we focus on how implementation details differ for
the different target languages.

\section{Architecture}
\label{Architecture}

% TODO: Configuration

When designing the architecture of the tool, great care was taken to
separate unrelated concerns. Every part of the program lives in a
separate module, with dependencies (via \pythoninline{import}) reduced to a minimum.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{images/dot/architecture}
\caption{\textit{Autobump} architecture}
\end{figure}
In normal operation, the top-level program performs the
following steps:

\begin{enumerate}
\item Identify which VCS the repository is using and select the
appropriate handler.
% TODO: drop vcs handlers?
\item Identify which two revisions should be compared -- either from
command-line arguments or through an educated guess. An educated guess
consists of comparing the last tagged release of the project with the
latest commit. The version number of the previous release is
determined from the name of tag.
\item Invoke a language handler a total of two times for both
revisions to construct common\footnote{Common between different
source languages, not the two representations.} representations of the
codebase out of the public APIs. If the language handler needs
introspection, this may require building the two revisions of the project.
The nature of this common representation is discussed in \ref{CAPIR}
and the exact mechanism that language handlers use for the conversion
in \ref{LanguageHandlers}.
\item Run the core comparison logic on both representations to find
out how the public API has changed in the later one compared to the
earlier one. This step can generate a changelog and will identify what
the version bump should be. This is discussed in \ref{Logic}.
\item Output the new version number by applying the bump to the old
one. At this point, a changelog may be output as well if requested.
\end{enumerate}

\section{Common representation for APIs}
\label{CAPIR}

Before proceeding with the issue of representing APIs, it is important
to note that for the purposes of this project we are not interested in
any internal APIs software packages might use -- they should not
influence external applications that depend on the packages in question.
We are interested only in the public APIs which are meant for external
use, as changing these indicates an incrementation of the semantic version.

% TODO: public only in java
Most general-purpose languages provide ways to restrict visibility of
entities in the source code (such as classes, functions and so on).
Technically all externally visible entities, usually indicated with the keyword
\javainline{public}, are accessible from client applications. In practice,
however, it may be that not all of these form the intended, official
and documented public API. Faced with this problem, Autobump takes the
safer approach and only considers visibility when determining whether
an entity is part of the public API. The reason for that is that there
is nothing technically restricting client applications from using
visible entities that are otherwise undocumented or not formally
intended for outside use.

By relying on language-specific handlers to extract the public API of
a codebase and convert it into a common representation, the same
comparison logic can be reused. Beside reducing repetitiveness in the
code, this has the additional effect of allowing handlers for new
languages to be ``plugged in'' with virtually no changes to the
program itself.

Choosing an appropriate representation was an important, and
difficult, part of this project. The problem of representing an API
regardless of language is solved for some contexts by
\textit{interface descriptor languages} (IDLs), such as
WSDL\cite{WSDL}, SWIG\cite{SWIG} or Apache Thrift\cite{Thrift}. All of
these are meant for a specific purpose, usually interoperability between
different technologies. While adapting one of these to use in Autobump
was certainly possible, we opted to use our own model instead for
several reasons.

\begin{itemize}
\item Barely any features are needed except for API representation.
Using an established IDL may be considered overkill.
\item Using an established IDL would mean either importing an external
library as a dependency, or writing our own, and both are undesirable.
\item Should extensions be needed, it would be trivial to extend our
own model compared to a third-party one.
\item Autobump's representation of APIs is internal -- the user should
never care about it, so the choice of IDL can ignore any user
experience issues.
\end{itemize}

The representation that Autobump uses (fig. \ref{RepresentationUML})
consists of several entities that are common across public APIs
exposed by different languages. From now on, we will refer to this
model as \textbf{CAPIR} (standing for ``common API representation'') for the
sake of brevity.

\pythoninline{Entity} is a base class representing an entity of
interest found in source code. All deriving classes are guaranteed to
implement equality comparison to other entities (via
\pythoninline{__eq__} and \pythoninline{__ne__}).
% TODO: what are eq and ne?

\pythoninline{Type} is a base class that is the basis of the type
systems used by the language handlers. This is discussed in further
detail in \ref{TypeSystems}.

\pythoninline{Unit}s are a recursive data structure that indicate a
general grouping of entities. They can represent a range of grouping
techniques found in different languages -- translation units, modules,
classes, interfaces, mixins, traits etc. In fact, Autobump uses a
single top-level Unit to represent an entire codebase.

\pythoninline{Field}s and \pythoninline{Function}s are found in units. Fields have
a type and denote the setting of a variable or a constant. Functions
represent callable pieces of code -- functions, methods, routines, or
whatever the source language calls them.

Every function has one or more \pythoninline{Signature}s which in turn are
lists of \pythoninline{Parameter}s. Parameters have a type and can have a
default value.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/dot/repr}
\caption{Class diagram -- Autobump's public API representation (CAPIR)}
\label{RepresentationUML}
\end{figure}

% TODO: Expand on fields and parameters.

\clearpage
As an example of what CAPIR looks like in practice, fig.
\ref{RepresentationExample} shows the end result of transforming a
simple Python codebase consisting of two modules. Note how a tree is
formed akin to simplified abstract syntax tree -- this is instrumental
in allowing the comparison logic to be recursively defined later
(\ref{Logic}).

\begin{halfmini}{moduleA.py}
\begin{python}
def accept_foo_bar(foo, bar):
    pass

class ThingDoer(object):
    THING_QUOTA = 5

    def do_thing():
        pass
\end{python}
\end{halfmini}
\begin{halfmini}{moduleB.py}
\begin{python}
class Discombobulator(object):
    def discombobulate(thingimajig):
        raise self.Discombobulated()

    class Discombobulated(Exception):
        pass
\end{python}
\end{halfmini}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/dot/reprexample}
\caption{CAPIR of a codebase consisting of two modules}
\label{RepresentationExample}
\end{figure}
\clearpage

\section{Handlers for programming languages}
\label{LanguageHandlers}

In this section we discuss the overall idea behind how we go about
implementing support for multiple programming languages in the same
tool, and then move on to the specifics for each supported language.

The conversion of source code into CAPIR is delegated to separate
modules that expose a single function to the rest of the program:

\begin{center}
\begin{tabular}{c}
\begin{python}
def codebase_to_units(location):
    ...
\end{python}
\end{tabular}
\end{center}

This function takes in a location on disk where some version of a
repository is checked out, and returns a list of units that represent
every top-level entity found in the codebase -- e.g. for Python, that
would be individual \texttt{.py} files.

Generally, all current implementations of that function follow a
similar pattern in how they work:

\begin{enumerate}
\item Recursively walk the file system tree at \pythoninline{location} and
filter file names of interest with some regular expression -- e.g.
only \texttt{.py} files are considered, but not if they begin with \texttt{test-}.
\item For every file of interest, convert it to a CAPIR unit in some
way appropriate for the language.
\item Return the list of units.
\end{enumerate}

This workflow is not enforced and it is completely up to the handler
to decide how to implement the function and the CAPIR conversion.
Sometimes it is viable to have more than one handler per language --
see \ref{JavaHandlers}.

Autobump comes with handlers for three languages -- Python,
Java and Clojure. The rationale for picking those is that they are
different from one another by characteristics that matter when trying
to convert source code into the internal API representation. Having a
handler for a dynamically typed, interpreted language like Python
would make it easier to write one for a similar language, like Ruby,
in the future.

% TODO: Table of handlers, build required, so on
% TODO: what language are the handlers in?

\subsection{Transforming source code into CAPIR}

Each language handler, as part of its implementation, essentially has
to have a function that maps a source file to a CAPIR unit. In
general, there are two convenient ways to achieve this: through
introspection, and through inspecting the abstract syntax tree.
Introspection would usually yield better results, but it requires
first compiling the library. In dynamic languages, there is virtually no
difference between using introspection and the AST.

% TODO: Dynamic?
% TODO: Not true, what about method generation?

\subsection{Type systems}
\label{TypeSystems}

In \ref{BreakingChanges} we found that changing the type of a public
entity to a non-compatible one should be considered a breaking change.
However, what it means for two types to be compatible varies. Python,
Java and Clojure were chosen as candidate target languages for this
tool partly because of their different type systems.

In CAPIR, the only thing required of a \pythoninline{Type} is to be able to
identify whether it is compatible or not with another type. We are not
interested in whether or not they are the same type, or what those
types actually are\footnote{The point is that what the types actually
are is irrelevant to determining the next semantic version. It may be
of interest only to Autobump's secondary function, that is generating
changelogs}.

In the following sections on the individual language handlers, we
discuss how the three languages determine compatibility of types.

\subsection{Python}

Python is a general-purpose, dynamically typed, interpreted language.
That makes it an interesting target and a good first choice,
especially compounded with the fact that Python is a good middleware
language and Autobump itself is written in Python.
% TODO: is it though?

The Python handler uses the AST and is itself written in Python. It
takes advantage of Python's ``batteries included'' approach which
bundles all sorts of useful libraries along with the core Python
distribution, and that includes a Python parser. Converting to CAPIR
is a simple matter of opening each source file as text, parsing it and
then walking the resulting tree to produce the appropriate CAPIR
structures for each node.

Early iterations of the Python handler used introspection. It used to
import source files dynamically, which causes them to be compiled and
made available to the whole program. Then, the individual items in the
module were visited to generate their CAPIR counterparts. This led to
several problems. It's not guaranteed that Autobump runs in the same
environment as the library, so missing dependencies would halt the
whole process. But probably the biggest problem is that this approach
essentially evaluates the code it comes across -- after all, in order
to perform introspection on a function for example, a \pythoninline{def} has
to be evaluated first. If there is top-level code that has side
effects it will get executed along with the function definitions.

Python, as a dynamic language, can modify the structure of objects at
runtime by using functions such as \pythoninline{setattr}. This makes
it possible for a library to construct its functionality at runtime
instead of defining outright. This is fairly unlikely to happen in the
real-world libraries that Autobump would be useful for, so we just
discount that possibility.

\subsubsection{Types}

Python, by design and philosophy, encourages the use of duck typing.
When an operation is performed on an object, only the compatiblity of
that object with the operation is considered, not the actual type of
the object. This is a form of runtime structural typing (as opposed to
nominal typing) -- the structure of the object is what's important,
not what it is called.

Consider the following trivial example:

\begin{center}
\begin{tabular}{c}
\begin{python}
def operate(obj):
    obj.operation()
\end{python}
\end{tabular}
\end{center}

The contract that Python enforces is that \pythoninline{obj} has an
implementation of the method \pythoninline{operation}. It makes no
difference what the type of \pythoninline{obj} is -- the program will not
fail as long as the parameter passed into \pythoninline{operate} implements
\pythoninline{operation}.

If the only thing that matters are the accessed properties and methods
of an object to determine whether the program fails or not, this makes
the task of figuring out how the types of parameters have changed more
difficult. It would be unnecessary and even misleading to attempt to
look up usage of a function inside the project itself, for example in
tests. We may find that all parameters passed into the function are
instances of a particular class, but client code absolutely could pass
in objects of a different type that happen to have the same properties
and methods as needed by the function.

% TODO: What about types of entities? Need to implement AST scanning
% as well

A better choice is to consider two sets $S_a$ and $S_b$ -- the sets of
all properties and methods requested from the object in the two
versions we want to compare, $S_a$ being the earlier version chronologically.
If $S_a \supseteq S_b$, then we can safely claim that there was no
breaking change as far as the type of the object is concerned. After
all, if client code was passing an object successfully into the
earlier version of the function, then it should continue to do so
because either the set hasn't changed, or some of the expected
properties and methods are no longer expected.

However, if $S_a \subset S_b$, then there are additional properties
and methods that the later version expects. It's not necessary that
client code breaks in this case, but it is definitely a possibility.
Likewise in all other less likely scenarios, like $S_a$ and $S_b$
being neither super nor subsets of one another.

Consider the following example:

\begin{halfmini}{Variant A}
\begin{python}
def func(obj):
    obj.method1()
    obj.method2()
\end{python}
\end{halfmini}
\begin{halfmini}{Variant B}
\begin{python}
def func(obj):
    obj.method1()
\end{python}
\end{halfmini}

In this case, because \pythoninline{method2} is no longer expected in
Variant B, $S_a \supset S_b$ and we safely claim that there is no
breaking change. If it were the other way around, i.e. Variant A came
after Variant B, then that is not guaranteed. Client code previously
passing in objects that implement \pythoninline{method1} was not obliged to
have them implement \pythoninline{method2} as well, so it may break once the
new revision of the library starts needing it.

Depending on whether the configuration option
\texttt{Python/Structural Typing} is set, Autobump can examine the use
of a parameter inside a function and determine those two sets. This is
accomplished by walking the AST of the function definition and
checking what methods are called on that object in all possible
branches of execution. Afterwards, it takes the pessimistic view that
unless $S_a \supseteq S_b$ there was definitely a breaking change and
bumps the version number accordingly.

Depending on the nature of the project, this behaviour can be a
hinderance and yield lots of false positivies. That is why it is
configurable. If the option is disabled, then the Python handlers
never considers the types of entities and assumes them to be compatible.

\subsubsection{Type hinting}

As of Python 3.5 there is support for type hinting in the language
itself. It is purely syntactic and only affects the metadata of
objects when evaluated in the interpreter. No actual type checking is
done and this is done to static analysis tools, mostly type checkers.
The Python handler functions as another static analysis tool, but with
a somewhat different goal -- checking compatibility between different
versions of the same code, not validating invocations.

In Python 3.5 and 3.6, which are the only versions of Python with
support for type hinting at the time of writing, there is a small
oversight. When given code, the only to check the type hints is to
actually evaluate the code and check the metadata of the relevant
objects. This is undesirable in the case of Autobump where we want to
use the AST instead. A third-party fork of Python's built in
\textttd{ast} module exists, named \textttd{typed_ast}\cite{TypedAST}.
It gives access to type hinting information as part of AST itself.
It's expected that this fork will get merged back into mainline Python
eventually, but for the time being Autobump uses it as an external
library.

If the option \textttd{Python/Type Hinting} is enabled, whenever there
is type hinting information for a function or field it's actual usage
in the AST is ignored regardless of whether \textttd{Python/Use
Structural Typing} is enabled. Given the obvious incompatibility of
having one version be parsed with structural typing, and another one
having a type hint, we always pessimistically assume that the two are
not going to be compatible. If both versions have type hints, they are
compared using Python's native facilities.
% TODO: no they're not

\begin{halfmini}{Variant A}
\begin{python}
def func(a: str):
    a.split()
\end{python}
\end{halfmini}
\begin{halfmini}{Variant B}
\begin{python}
def func(a: str):
    a.split()
    a.strip()
\end{python}
\end{halfmini}

In the above example, if both type hinting and structural typing are
enabled, Autobump won't find a breaking change because the type hint
has remained the same. If the type hint were missing in both, it would
pessimistically claim there is a breaking change due to a new method
being called on the parameter \pythoninline{a}, unaware that it is
actually a string.

\subsection{Java}
\label{JavaHandlers}

% TODO: What is Java?
Java is a compiled, statically-typed language which makes it very
different from Python. Nevertheless, the two approaches -- AST
and introspection -- are still reasonable options for transforming a
Java source tree into CAPIR. Java is the only language supported by
Autobump that actually has two separate handlers.

\subsubsection{Using the abstract syntax tree}

\textttd{java_ast} is written entirely in Python and
values speed over corectness. It is a quick-and-dirty handler that
visits all Java source files in the repository and generates their
ASTs via the third-party package \texttt{javalang}\cite{Javalang}.
Transforming the AST into CAPIR is not quite as straightforward as it
is in Python, because Java is statically typed.

Consider this trivial example:

\begin{halfmini}{Variant A}
\begin{java}
public void method(A arg) { ... }
\end{java}
\end{halfmini}
\begin{halfmini}{Variant B}
\begin{java}
public void method(B arg) { ... }
\end{java}
\end{halfmini}

Note that the only thing that has changed is type of the parameter.
Whether or not there was a breaking change in this case depends on
two things.
\begin{enumerate}
\item What the relative names \javainline{A} and \javainline{B} get resolved
to. This is controlled by a \javainline{package} statement at the top of
the file and subsequent \javainline{import} statements. If the code
compiles fine, and the names are not explicitly imported, then it is
certain that they are from the same package as the source file. In the
case of a wildcard import, i.e. something like \javainline{import
org.apache.util.*}, there's no way to know whether the names are
coming from the wildcard or the current package. The problem
exacerbates when there are multiple wildcard imports. It would be
impossible to fully qualify the type names without asking Java itself
with introspection.
\item The relationship between the two types. Java supports class
inheritance and the conceptually similar implementation of interfaces.
If \texttt{B} is more concrete than \texttt{A}, i.e. it is further
down the inheritance tree, then there was definitely a breaking change
-- client code still passing in objects of type \texttt{A} cannot
implicitly cast to \texttt{B}. If \texttt{B} is more abstract, then
the parameter itself was generalised and client code will still work
fine passing in more concrete variants. While the AST of the relevant
classes can be inspected to see where they derive from or which
interfaces they implement, if the types are found in third-party code
or the AST is absent for whatever reason, it is again impossible to
decide whether there was a breaking change without using introspection.
\end{enumerate}

\textttd{java_ast}, as it is constructing the ASTs of all source files,
does its best to fully qualify type names based on the surrounding
statements and keeps track of the inheritance tree for the project. It
has obvious shortcomings in that it simply misses cases that can be
determined only by introspection. The optional configuration option
\texttt{Java/Error On External Types} forces the handler to give up
when the AST of a source file is needed, but not present in the project.

\subsubsection{Using reflection}

The alternative is \textttd{java_native}. It consists of two utilities
written in Java, along with some Python middleware that interacts with
the rest of Autobump. It relies solely on introspection of the
\texttt{.class} files generated after a full compilation of the
project. One tool, the \texttt{Inspector}, loads a list of class files
and then prints to standard out an XML representation of all classes,
including fields, methods, inner classes and so on. This is invoked
once, immediately after compilation. Note that the XML representation
is semantically absolutely indentical to CAPIR. The Python middleware then
translates this XML representation into a list of CAPIR units, which is
trivial.

\begin{halfmini}{Java Source}
\begin{java}
package com.autobump.somepackage;

public class SomeClass {
    public int field;

    public void method(String a, Integer b) {}

    public class InnerClass {
    }
}
\end{java}
\end{halfmini}
\begin{halfmini}{CAPIR (XML)}
\begin{xml}
<introspection>
  <class name="com.autobump.somepackage.SomeClass">
    <field name="field" type="int"/>
    <method name="method" returns="void">
      <signature>
        <parameter name="arg0" type="java.lang.String"/>
        <parameter name="arg1" type="java.lang.Integer"/>
      </signature>
    </method>
    <!-- omitted properties derived from java.lang.Object -->
    <class name="com.autobump.somepackage.SomeClass＄InnerClass">
      <!-- omitted properties derived from java.lang.Object -->
    </class>
  </class>
</introspection>
\end{xml}
\end{halfmini}

\begin{figure}[H]
\centering
\includegraphics[height=0.5\textheight]{images/dot/xmlcapir}
\caption{CAPIR of Java source code produced from intermediary XML}
\label{CAPIRXMLExample}
\end{figure}

Type comparison is handled through another tool --
\texttt{TypeCompatiblityChecker}. Given two types, it simply prints
whether or not the first one is compatible with the second one. This
again works by loading the relevant class files and simply asking Java
itself. CAPIR types set by the Python middleware will automatically
invoke this utility when \pythoninline{is_compatible} is called
later on in the comparison.

Having to compile the project first and then run these tools
introduced substantial complexity to the handler. It would be naïve to
hardcode any commands that compile the project. Instead, we should
delegate to the build system that comes with the project to properly
handle things like dependencies and \textttd{CLASSPATH} issues.
\textttd{java_ast} requires that a build command and a build root are
passed in to Autobump as command-line arguments.
% TODO: why arguments and not in config?

The build command is a shell command that runs before using the
\textttd{Inspector} to introspect all classes, for example
\textttd{mvn compile} if the project is using Maven as its build
system. The build root is a path relative to the root directory where
the classes get placed after compilation. It is important that the
handler knows that in order to correctly fully qualify class names. In
Java, the package hierarchy corresponds to directory structure, and
the wrong build root would result in the \textttd{Inspector} not
finding the needed classes.

The native handler produces much more accurate results, but is also
much slower -- it necessitates compiling the project, twice. There's
value in using the AST handler for self-contained libraries where it
would give identical results to the native one. Alternatively, if all
we are interested in is the new version number and not the changelog,
running the AST handler once can be a shortcut. If it reports a major
bump, then there's no reason to run the native one. After all, the AST
handler can detect a subset of changes compared to the native one. If
not, then \textttd{java_native} can be run to make sure.

\subsection{Clojure}
\label{Clojure}

Clojure is general-purpose variant of the Lisp programming language,
with an emphasis on functional programming. It has several properties
that make it noticeably different from the previous two languages we
considered:

\begin{itemize}
\item While not purely functional, it heavily emphasises programming
in a functional style and discourages stateful programming both through the
language itself and the culture surrounding the language.
\item It is homoiconic, that is programs are written in terms of the
language's own data structures (``code is data''). In essence the
programmer is always creating and editing the AST directly. This
allows for powerful metaprogramming features.
\item It is a dynamically typed language running on the statically
typed Java Virtual Machine.
\item It has support for gradual typing --
some variables and expressions may be given types that can be checked
at compile time, while others remain dynamically typed.
\end{itemize}

See appendix \ref{ClojurePrimer} for a primer on Clojure that
elaborates on these features and gives concrete examples.

The Clojure handler follows a model identical to \textttd{java_native}
-- the handler itself being written in Clojure and linking to Autobump
using some Python middleware. The bulk of the work done in Clojure is
just mapping a program to its public API. As was with Java and XML,
the most convenient structures to generate with Clojure are
S-expressions, so we use those as an intermediary before the
middleware can translate that into Autobump's internal CAPIR. The
handler takes advantage of the language's homoiconicity, which makes
it easy to inspect programs using the language itself.

At the core of every Lisp sits the \textit{reader}. This is the
function that converts Lisp objects from their textual form into
their internal representation. Clojure provides us with the
\clojureinline{read-string} function that reads in the string
representation of a form and returns it as a proper Lisp object.
Combining this with the \clojureinline{slurp} function that returns
the contents of a file as a string, we can build a function that
given a file name, returns the Clojure program stored inside as a list
of forms (other lists):

\begin{center}
\begin{tabular}{c}
\begin{clojure}
(defn read-source
  "Read in a Clojure source file and return a list of forms."
  [file-name]
  (read-string (str "(" (slurp file-name) ")")))
\end{clojure}
\end{tabular}
\end{center}

From there, we can filter out forms of interest and describe the part
of the API they expose using an S-expression. We can nest the
S-expressions together, forming a tree identical in structure to
CAPIR.

\begin{center}
\begin{tabular}{c}
\begin{clojure}
(defn describe-file
  "Describe the public API of a file as an S-expression."
  [file-name]
  (let* [forms         (read-source file-name)
         fields-defs   (filter public-field-def? forms)
         ...
         function-defs (filter public-fn-def? forms)]
    (list 'file
          (get-ns forms)
          (map describe-field fields-defs)
          ...
          (map describe-function function-defs))))
\end{clojure}
\end{tabular}
\end{center}

The functions prefixed with \clojureinline{describe-} return an
S-expression representing a CAPIR entity. The Python middleware's only
task from there is to translate those S-expressions into Autobump's
internal representation, which is done by implementing a minimal Lisp
reader in Python -- see \ref{LispReader}.
% TODO: no ref LispReader

% TODO: maybe don't clear here
\clearpage
To illustrate the process, consider the following example:

\begin{halfmini}{Clojure Source}
\begin{clojure}[otherkeywords={def,defn,defn-,ns}]
(ns lib.core)

(def cst 5)
(def ^{:private true} private-field)
(defn- private-func [])
(defn func
  ([x] (+ cst x))
  ([x ^Integer y] (* (func x) y)))
(defn get-nil [])
\end{clojure}
\end{halfmini}
\begin{halfmini}{CAPIR (S-expressions)}
\begin{clojure}[otherkeywords={file,field,function,signature}]
((file lib.core
  ((field cst nil))
  ((function func
     ((signature ((x nil)))
      (signature ((x nil) (y Integer)))))
   (function get-nil
      ((signature ()))))))
\end{clojure}
\end{halfmini}
\begin{figure}[H]
\centering
\includegraphics[height=0.5\textheight]{images/dot/sexpcapir}
\caption{CAPIR of Clojure source code produced from intermediary S-expressions}
\label{CAPIRSexpExample}
\end{figure}

\subsubsection{Macro expansion}

Clojure is a language that has a relatively small core, and a lot of
functionality is built by using macros that expand to special forms.
For example, the macro for defining a function \clojureinline{defn}
eventually expands to a \clojureinlinedef with an anonymous function.
This makes it somewhat tricky to extract functions and fields from
namespaces without actually evaluating the code. Clojure provides a
special form \clojureinline{macroexpand} which takes in a form and
returns the form with one step of macro expansion.
\clojureinline{macroexpand-all} may be more appropriate, because it
recursively expands all macros until there are no more.

It may be the case that a library is entirely written with the
top-level forms being macros that when evaluated populate the
namespace with functions and fields. This is in essence the same
problem as with Python, and likely with any dynamic language, and is
fairly unlikely to happen in real-world libraries.

Knowing all of this, we have two possible solutions to extracting the
public API of a Clojure namespace -- either running
\clojureinline{macroexpand} or \clojureinline{macroexpand-all} on all
top-level forms and then trying to parse them, or parsing them as they are.

Autobump takes the second approach. It makes the tradeoff of not
further complicating the handler by expanding macros and instead
parsing the forms as they are, e.g. only looking at
\clojureinline{defn} forms for function definitions, and not
considering \clojureinlinedef that have an anonymous function as the
value. This means that there are definitely cases where the handler
would not produce the correct API from the source code, but it's very
unlikely examples like that would be found in non-contrived,
real-world libraries.

\subsubsection{Type hinting}



% TODO: clojure type hinting
% TODO: jvm interop

\section{Handlers for version control systems}
\label{VCSHandlers}

\section{Comparison logic}
\label{Logic}

After the language handler has run twice and generated two CAPIR
trees, the comparison algorithm can be invoked to find the differences
between the two variants. It itself is fairly straightforward and
defined recursively:

\begin{algorithm}
\caption{CAPIR Comparison}
\begin{algorithmic}[1]
% TODO: Better name instead of 'comparisons'
% TODO: Member instead of attribute
\Procedure{Compare}{A, B}
\State $assert(type(A)$ is $type(B))$
\State $comparators \gets \text{map of the form }(member \rightarrow function)$
\For{$member, comparator$ in $comparators$}
\If{$member$ in $A$}
$report(comparator(A[member], B[member]))$
\EndIf
\EndFor
\State $all \gets members(A) \cup members(B)$
\For{$member$ in $all$}
\If {$member$ not in $A$ and $member$ in $B$}
$report(\detokenize{ENTITY_INTRODUCED})$
\EndIf
\If {$member$ in $A$ and $member$ not in $B$}
$report(\detokenize{ENTITY_REMOVED})$
\EndIf
\EndFor
\State $common \gets members(A) \cap members(B)$
\For{$member$ in $common$}
$compare(A[member], B[member])$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

In short, the algorithm compares two entities by examining their list
of attributes and runs specific comparator functions on known
bottom-level members -- such as types and signatures. After that is
done, it checks for members that are missing from either variant and
recurses into members that are present in the intersection of both entities.

Note that this procedure technically compares entities, not trees. To
kick off the comparison of two trees it's necessary to wrap both in a
CAPIR unit, so that the first level of recursion actually looks at the
top-level units in both codebases.

$report$ is a simple function that takes care of adding a change to
the changelog and checking whether the latest reported change is more
notable than the most notable so far. For example, reporting a feature
addition after several patches would cause a feature addition to be
the most notable change. Note that nothing can top a breaking change.
Had Autobump only concerned itself with bumping the version number,
the algorithm could stop at finding a breaking change. However,
because we want to generate a changelog as well, we carry on looking
for changes even though the version number itself is clear at this point.

\section{User interface}

\subsection{Integration with other development tools}

\chapter{Testing}
\label{Testing}

In this chapter, we look at how we demonstrate that Autobump indeed
works and what testing techniques were used throughout development.
Testing is central to Autobump's development and was done from very
early on.

\section{Unit testing}

Non-trivial modules from the implementation, namely the comparison
logic and the language handlers, are tested in isolation by using
Python's built-in \pythoninline{unittest} framework. This has been very
useful for identifying edge cases where the modules don't quite do the
correct thing. Also, adding new features to the language handlers has
proven to be very error-prone, so having the unit tests in place
serves well for identifying regressions.

All tests treat the module they're testing as a black box. This is
very easy to do, because virtually all modules have a single
entrypoint that abstracts all implementation detail. Different inputs
are given to that function and then the results are compared against
the expected ones.

\subsection{Comparison logic}

% TODO: What about VCS handlers?
\subsection{Language handlers}

The entrypoint of every language handler is the function
\pythoninline{codebase_to_units} that has a single input parameter, the
location of that codebase on disk. Typically that directory would be
produced by Autobump in advance of calling the handler by cloning and
checking out some revision of a repository.
% TODO: What if it's not Git?

To test this function, we need to mock the presense of a directory
containing some fixture instead of an actual repository. This is best
done with Python's built-in \pythoninline{tempfile} module which can create
temporary files and directories, not necessarily physically on the
disk. The fixtures are found in the tests themselves and are written
to temporary files as part of the set up process. By holding onto the
handle obtained by calling functions from \texttt{tempfile} , we can
ensure that everything is cleaned up during the tear down phase of
testing.

A fixture consists of a list of tuples, each tuple representing the
path to a file and its content. A minimal Clojure fixture looks like
this:

\begin{center}
\begin{tabular}{c}
\begin{python}
sources = [
    ("lib/core.clj",
    "
    (ns lib.core)
    (def constant 1)
    ")
]
\end{python}
\end{tabular}
\end{center}

Having the fixture actually present in the filesystem is necessary
because the handlers rely on external utilities, which need to load
and parse those files. The notable exception is the Python handler,
which is the only one written entirely in Python. For that reason we
can take a shortcut when writing tests for it. Instead of calling the
entrypoint \pythoninline{codebase_to_units} which scans a location for
relevant files and only then converts them to CAPIR units, we can
directly call \pythoninline{_source_to_unit} which is an
implementation-specific function found only in the Python handler.
It takes in Python source code as a string and returns a unit. Even
though this is technically white box testing, we still treat this
function as if it were the entrypoint to the module. This has two
advantages: it greatly speeds up the tests because there's no need
to create temporary files and directories, and it simplifies the
fixtures -- they can be the source code of a single file with no metadata.

\section{Acceptance testing}

Even if the unit tests all pass and reasonably cover edge cases for
the individial modules, Autobump itself can still misbehave and
produce wrong results when used in practice. Early on in the project,
Autobump was tested against some real-life and some specially
constructed repositories manually. This was horribly inefficient and
error-prone. For that reason it was crucial to develop acceptance, or
end-to-end, tests that can quickly show that Autobump works correctly
when applied to a full-blown repository.

We can consider the entirety of Autobump as a single entity, with the
input being a repository\footnote{There are of course additional
inputs, like what handler to use or which revision to compare.
However, those are trivial to mock.} and the output the proposed
version number. Having a repository as an input is non-trivial.

Three approaches were considered.
\begin{enumerate}
\item Maintain a Git \textit{submodule}, a kind of nested repository,
inside Autobump's own repository. The submodule would contain the
history of a mock project that changes its API over time. Specific
commits would be tagged with version numbers, representing the version
number that Autobump is expected to produce when comparing that
revision to the previously tagged one. This approach was quickly
ruled, because it makes editing the progression of the project
unnecessarily difficult.
% TODO: Why?
\item Reconstruct a Git repository containing the history of a project
by using an acceptance test framework like \textit{Robot
Framework}\cite{Robot}. Frameworks like these generally let you
specify pre and postconditions either in freeform or
\textit{given-when-then} form, so an example test may look something
like this:

% TODO: should this be verbatim?
\begin{center}
\begin{BVerbatim}
Given a public method in a class
When the method is removed
Then version bump should be major
\end{BVerbatim}
\end{center}

This would have produced very clear and easy to read tests, at the
expense of having to spend much development time translating
instructions like these into an actual repository that can be fed into Autobump.
\item Reconstruct a Git repository from a series of patches kept in
text form as part of test itself. Every patch would have metadata that
indicates the expected version of the project at that commit. This is
the option that Autobump uses, as it achieves a convenient
middleground between the other two: it allows for the history of the
project to be edited relatively easily, and it has much smaller
boilerplate needs. The only accompanying code is one that creates a
Git repository in a temporary directory from that series of patches.
% TODO: accompanying code
\end{enumerate}

\chapter{Evaluation}
\label{Evaluation}

In this chapter, we try to use Autobump with real-world libraries
written in the different target languages. We look at whether Autobump
automatically produces similar or identical version numbers compared
to the ones assigned manually by the developers and interpret our findings.

% TODO: Evaluation was continuous in nature

\section{Evaluation against real-world projects}

% TODO: Evaluate with students in lab as well?

\chapter{Conclusion}
\label{Conclusion}

% TODO: handler for binaries using debugging information, supports
% multiple languages

\begin{appendices}

\chapter{Instructions for building and running \textit{autobump}}

Autobump requires at least Python 3.5. Other than that, it has no
dependencies. A typical Python setup involving a virtual environment
is not necessary.

\section{Building}

As it is written in Python, Autobump does not have a build step. The
\texttt{Makefile} found in the repository has the following targets:

\begin{itemize}
\item \texttt{make test} runs the test suite.
\item \texttt{make lint} runs the linter \texttt{flake8} (or the one
specified as \texttt{LINTER}) against all source code.
\item \texttt{make all} or \texttt{make} runs all of the above.
\end{itemize}

\section{Usage}

\chapter{Project structure}
% TODO: fill in

\chapter{Representative source code listings}

\chapter{Primer on Clojure}
\label{ClojurePrimer}

This appendix gives a simplified view of the language and briefly
covers aspects of it that relate to Autobump. The goal is to give a
reader unfamiliar with Clojure enough knowledge to make sense of
subsection \ref{Clojure}.

Clojure is a functional, dynamically typed language from the Lisp
family of languages that runs on top of the Java Virtual Machine. It
has a small set of immutable data structures that are reused
throughout the standard and external libraries instead of defining
ad-hoc ones (``It is better to have 100 functions operate on one data
structure than to have 10 functions operate on 10 data
structures.''\cite{ClojureRationale}).

The most frequently used data structures are lists, vectors, maps and
sets:

\begin{center}
\begin{tabular}{c}
\begin{clojure}
(1 2 3 4)    ;; list
[1 2 3 4]    ;; vector
{:a 1 :b 2}  ;; map
#{1 2 3 4}   ;; set
\end{clojure}
\end{tabular}
\end{center}

Programs written in Clojure are themselves expressed in those data
structures, mostly lists and vectors. The source code of a Clojure
program is nearly identical to its abstract syntax tree, with the
parser having to do very little work. Every node in the AST is denoted
by an \textit{S-expression}, which is classically defined as being either an
atom, or an ordered pair of two other S-expressions. S-expressions are
also known as \textit{forms}.

Function calling is done by constructing a list where the first
element is the function and the rest are arguments (i.e. prefix
notation):

\begin{center}
\begin{tabular}{c}
\begin{clojure}
(+ 1 2)      ;; => 3
\end{clojure}
\end{tabular}
\end{center}

Functions are defined by invoking the macro \clojureinline{defn},
where the first argument is the name of function, the second is a
vector comprising the arguments and the rest are the body of the
function. Macro forms, when evaluated, produce other forms.

\begin{center}
\begin{tabular}{c}
\begin{clojure}[emph=add]
(defn add [a b]
  (+ a b))
\end{clojure}
\end{tabular}
\end{center}

Functions can have multiple signatures by wrapping argument-body pairs
in lists:

\begin{center}
\begin{tabular}{c}
\begin{clojure}[emph=add]
(defn add
  ([a b]
   (+ a b))
  ([a b c]
   (+ a b c)))
\end{clojure}
\end{tabular}
\end{center}

Special forms are built-in baseline forms. The special form
\clojureinlinedef\enspace is used to define symbols that
match to values:

\begin{center}
\begin{tabular}{c}
\begin{clojure}[emph=pi,otherkeywords=def]
(def pi 3.14)
\end{clojure}
\end{tabular}
\end{center}

In fact, the \clojureinline{defn} macro expands as such:

\begin{center}
\begin{tabular}{c}
\begin{clojure}[emph=add]
(macroexpand '(defn add [a b] (+ a b)))

;; => (def add (fn ([a b] (+ a b))))
\end{clojure}
\end{tabular}
\end{center}

Evaluating a list can be avoided by prefixing it with
\clojureinline{'} as done in the above example. This is shorthand for
the special form \clojureinline{quote}. \clojureinline{'(a b c)} and
\clojureinline{(quote (a b c))} are equivalent. Quoting a list returns
it literally.

Defined symbols in Clojure can have metadata attached to them. This
feature is most frequently used for docstrings, type hinting or hiding
implementation details. Metadata is handled by the Lisp reader
(parser) which converts the textual representation of objects into the
internal one.

\begin{center}
\begin{tabular}{c}
\begin{clojure}[otherkeywords=def]
(defn add [^Integer a ^Integer b] ;; Both parameters are hinted to be integers.
  (+ a b))

(def ^{:private true} pi 3.14)    ;; Field is now private.
\end{clojure}
\end{tabular}
\end{center}

To make functions private, there is a built-in macro
\clojureinline{defn-}. It expands to exactly the same form as
\clojureinline{defn} but sets the \clojureinline{:private} tag to be
\clojureinline{true} just like defining a private field.

Clojure code is divided into namespaces, similarly to Java packages.
Namespace declarations are put at the top of the file.

\begin{center}
\begin{tabular}{c}
\begin{clojure}[otherkeywords=ns]
(ns lib.core)
\end{clojure}
\end{tabular}
\end{center}

The same special form can be used to make other namespaces available
in the current one, similarly to Java's \javainline{import} statement.

\begin{center}
\begin{tabular}{c}
\begin{clojure}[otherkeywords=ns]
(ns lib.core
  (:require [clojure.test :as test]
            [lib.other :refer [function1 function2]]))
\end{clojure}
\end{tabular}
\end{center}

\end{appendices}

%% Bibliography
\bibliographystyle{ieeetr}
\bibliography{dissertation}

\end{document}

% The below list of file-local variables enables
% auto filling of text and completely nukes all indentation
% except for code blocks.

% Local Variables:
% eval: (auto-fill-mode 1)
% sentence-end-double-space: nil
% LaTeX-indent-environment-list: (("verbatim" current-indentation) ("lstlisting" current-indentation))
% TeX-brace-indent-level: 0
% LaTeX-indent-level: 0
% LaTeX-item-indent: 0
% tex-indent-basic: 0
% tex-indent-item: 0
% tex-indent-arg: 0
% eval: (highlight-regexp "TODO")
% End:
