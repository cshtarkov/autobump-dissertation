\pdfoutput=1

\documentclass{l4proj}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{microtype}
\usepackage{enumerate}
\usepackage{url}
\usepackage{breakurl}
\usepackage[breaklinks]{hyperref}
\usepackage{listings}
\lstset{language=Python}
\def\UrlBreaks{\do\/\do-}
\usepackage[T1]{fontenc}
\usepackage{float}

\begin{document}
\title{Automatically proposing next semantic version of a project by \\
inspecting changes to the codebase}
\author{Christian Shtarkov}
\date{\today}
\maketitle

\begin{abstract}
In software development, assigning a version number
to each release is crucial. It allows customers and other developers
to identify that a software package has changed from a previous
iteration. It is especially important when considering libraries and
frameworks, where changes can greatly affect applications built on top
of them. \\ Different policies exist dictating how to assign version
numbers, one of the most popular being \textit{Semantic
Versioning}\cite{SemanticVersioning}, where the version gives
information about how exactly the software package has changed. This
allows for better estimation whether a new version of a library or a
framework would result in incompatibilities.
\\\\
Regardless of
policy, assigning version numbers is typically done manually and
leaves room for human mistakes or deviations from the established
scheme. We propose the tool \textit{autobump} that automatically
inspects changes to a codebase and proposes the next version number
according to semantic versioning. This reduces friction when releasing
packages, and encourages more frequent, even fully automated,
releases.
\end{abstract}

\educationalconsent

\tableofcontents

%% Contents
\chapter{Introduction}
\pagenumbering{arabic}

In this chapter, we discuss the background of software versioning and
more specifically, semantic versioning of software. Then, we move to
discuss existing tools that help automate the assignment of semantic
version numbers.

\section{Software versioning}
Software versioning is the practice of assigning nominal or numerical
identifiers to subsequent iterations of a software package. Every
variant of a piece of software is expected to have a different
(however small the difference) set of functionality, and versioning
allows for finding out how exactly a piece of software is expected to
behave. In practice, all non-trivial software is versioned in one way
or another.

Versioning can have different levels of granularity. Revision control
systems, such as Git or Subversion, allow developers to introduce
small incremental changes to a codebase. After every change, the new
state of the codebase is given a unique identifier and made
recoverable. For example, Subversion assigns a single number to every
revision starting at 1 and increasing in chronological order. This
allows for many productivity-boosting features, such as reverting code
to a previous state, proving that a particular changeset is
responsible for the introduction of a bug, or identifying differences
between divergent branches of the same initial piece of code.

On a macro scale, releases of software packages are also versioned.
Because releases happen much less often than small changes, versions
also change much less often and follow different policies.
The de facto standard are sequence-based schemes where each
version identifier is a tuple consisting of numbers or letters
separated by dots -- e.g. \textit{1.3.2} or \textit{10.6-rc1}. The
vast majority of computer software follows a scheme of this type. It
is usually implied that the further to the left an element of the
tuple is, the more significant it is. What ``significant'' means is
not well-defined and depends on the kind of software that is being
versioned. Comparing versions \textit{5.0} and \textit{6.0} of a Web
browser may imply that the latter has new features, whereas comparing
the same versions of a library may imply that client code using the
former is incompatible with the latter. In general, incrementing
versions in this format follows these rules:
\begin{enumerate}[(i)]
\item Incrementing any non-final element ``resets'' all elements to
the right, i.e. \textit{3.5.4} $\rightarrow$ \textit{3.6.0}.
\item Incrementing a version which has a label attached at the end
means to either change the label or drop it, i.e. \textit{3.5.4-beta}
$\rightarrow$ \textit{3.5.4} or \textit{3.5.4-rc1} $\rightarrow
$ \textit{3.5.4-rc2}.
\end{enumerate}

Note that software can be versioned by more than one scheme -- it is a
typical scenario that revision numbers are used by developers
internally and a sequence-based scheme is used for public releases.
Large consumer-facing software with long release cycles, like
operating systems, may use a codename or date-based scheme for
versioning public releases -- e.g. \textit{Ubuntu 14.04 (Trusty Tahr)}
which was released in April of 2014 happens to use both. Those
typically serve a marketing purpose rather than provide any technical
benefit.

\section{Semantic versioning}

In software development, it is a widespread practice to leverage
pre-existing code in the form of libraries or
frameworks\footnote{Those are the most common use cases for one piece
of software interfacing with another. The same argument also applies
to philosophically similar use cases, like invoking command-line tools
such as \textit{sed} or \textit{awk} from a script.} to avoid
duplication of effort. Given that client applications and the
libraries they depend on have separate development cycles, a problem
arises in determining whether a client application is compatible with
a different version of the same library. A common solution is to take
a sequence-based scheme and attach meaning to the positions in order
to measure degree of compatibility.

Semantic versioning\cite{SemanticVersioning} is a strict formalization of such a
scheme, where the format and significance of each position are
well-defined:

\begin{center}
$\mathrm{Major.Minor.Patch}(-\mathrm{Prerelease})^?(+\mathrm{Build})^?$
\end{center}

\begin{itemize}
\item \textit{Major} increments when incompatible changes to the
public API have been and it is possible for interfacing software to no
longer work.
\item \textit{Minor} increments when new features have been added in a
backwards-compatible manner.
\item \textit{Patch} increments when bugs have been fixed in a
backwards-compatible manner.
\item \textit{Prerelease} is an optional label indicating that the
version does not yet meet the compatibility requirements imposed by
the first three numbers.
\item \textit{Build} is another optional label indicating the
corresponding internal version numbers, e.g. the one used by the
revision control system.
\end{itemize}

\subsection{Benefits}

Semver is especially useful for software meant for use by other
software, such as libraries and frameworks. When a new version of a
library is released, someone developing an application based on that
library can quickly judge how the update affects their application.

If only the minor or patch numbers have changed, then the application
should work fine with the new version. That is because incrementing
just those numbers implies that there are no breaking changes. If,
however, the major number has increased, then further work on part of
the application developer may be needed to make it compatible again
with the library.

This scheme also helps with automatic dependency management systems,
like ones found in package managers. A developer can specify that
their application relies on version \textit{2.4} of a particular
library. Semver guarantees that versions of the form \textit{(2.x | x
> 4)} will contain all features found in \textit{2.4} and be
backwards-compatible. If later on \textit{2.4.1} or \textit{2.5} come
along, the library can be automatically upgraded without breaking the
application, with the added benefit of fixing bugs.

In summary, the most important benefit provided by Semver is the
alleviation of dependency hell through unambiguous identification of
breaking changes. The major version number is increased if and only if
backwards compatibility is deprecated.

\subsection{Application}

Semver only provides these benefits when it is applied consistently.
If applied haphazardly, then two versions cannot be compared
reliably and decisions made by simply examining version numbers are
risky. A good example of that is the programming language PHP.
Although minor releases are usually meant for feature additions, it
can happen that they contain breaking changes along with the new
features. One the many examples are the changes introduced in PHP 5.3
compared to PHP 5.2\cite{PHPChangelog} where the signature of an array
of functions changed -- constituting a breaking change. The PHP
changelogs routinely document what backwards-incompatible changes are
in a release, but using the versioning scheme incosistently negates
most benefits.

In practice, semantic versioning is applied poorly. A
study\cite{SemverMaven} of more than 22,000 libraries in the Maven
repository found that there was virtually no difference in the
frequency of breaking changes in major and minor version increments.
It also found that this has a significant impact on client
applications, mostly by the introduction of compilation errors.

The study does not go into determining what the reasons are for these
inconsistencies, but some possibilities are:
\begin{itemize}
\item Some libraries never claimed to follow Semver, instead opting
for a similar, yet not identical, scheme.
\item Semver is very strict about what's considered a breaking change.
This is great for robots, but not so much for humans. Small changes to
the API may not be considered ``major'' by the developers even if they
are technically breaking.
\item Misinterpretation of Semver about what constitutes a breaking
change. Some developers may be inclined to believe that an API change
that affects none of their users (to their knowledge) is not breaking.
\end{itemize}

One possible solution to this problem, which we propose in this
project, is to attempt to automate the assignment of version numbers.

\section{Tools for automatically assigning semantic version numbers}

Automatically assigning version numbers is not a new idea and projects
exist that already do that to some extent.

\subsection{semantic-release}

\textit{semantic-release}\cite{SemanticRelease} is a Javascript tool
for ``fully automated package publishing''. It can do a range of
functions related to automated publishing of packages in the
Javascript ecosystem through the Node Package Manager, but of interest
are the two ways it can deal with proposing a new version number.

One way is that it tries to interpret Git commit messages and from
that determines what the next version number should be. A minor
(feature) release for example would be indicated by a commit message
of the following format:

\begin{center}
\texttt{feat(pencil): add 'graphiteWidth' option}
\end{center}

This approach suffers from the same drawback as incrementing the
version number manually does -- it has to be done consistently.

Another way is by employing an additional tool called
\textit{cracks}\cite{Cracks}. According to the official description
``this module can automatically detect breaking changes by running the
test suite of your last-release against the current codebase. This
shouldn't fail''. This is generally a good approach -- by definition
the test suite of the previous version should pass if the new one is
not meant to be a major release. However, it raises some issues.

\begin{itemize}
\item It could be that the library in question has whitebox unit
tests. If they start failing, that does not necessarily constitute a
breaking change, because while the internal implementation of those
methods may have changed, the public API is still the same. Cracks has
no way of differentiating blackbox from whitebox tests unless manually
specified.
\item The unit tests may not cover the public API in its entirety.
Modifying the signature of a public method, for example, that's not
covered by the tests may go completely unnoticed.
\item The library may be in a stage of development such that it has no
unit tests, or they cover a very small portion of the public API, yet
the authors still want to use semantic versioning.
\end{itemize}

\subsection{GitVersion}

\textit{GitVersion}\cite{GitVersion} ``looks at your Git history and
works out the semantic version of the commit being built''.  By
knowing in advance what workflow contributors to the repository are
using, it can determine when breaking changes occur, when new features
are being introduced and when it's only patches that are going
through. Previous versions of GitVersion have implicit knowledge of
the workflows \textit{GitFlow}\cite{GitFlow} and
\textit{GitHubFlow}\cite{GitHubFlow}, but the current one (3.0) at the
time of writing can be configured to work with more custom workflows.

Consider an example usage of GitVersion on a history of commits.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/dot/gitversionexample}
\caption{Example Git commit history}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\textbf{Commit} & \textbf{Version} & \textbf{Explanation}                              \\
0      & 1.0.0                 & First commit that has a version.                      \\
1      & 1.0.1+1               & Patch was introduced, and one commit since then.      \\
2      & 1.0.1-feature-foo.1+1 & Feature was introduced, and one commit since then.    \\
3      & 1.0.1+3               & Three commits since the last patch.                   \\
4      & 1.1.0-rc.1            & First release candidate for the new feature.          \\
5      & 1.1.0                 & Feature branch was merged.                            \\
6      & 1.1.1+1               & Patch was introduced, and one commit since then.      \\
7      & 1.1.1+2               & Patch was introduced, and two commits since then.     \\
8      & 2.0.0-rc.1            & First release candidate for the new breaking changes. \\
9      & 2.0.0                 & Breaking release branch was merged.
\end{tabular}
\caption{Proposed semantic version numbers by GitVersion, if every
commit were to be released}
\end{table}

Notice that similarly to semantic-release, GitVersion relies on the
naming of the branches. This results in the same set of drawbacks. It
has to be applied consistently, and forces contributors to use one
particular naming scheme. The latter one is somewhat alleviated in
this case, as the workflow GitVersion expects is highly
configurable\cite{GitVersionConfiguration}.

\subsection{Endjin.Assembly.ChangeDetection}

\textit{Endjin.Assembly.ChangeDetection}\cite{Endjin} is a
proof-of-concept project that takes a very different and less
error-prone approach. It can identify breaking changes in .NET
assemblies by looking at the assembly itself. It does this
by examining any changes made to public types in the assembly, such as
removing them or changing their signature.

This way of proposing a version number truly reflects changes to
the code and imposes no additonal requirements on contributors to
the repository. However, Endjin.Assembly.ChangeDetection is not really
a tool, but an experiment, and is tied to the .NET ecosystem.

\subsection{elm-package}

\subsection{SpiseMesu.SemanticVersioning}

\chapter{Determining next semantic version through code analysis}

In this chapter, we discuss the approach that the tool developed for
this project, Autobump, uses to determine semantic version numbers.

Much like Endjin.Assembly.ChangeDetection, the tool we propose tries
to look at how the code itself has changed to estimate what the next
version should be. This requires no manual intervention from the user,
no special naming schemes, and is as reliable as it can get.

% TODO: Determining vs Estimation?

\section{Breaking changes}
\label{BreakingChanges}

For the definition of breaking change to be useful, it ought to be
strict -- and that is the way Semver approaches it. It defines
\textbf{any} change to the API which makes it incompatible\footnote{By
``incompatible'', we mean changes that make the new code technically
incompatible with the old, i.e. it doesn't compile in the case of
static languages, or throws unexpected runtime errors with dynamic
languages.} with the last version as breaking. It does not take into
account number of affected users, difficulty of transitioning to the
new version, or changes in behaviour, because those things cannot be
possibly known reliably.

In the spirit of Semver, for this project we adopted a technical and
deterministic list of changes to entities that are considered
to be breaking. We make the assumption that all public-facing entities
(i.e. those accessible externally in a normal for the language way)
are part of the API.

\begin{table}[H]
\centering
\begin{tabular}{|p{0.45\linewidth}|p{0.45\linewidth}|}
\hline
\textbf{Change}                                                                               & \textbf{Rationalisation}                                                 \\
\hline
Removing an entity.                                                                           & User applications may be using it.                                       \\
\hline
Changing the type of an entity to an incompatible type.                                       & User applications may be expecting a different type.                     \\
\hline
Adding a parameter with no default value to a function signature.                             & Calls to that function will immediately fail due to a missing parameter. \\
\hline
Removing a parameter from a function signature, regardless of whether it had a default value. & Calls to that function \textit{may} fail due to a missing parameter.     \\
\hline
Removing the default value of a parameter.                                                    & Calls to that function may be relying on the now missing default value.  \\
\hline
\end{tabular}
\caption{List of changes that are considered breaking}
\end{table}

Note that the list is sufficiently abstract to accommodate for
differences in widely used languages. It makes only three assumptions
about the structure of the language.

\begin{enumerate}
\item The language provides some mechanism of grouping functionality
into entities, be it object-orientation, namespacing, modules and so
on. Note that groupings are absolutely allowed to be nested --
consider a Java library which defines a constant inside a class inside
a class. In this case the constant and both classes are considered to
be ``entities''.
\item The language has some notion of parameterized callable pieces of
code, which for simplicity's sake we refer to only as ``functions'',
even though in reality they may be object methods, macros etc.
Additionally, some of those functions are callable from outside
of the program where they are defined, effectively being an API.
\label{SecondAssumption}
\item The language has a type system which has the notion of binary
compatibility:
\begin{equation}
\forall(T,Y) \in S, \mathrm{compatible}(T,Y) \in \{\mathrm{True},\mathrm{False}\}
\end{equation}
where $S$ is the set of all admissable types in the language and
program. \\

We define compatibility as $T$ being able to replace $Y$
without violating the rules of the language -- e.g. the Liskov
substitution principle\cite{Liskov} in a typical object-oriented
language.
\end{enumerate}

There are languages that do not conform at all to these assumptions
(e.g. minimalistic ones such as Brainfuck\cite{Brainfuck}), but these
are also languages where a tool like Autobump would be utterly
unuseful. There are also languages that partially conform. In those
cases, as long as assumption \ref{SecondAssumption} is valid (i.e.
there is an API), we can ignore the rest -- of course making obsolete
some functionality of Autobump\footnote{Namely identifying in which
parts of the program the changes have occurred and checking type
compatibility.}. For this project, we examined the radically
different languages Python, Java and Clojure\ref{LanguageHandlers} and
the assumptions were found to be reasonable.

\section{Feature additions}

Semver defines feature additions as backwards-compatible changes to
the API. This means that changes which colloquially may be thought of
new features, such as making the library run faster or handling more
edge cases are considered patch changes by Semver.

Going by the list of breaking changes and replacing its destructive
aspect with a constructive one, we get the following list:

\begin{itemize}
\item Adding a new entity.
\item Changing the type of an entity into a compatible type.
\item Adding a parameter with a default value to a function signature.
\end{itemize}

\section{The \textit{autobump} tool}

The goal of this project was to build \textit{autobump} -- a tool which
given two revisions in a version controlled project reliably
determines what the version of the later revision should
be\footnote{In some contexts, ``version'' and ``revision'' may be
identical. In this dissertation, we refer to the semantic version of a
release as ``version'', and the commit identifier in a VCS as ``revision''}.

Although technically Autobump can be used with any project, it is
mainly aimed at libraries, frameworks and otherwise projects whose
main usefulness comes from the ability to be integrated in client
applications. It is with those types of projects that semantic
versioning is most practical.

Because identifying the version delta requires first identifying a
list of changes to the public API, a secondary function of autobump is
the generation of a changelog in preparation for a release.

Autobump can be part of a manual or automated workflow of bumping the
version number with every release, generating a changelog, or both. It
is a command-line application with behaviour driven by passing
options, so it can be easily integrated in an automated deployment
system. Another possibility is using it standalone as a sanity check.
Developers can run it to find out if they have unintentionally broken
backwards compatibility with a recent commit.

Autobump, while its core is written in Python, is modular in
nature (\ref{Architecture}) and supports different
languages (\ref{LanguageHandlers}) and version control
systems (\ref{VCSHandlers}).

\chapter{Implementation}

In this chapter, we look at how Autobump works internally in terms of
its architecture and how it exposes its functionality to the end
user. Furthermore, we focus on how implementation details differ for
the different target languages.

\section{Architecture}
\label{Architecture}

% TODO: Configuration

When designing the architecture of the tool, great care was taken to
separate unrelated concerns. Every part of the program lives in a
separate module, with dependencies (via \texttt{import}) reduced to a minimum.

\begin{minipage}{0.5\textwidth}
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/dot/architecture}
\caption{\textit{Autobump} architecture}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\textwidth}
In normal operation, the top-level program generally performs the
following steps:

\begin{enumerate}
\item Identify which VCS the repository is using and select the
appropriate handler.
\item Identify which two revisions should be compared -- either from
command-line arguments or through an educated guess.
\item Run the comparison logic on the two revisions. This involves
converting the APIs of both into a common\footnote{Common between
different source languages, not the two representations.} representation via
the appropriate language handler. This may necessitate a ``build'' step.
\item Calculate the new version number by bumping the old one.
\end{enumerate}
\end{minipage}

\subsection{Common representation for APIs}

Before proceeding with the issue of representing APIs, it is important
to note that for the purposes of this project we are not interested in
any internal APIs software packages might use -- they should not
influence external applications that depend on the packages in question.
We are interested only in the public APIs which are meant for external
use, as changing these indicates an incrementation of the semantic version.

Most general-purpose languages provide ways to restrict visibility of
entities in the source code (such as classes, functions and so on).
Technically all externally visible entities, usually indicated with the keyword
\texttt{public}, are accessible from client applications. In practice,
however, it may be that not all of these form the intended, official
and documented public API. Faced with this problem, Autobump takes the
safer approach and only considers visibility when determining whether
an entity is part of the public API. The reason for that is that there
is nothing technically restricting client applications from using
visible entities that are otherwise undocumented or not formally
intended for outside use.

By relying on language-specific handlers to extract the public API of
a codebase and convert it into a common representation, the same
comparison logic can be reused. Beside reducing repetitiveness in the
code, this has the additional effect of allowing handlers for new
languages to be ``plugged in'' with virtually no changes to the
program itself.

Choosing an appropriate representation was an important, and
difficult, part of this project. The problem of representing an API
regardless of language is solved for some contexts by
\textit{interface descriptor languages} (IDLs), such as
WSDL\cite{WSDL}, SWIG\cite{SWIG} or Apache Thrift\cite{Thrift}. All of
these are meant for a specific purpose, usually interoperability between
different technologies. While adapting one of these to use in Autobump
was certainly possible, we opted to use our own model instead for
several reasons.

\begin{itemize}
\item Barely any features are needed except for API representation.
Using an established IDL may be considered overkill.
\item Using an established IDL would mean either importing an external
library as a dependency, or writing our own, and both are undesirable.
\item Should extensions be needed, it would be trivial to extend our
own model compared to a third-party one.
\item Autobump's representation of APIs is internal -- the user should
never care about it, so the choice of IDL can ignore any user
experience issues.
\end{itemize}

The representation that Autobump uses (fig. \ref{RepresentationUML})
consists of several entities that are common across public APIs
exposed by different languages. From now on, we will refer to this
model as \textbf{CAPIR} (standing for ``common API representation'') for the
sake of brevity.

\textbf{Entity} is a base class representing an entity of
interest found in source code. All deriving classes are guaranteed to
implement equality comparison to other entities (via
\texttt{\detokenize{__eq__}} and \texttt{\detokenize{__ne__}}).

\textbf{Type} is a base class that is the basis of the type
systems used by the language handlers. This is discussed in further
detail in \ref{TypeSystems}.

\textbf{Unit}s are a recursive data structure that indicate a
general grouping of entities. They can represent a range of grouping
techniques found in different languages -- translation units, modules,
classes, interfaces, mixins, traits etc. In fact, Autobump uses a
single top-level Unit to represent an entire codebase.

\textbf{Field}s and \textbf{Function}s are found in Units. Fields have
a type and denote the setting of a variable or a constant. Functions
represent callable pieces of code -- functions, methods, routines, or
whatever the source language calls them.

Every Function has one or more \textbf{Signature}s which in turn are
lists of \textbf{Parameter}s. Parameters have a type and can have a
default value.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/dot/repr}
\caption{Class diagram -- Autobump's public API representation (CAPIR)}
\label{RepresentationUML}
\end{figure}

% TODO: Expand on fields and parameters.

\clearpage
As an example of what CAPIR looks like in practice, fig.
\ref{RepresentationExample} shows the end result of transforming a
simple Python codebase consisting of two modules. Note how a tree is
formed akin to simplified abstract syntax tree -- this is instrumental
in allowing the comparison logic to be recursively defined later
(\ref{Logic}).

\begin{minipage}[t]{0.5\textwidth}
\noindent\textbf{\textit{moduleA.py}}:\\
\begin{lstlisting}
def accept_foo_bar(foo, bar):
    pass

class ThingDoer(object):
    THING_QUOTA = 5

    def do_thing():
        pass
\end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\noindent\textbf{\textit{moduleB.py}}:\\
\begin{lstlisting}
class Discombobulator(object):
    def discombobulate(thingimajig):
        raise self.Discombobulated()

    class Discombobulated(Exception):
        pass
\end{lstlisting}
\end{minipage}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/dot/reprexample}
\caption{CAPIR of a codebase consisting of two modules}
\label{RepresentationExample}
\end{figure}
\clearpage

\subsection{Handlers for programming languages}
\label{LanguageHandlers}

The conversion of source code into CAPIR is delegated to separate
modules that expose a single function to the rest of the program:

\begin{center}
\begin{tabular}{c}
\begin{lstlisting}
def codebase_to_units(location):
    ...
\end{lstlisting}
\end{tabular}
\end{center}


This function takes in a location on disk where some version of a
repository is checked out, and returns a list of units that represent
every top-level entity found in the codebase -- e.g. for Python, that
would be individual \texttt{.py} files.

Generally, all current implementations of that function follow a
similar pattern in how they work:

\begin{enumerate}
\item Recursively walk the file system tree at \texttt{location} and
filter file names of interest with some regular expression -- e.g.
only \texttt{.py} files are considered, but not if they begin with \texttt{test-}.
\item For every file of interest, convert it to a CAPIR unit in some
way appropriate for the language.
\item Return the list of units.
\end{enumerate}

This workflow is not enforced and it is completely up to the handler
to decide how to implement the function and the CAPIR conversion.
Sometimes it is viable to have more than one handler per language --
see \ref{JavaHandlers}.

Autobump comes with handlers for three languages -- Python,
Java and Clojure. The rationale for picking those is that they are
different from one another by characteristics that matter when trying
to convert source code into the internal API representation. Having a
handler for a dynamically typed, interpreted language like Python
would make it easier to write one for a similar language, like Ruby,
in the future.

\subsubsection{Transforming source code into CAPIR}

Each language handler, as part of its implementation, essentially has
to have a function that maps a source file to a CAPIR unit. In
general, there are two convenient ways to achieve this: through
introspection, and through inspecting the abstract syntax tree.
Introspection would usually yield better results, but it requires
first compiling the library. In dynamic languages, there is virtually no
difference between using introspection and the AST.

% TODO: Dynamic?
% TODO: Not true, what about method generation?

\subsubsection{Type systems}
\label{TypeSystems}

In \ref{BreakingChanges} we found that changing the type of a public
entity to a non-compatible one should be considered a breaking change.
However, what it means for two types to be compatible varies. Python,
Java and Clojure were chosen as candidate target languages for this
tool partly because of their different type systems.

In CAPIR, the only thing required of a \textbf{Type} is to be able to
identify whether it is compatible or not with another type. We are not
interested in whether or not they are the same type, or what those
types actually are\footnote{The point is that what the types actually
are is irrelevant to determining the next semantic version. It may be
of interest only to Autobump's secondary function, that is generating
changelogs}.

In the following sections on the individual language handlers, we
discuss how the three languages determine compatibility of types.

\subsubsection{Python}

The Python handler uses the AST. It takes advantage of
Python's ``batteries included'' approach which bundles all sorts of
useful libraries along with the core Python distribution, and that
includes a Python parser. Converting to CAPIR is a simple matter of
opening each source file as text, parsing it and then walking the
resulting tree to produce the appropriate CAPIR structures for each node.

% TODO: Nominal vs structural
% TODO: Example?

Early iterations of the Python handler used introspection. It used to
import source files dynamically, which causes them to be compiled and
made available to the whole program. Then, the individual items in the
module were visited to generate their CAPIR counterparts. This led to
several problems. It's not guaranteed that Autobump runs in the same
environment as the library, so missing dependencies would halt the
whole process. But probably the biggest problem is that this approach
essentially evaluates the code it comes across -- after all, in order
to perform introspection on a function for example, a \texttt{def} has
to be evaluated first. If there is top-level code that has side
effects it will get executed along with the function definitions.

We don't lose out on any important information for CAPIR using just
the AST in Python, so that is why we prefer it to introspection in
this case.

% TODO: Rewrite!!!!!!!

Python, by design and philosophy, encourages the use of duck typing.
When an operation is performed on an object, only the compatiblity of
that object with the operation is considered, not the actual type of
the object. This is a form of runtime structural typing (as opposed to
nominal typing) -- the structure of the object is what's important,
not what it is called.

Consider the following trivial example:

\begin{center}
\begin{tabular}{c}
\begin{lstlisting}
def operate(obj):
    obj.operation()
\end{lstlisting}
\end{tabular}
\end{center}

The contract that Python enforces is that \texttt{obj} has an
implementation of the method \texttt{operation}. It makes no
difference what the type of \texttt{obj} is -- the program will not
fail as long as the parameter passed into \texttt{operate} implements
\texttt{operation}.

If the only thing that matters are the accessed properties and methods
of an object to determine whether the program fails or not, this makes
the task of figuring out how the types of parameters have changed more
difficult. It would be unnecessary and even misleading to attempt to
look up usage of a function inside the project itself, for example in
tests. We may find that all parameters passed into the function are
instances of a particular class, but client code absolutely could pass
in objects of a different type that happen to have the same properties
and methods as needed by the function.

% TODO: What about types of entities?

A better choice is to consider two sets $S_a$ and $S_b$ -- the sets of
all properties and methods requested from the object in the two
versions we want to compare, $S_a$ being the earlier version chronologically.
If $S_a \supseteq S_b$, then we can safely claim that there was no
breaking change as far as the type of the object is concerned. After
all, if client code was passing an object successfully into the
earlier version of the function, then it should continue to do so
because either the set hasn't changed, or some of the expected
properties and methods are no longer expected.

However, if $S_a \subset S_b$, then there are additional properties
and methods that the later version expects. It's not necessary that
client code breaks in this case, but it is definitely a possibility.
Likewise in all other less likely scenarios, like $S_a$ and $S_b$
being neither super nor subsets of one another.

Consider the following example:

\begin{minipage}[t]{0.5\textwidth}
\noindent\textbf{\textit{Variant A}}:\\
\begin{lstlisting}
def func(obj):
    obj.method1()
    obj.method2()
\end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\noindent\textbf{\textit{Variant B}}:\\
\begin{lstlisting}
def func(obj):
    obj.method1()
\end{lstlisting}
\end{minipage}

In this case, because \texttt{method2} is no longer expected in
Variant B, $S_a \supset S_b$ and we safely claim that there is no
breaking change.

Depending on whether the configuration option \texttt{Python/Use
Structural Typing} is set, Autobump can examine the use of a parameter
inside a function and determine those two sets. Afterwards, it takes
the pessimistic view that unless $S_a \supseteq S_b$ there was
definitely a breaking change and bumps the version number accordingly.

Depending on the nature of the project, this behaviour can be a
hinderance and yield lots of false positivies. That is why it is
configurable. If the option is disabled, then the Python handlers
never considers the types of entities and assumes them to be compatible.

\subsubsection{Java}
\label{JavaHandlers}

\subsubsection{Clojure}

The Clojure handler takes advantage of the language's
homoiconicity. That is, programs in the language are represented using
the language's own data structures. Being a Lisp variant, Clojure
programs consist mostly of lists and vectors.

% TODO: Clojure primer?

\subsection{Handlers for version control systems}
\label{VCSHandlers}

\subsection{Comparison logic}
\label{Logic}

After the language handler has run twice and generated two CAPIR
trees, the comparison algorithm can be invoked to find the differences
between the two variants. It itself is fairly straightforward and
defined recursively:

\begin{algorithm}
\caption{CAPIR Comparison Algorithm}
\begin{algorithmic}[1]
% TODO: Better name instead of 'comparisons'
% TODO: Member instead of attribute
\Procedure{Compare}{A, B}
\State $assert(type(A)$ is $type(B))$
\State $comparators \gets \text{map of the form }(member \rightarrow function)$
\For{$member, comparator$ in $comparators$}
\If{$member$ in $A$}
$report(comparator(A[member], B[member]))$
\EndIf
\EndFor
\State $all \gets members(A) \cup members(B)$
\For{$member$ in $all$}
\If {$member$ not in $A$ and $member$ in $B$}
$report(\detokenize{ENTITY_INTRODUCED})$
\EndIf
\If {$member$ in $A$ and $member$ not in $B$}
$report(\detokenize{ENTITY_REMOVED})$
\EndIf
\EndFor
\State $common \gets members(A) \cap members(B)$
\For{$member$ in $common$}
$compare(A[member], B[member])$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

In short, the algorithm compares two entities by examining their list
of attributes and runs specific comparator functions on known
bottom-level members -- such as types and signatures. After that is
done, it checks for members that are missing from either variant and
recurses into members that are present in the intersection of both entities.

Note that this procedure technically compares entities, not trees. To
kick off the comparison of two trees it's necessary to wrap both in a
CAPIR unit, so that the first level of recursion actually looks at the
top-level units in both codebases.

$report$ is a simple function that takes care of adding a change to
the changelog and checking whether the latest reported change is more
notable than the most notable so far. For example, reporting a feature
addition after several patches would cause a feature addition to be
the most notable change. Note that nothing can top a breaking change.
Had Autobump only concerned itself with bumping the version number,
the algorithm could stop at finding a breaking change. However,
because we want to generate a changelog as well, we carry on looking
for changes even though the version number itself is clear at this point.

\section{User interface}

\subsection{Integration with other development tools}

\chapter{Testing}

In this chapter, we look at how we demonstrate that Autobump indeed
works and what testing techniques were used throughout development.

\section{Unit testing}

\section{Acceptance testing}

% TODO: Representation of a Git repository in a text form.

\chapter{Evaluation}

In this chapter, we try to use Autobump with real-world libraries
written in the different target languages. We look at whether Autobump
automatically produces similar or identical version numbers compared
to the ones assigned manually by the developers and interpret our findings.

% TODO: Evaluation was continuous in nature

\section{Evaluation against real-world projects}

% TODO: Evaluate with students in lab as well?

\chapter{Conclusion}

\begin{appendices}

\chapter{Instructions for building and running \textit{autobump}}

Autobump requires at least Python 3.5. Other than that, it has no
dependencies. A typical Python setup involving a virtual environment
is not necessary.

\section{Building}

As it is written in Python, autobump does not have a build step. The
\texttt{Makefile} found in the repository has the following targets:

\begin{itemize}
\item \texttt{make test} runs the test suite.
\item \texttt{make lint} runs the linter \texttt{flake8} (or the one
specified as \texttt{LINTER}) against all source code.
\item \texttt{make all} or \texttt{make} runs all of the above.
\end{itemize}

\section{Usage}

\end{appendices}

%% Bibliography
\bibliographystyle{ieeetr}
\bibliography{dissertation}

\end{document}

% The below list of file-local variables enables
% auto filling of text and completely nukes all indentation
% except for code blocks.

% Local Variables:
% eval: (auto-fill-mode 1)
% sentence-end-double-space: nil
% LaTeX-indent-environment-list: (("verbatim" current-indentation) ("lstlisting" current-indentation))
% TeX-brace-indent-level: 0
% LaTeX-indent-level: 0
% LaTeX-item-indent: 0
% tex-indent-basic: 0
% tex-indent-item: 0
% tex-indent-arg: 0
% End:
