\pdfoutput=1

\documentclass{l4proj}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{microtype}
\usepackage{epigraph}
\usepackage{enumerate}
\usepackage{url}
\usepackage{breakurl}
\usepackage[breaklinks]{hyperref}
\usepackage{fancyvrb}
\def\UrlBreaks{\do\/\do-}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage[dvipsnames]{xcolor}
\usepackage{listings}
\hypersetup{
colorlinks,
linkcolor={red!50!black},
citecolor={blue!50!black},
urlcolor={blue!80!black}
}
\setlength{\belowcaptionskip}{17pt plus 3pt minus 2pt}

\newcommand\textttd[1]{\texttt{\detokenize{#1}}}
\newenvironment{halfmini}[1]
{
\begin{minipage}[t]{0.5\textwidth}
\noindent\textbf{\textit{#1}}\\
}
{
\end{minipage}
}

% Source listings
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{10}
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{10}
\DeclareFixedFont{\ttbs}{T1}{txtt}{bx}{n}{8}
\DeclareFixedFont{\ttms}{T1}{txtt}{m}{n}{8}
\DeclareFixedFont{\ttbt}{T1}{txtt}{bx}{n}{7}
\DeclareFixedFont{\ttmt}{T1}{txtt}{m}{n}{7}
\definecolor{keyword}{HTML}{8C0D40}
\definecolor{name}{HTML}{000F55}
\newcommand\genericstyle{\lstset{basicstyle=\ttm}}
\newcommand\codeinline[1]{{\genericstyle\lstinline!#1!}}
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},
keywordstyle=\ttb\color{keyword},
emph={__init__,ThingDoer,accept_foo_bar,THING_QUOTA,do_thing,Discombobulator,discombobulate,
Discombobulated,func,capir_at_commit,get_commit,codebase_to_units},
emphstyle=\ttb\color{name},
stringstyle=\color{PineGreen},
rulecolor=\color{gray},
showstringspaces=false
}}
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}
\newcommand\javastyle{\lstset{
language=Java,
basicstyle=\ttm,
otherkeywords={},
keywordstyle=\ttb\color{keyword},
emph={method,MyClass,OtherClass,somePackage,YetAnotherClass,SomeClass,InnerClass,field,String,Integer},
emphstyle=\ttb\color{name},
stringstyle=\color{PineGreen},
rulecolor=\color{gray},
showstringspaces=false
}}
\lstnewenvironment{java}[1][]
{
\javastyle
\lstset{#1}
}
{}
\newcommand\javainline[1]{{\javastyle\lstinline!#1!}}
\newcommand\clojurestyle{\lstset{
language=Lisp,
basicstyle=\ttm,
otherkeywords={defn,defn-,macroexpand,macroexpand-all},
keywordstyle=\ttb\color{keyword},
emph={lib.core,constant,func,get-nil},
emphstyle=\ttb\color{name},
stringstyle=\color{PineGreen},
rulecolor=\color{gray},
showstringspaces=false
}}
\lstnewenvironment{clojure}[1][]
{
\clojurestyle
\lstset{#1}
}
{}
\newcommand\clojureinline[1]{{\clojurestyle\lstinline!#1!}}
\newcommand\clojureinlinedef{{\clojurestyle\lstset{otherkeywords=def}\lstinline!def!\enspace}}
\newcommand\xmlstyle{\lstset{
language=XML,
basicstyle=\ttm,
otherkeywords={},
keywordstyle=\ttb\color{keyword},
emph={},
emphstyle=\ttb\color{name},
stringstyle=\color{PineGreen},
rulecolor=\color{gray},
showstringspaces=false
}}
\lstnewenvironment{xml}[1][]
{
\xmlstyle
\lstset{#1}
}
{}
\newcommand\xmlinline[1]{{\xmlstyle\lstinline!#1!}}

% Body

% TODO: inconsistent british/american spelling
% TODO: label after caption!

\begin{document}
\title{Automatically proposing next semantic version of a project by \\
inspecting changes to the codebase}
\author{Christian Shtarkov}
\date{\today}
\maketitle

\begin{abstract}
In software development, assigning a version number
to each release is crucial. It allows end users and other developers
to identify that a software package has changed from a previous
iteration. It is especially important when considering libraries and
frameworks, where changes can greatly affect applications built on top
of them. \\ Different policies exist dictating how to assign version
numbers, one of the most popular being \textit{Semantic
Versioning}, where the version gives
information about how exactly the software package has changed.
\\\\
Regardless of
policy, assigning version numbers is typically done manually and
leaves room for human mistakes or deviations from an established
scheme. This project proposes the tool \textit{Autobump} that automatically
inspects changes to a codebase and suggests the next version number
according to semantic versioning.
\end{abstract}

\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
I would like to thank my project supervisor -- Dr. Tim Storer -- for
proposing the original idea behind this project and for his continuous
support throughout the academic year.

\noindent Furthermore, I would like to show appreciation for my fellow students
Pavel Dikov, Alex Leet, Hamza Tanveer and David Benicek for the
exchange of ideas that went on during our weekly project supervision
meetings.
\end{abstract}

\educationalconsent

\tableofcontents

%% Contents
\chapter{Introduction}
\pagenumbering{arabic}

\epigraph{The real world is a messy place; thereâ€™s nothing we can do
about that but be vigilant.}{Tom Preston-Werner}

This chapter discusses the background of software versioning and more
specifically, semantic versioning of software.
The overall aim of the project is briefly stated and then an outline
of the rest of the document is given.

\section{Background}

\subsection{Software versioning}
Software versioning is the practice of assigning nominal or numerical
identifiers to subsequent iterations of a software package. Every
variant of a piece of software is expected to have a different
(however small the difference) set of functionality, and versioning
allows for finding out how exactly a piece of software is expected to
behave. In practice, all non-trivial software is versioned in one way
or another.

Versioning can have different levels of granularity. Revision control
systems, such as Git or Subversion, allow developers to introduce
small incremental changes to a codebase. After every change, the new
state of the codebase is given a unique identifier and made
recoverable. For example, Subversion assigns a single number to every
revision starting at 1 and increasing in chronological order. This
allows for many productivity-oriented features, such as reverting code
to a previous state, proving that a particular changeset is
responsible for the introduction of a bug, or identifying differences
between divergent branches of the same initial piece of code.

On a macro scale, releases of software packages are also versioned.
Because releases happen much less often than small changes, versions
also change much less often and follow different policies. The de
facto standard are sequence-based schemes where each version
identifier is a tuple consisting of numbers or letters separated by
dots -- e.g. \textit{1.3.2} or \textit{10.6-rc1}. The vast majority of
computer software follows a scheme of this type. It is usually implied
that the further to the left an element of the tuple is, the more
significant it is. What ``significant'' means is not well-defined and
depends on the kind of software that is being versioned. Comparing
versions \textit{5.0} and \textit{6.0} of a Web browser may imply that
the latter is more advanced and has new features, whereas comparing
the same versions of a library may imply that client code using the
former is incompatible with the latter. In general, incrementing
versions in this format follows these rules:
\begin{enumerate}[(i)]
\item Incrementing any non-final element ``resets'' all elements to
the right, i.e. bumping the middle element of \textit{3.5.4} results
in \textit{3.6.0}.
\item Incrementing a version which has a label attached at the end
necessitates either changing the label or droping it, i.e.
\textit{3.5.4-beta} $\rightarrow$ \textit{3.5.4} or \textit{3.5.4-rc1}
$\rightarrow $ \textit{3.5.4-rc2}.
\end{enumerate}

Note that software can be versioned by more than one scheme
simultaneously -- it is a
typical scenario that revision numbers are used by developers
internally and a sequence-based scheme is used for public releases.
Large consumer-facing software with long release cycles, like
operating systems, may use a codename or date-based scheme for
versioning public releases -- e.g. \textit{Ubuntu 14.04 (Trusty Tahr)}
which was released in April of 2014 happens to use both. Those
typically serve a marketing purpose rather than provide any technical
benefit.

\subsection{Semantic versioning}

In software development, it is a widespread practice to leverage
pre-existing code in the form of libraries or
frameworks\footnote{Those are the most common use cases for one piece
of software interfacing with another. The same argument also applies
to philosophically similar use cases, like invoking command-line tools
such as \textit{sed} or \textit{awk} from a script.} to avoid
duplication of effort.

The way that client code usually interacts with these libraries is
through a publically\footnote{There is a distinction between an
internal and public API in that the internal is meant for parts of the
library interacting with other parts. The public API is a superset of
that, exposing functionality to end users without allowing them to
meddle in the internals of the program. In this dissertation, we
always refer to the public API.} declared interface known as the
\textit{application program interface} (API). In some languages, such
as C, depending on how the vendor provides the library a binary
interface may be used instead (\textit{application binary interface},
ABI), but the general idea of a publically known interface remains the
same. Once client software starts using a library, its further
development is tied to that library, which is now a
\textit{dependency}.

Given that client applications and the libraries they depend on have
separate development cycles, a problem arises in determining whether a
client application is compatible with a different version of the same
library. In software development, backwards compatibility is often
valued because it increases confidence and reduces cost when upgrading
the dependencies of a software package.

Virtually all well-maintained libraries try to provide a \textit{changelog},
which is a compiled list of changes since the last version. They usually
place emphasis on changes that are breaking, as that is most
concerning to developers of client software. Changelogs can be
manually put together, or partially generated by tools such as
\textit{JDiff} \cite{JDiff}. Another common solution is to take a
sequence-based scheme and attach meaning to the positions in order to
measure degree of compatibility.

Semantic versioning \cite{SemanticVersioning} (Semver) is a strict
formalization of such a scheme, where the format and significance of
each position are well-defined:

\begin{center}
$\mathrm{Major.Minor.Patch}(-\mathrm{Prerelease})^?(+\mathrm{Build})^?$
\end{center}

\begin{itemize}
\item \textit{Major} increments when incompatible changes to the
public API have been made and it is possible for interfacing software
to no longer work.
\item \textit{Minor} increments when new features have been added in a
backwards-compatible manner.
\item \textit{Patch} increments when bugs have been fixed in a
backwards-compatible manner.
\item \textit{Prerelease} is an optional label indicating that the
version does not yet meet the compatibility requirements imposed by
the first three numbers.
\item \textit{Build} is another optional label indicating the
corresponding internal version numbers, e.g. the one used by the
revision control system.
\end{itemize}

Semver defines the public API to start at version \textit{1.0.0}.
Versions before that expect the minor and patch version numbers to
change radically and frequently, so their significance is diminished.
For all intents and purposes, software that is at major version
\textit{0} does not yet follow semantic versioning.
If the versioning rules had started applying from the first ever
release of the project, either the developers would be discouraged
from versioning unstable work or version numbers would balloon.

Semver, explicitly or implicitly, is by far the most popular
versioning scheme. As its specification claims, ``these rules are
based on but not necessarily limited to pre-existing widespread common
practices in use in both closed and open-source software.''

\subsubsection{Benefits}

The Semver specification outlines as motivation that the scheme guards
against a situation known as \textit{dependency hell}. Dependency hell
is when one or both of version lock and version promiscuity occur.
Version lock can happen when dependency requirements are specified too
tightly and the costs of providing the dependencies that satisfy them
start to increase over time. Version promiscuity is the overestimation
of what versions of a dependency are backwards-compatible with whatever
the client application is using at the moment. Specifying that version
``\textit{2.0.0} or better'' is needed can be dangerous if
\textit{2.1.0} stops being backwards-compatible.

Semver alleviates dependency hell by communicating how software has
changed in a new version compared to the previous one by the version
number alone. This allows for a sweet spot in specifying dependency
requirements that are neither too restrictive, nor overly liberal.
It increases confidence and reduces the risk of loss.

This scheme is especially important for automatic dependency
management systems, like ones found in package managers or build
systems. Even if version numbers bore no semantic meaning, humans
would still be able read changelogs and make decisions about whether
dependencies can be safely upgraded or whether their code should be
changed. However, automated systems are much better informed by
looking at a version number.

\subsubsection{Application}

Semver only provides these benefits when it is applied consistently.
If applied haphazardly, then two versions cannot be compared reliably
and decisions made by simply examining version numbers are risky. If
business-critical software has some dependecy which is guaranteed to
follow semantic versioning and a new patch version is released, the
decision to upgrade is a no-brainer: after all, this patch may resolve
bugs that affect clients. However, this confidence can backfire
even if a single patch release results in a breaking change that
causes the software to fail, or even worse -- behave in unexpected ways.
This leads to a trade-off between confidence and friction when
updating dependencies. This dissertation argues that this situation is
undesirable and can be alleviated.

In practice, semantic versioning is applied poorly. A
study \cite{SemverMaven} of more than 22,000 libraries in the Maven \cite{Maven}
repository found that there was virtually no difference in the
frequency of breaking changes in major and minor version increments.
It also found that this has a significant impact on client
applications, mostly by causing compilation errors. The study goes on
to suggest that similar results could be found in other ecosystems.

The study suggests that one possible explanation of this low adherence
could be the presence of ``internal'' packages. Those are parts of the
library designated as internal only in name, when they are actually
publically available in order to work around Java's access modifiers.
Although they may not be intended for public use, they are certainly
available to client code. An additional explanation may be that
libraries that poorly follow Semver are intended for use only by the
developers themselves. However, the authors argue, such libraries
should have probably never been released into a public repository.

Another possible reason might be that some authors are reluctant to follow
semantic versioning, or may be completely unaware that there is a
formalisation of it. Just because a versioning scheme resembles Semver
syntactically does not immediately impart any meaning on the
individual numbers.

Even if authors do intend to follow Semver, their own subjectivity may
get in the way. Developers may be inclined to believe that making a
breaking change to an obscure function that, to their knowledge, is
not being used by anyone is not worthy of being designated as
``breaking''. However, Semver makes no such allowance. It strictly
specifies that any breaking change to the public API warrants
the incrementation of the major version number, regardless of how
insignificant it may appear to be. That applies even for functionality
designated as deprecated in a previous version -- although the same
study found that deprecation annotations were barely used and removal
of functionality is usually sudden.

Even if authors were making a small breaking change to the public API,
and were fully aware that Semver stipulates bumping the major version,
they may still be inclined not to do so. Doing that frequently would
mean the major version number skyrocketing, which for purely
superficial reasons can be undesirable. However, it's important to
realise that this would happen because of development methodology and
not the versioning scheme itself. As the Semver specification
mentions, ``this is a question of responsible development and
foresight. Incompatible changes should not be introduced lightly to
software that has a lot of dependent code.''

Chapter \ref{Evaluation} looks at concrete examples of when
library developers fail to identify breaking changes by bumping the
major version number and examines whether the above reasons apply.

\subsubsection{Criticism}

Semantic versioning has received criticism for its inflexible way of
assigning version numbers. Most notably, the essay ``Why Semantic
Versioning Isn't'' \cite{WhySemanticVersioningIsnt} by CoffeeScript
creator Jeremy Ashkenas refers to the scheme as ``romantic
versioning'' because of its alleged failure in real-world scenarios.

The core argument against Semver is that it attempts to compress too
much information into a few numbers. Semver does not acknowledge any
degree of a breaking change, it is a binary decision -- breaking or not.
This fails to reflect the number of affected users, or the difficulty
in porting old software to use the new version of the dependency.

The essay argues that Semver is a false promise, because the majority
of projects do not officially follow it or do so inconsistently.
However, this is a problem of adoption, not something inherent to the
scheme itself.

\clearpage
\section{Aim}

It has been identified that the reluctance or inconsistency of
applying a strict versioning scheme like Semver is a problem in
software engineering that costs development time and disallows
automated systems like package managers from making decisions about
software upgrades.

This project proposes the automated assignment of version numbers
as a possible solution. The goal is to develop a tool that given a
previous iteration of a project and its version, can automatically
propose the version of the following iteration. This can increase the
level of adherence to semantic versioning in two ways:

\begin{enumerate}[(i)]
\item Taking away the responsibility of providing a new version number
from software developers, leaving less room for human error.
\item Examining the prime source of semantic changes to the project --
the codebase itself.
\end{enumerate}

As an additional requirement, this dissertation examines a
language-agnostic solution to this problem. The resulting tool is then
to be evaluated against the history of real-world libraries across
different languages.

\section{Outline}

The rest of this dissertation is comprised of the following chapters
and appendices:

% TODO: chapters go here
\noindent Chapter \ref{PreviousWork} -- \textbf{Previous work} \\
Goes into existing projects that to some extent solve the problem
of automatically assigning version numbers and the approaches that
they use.

\noindent Chapter \ref{Requirements} -- \textbf{Requirements} \\
Lays out the approach that Autobump is going to use for
determining version numbers and the set of features that would make
the tool useful.

\noindent Chapter \ref{Implementation} -- \textbf{Implementation} \\
Discusses how Autobump works in great detail, both internally and
in terms of being exposed to the end user.
% TODO: rewrite?

\noindent Chapter \ref{Testing} -- \textbf{Testing} \\
Goes into the techniques used to show that Autobump indeed works
as advertised.

\noindent Chapter \ref{Evaluation} -- \textbf{Evaluation} \\
% TODO: does it though?
Evaluates Autobump's multi-language design and ``replays'' the history
of different libraries across the target languages to find out whether
a tool like Autobump would have been beneficial in their development.

\noindent Chapter \ref{Conclusion} -- \textbf{Conclusion} \\
Summarizes what was accomplished by this project and proposes future work.
\\

\noindent Appendix \ref{AutobumpInPractice} -- \textbf{Running
Autobump in practice} \\
Gives practical tips on how to run Autobump in the real world.

\noindent Appendix \ref{ClojurePrimer} -- \textbf{Primer on Clojure}
\\
Explains some of Clojure's features that relate to this project.

\chapter{Previous work}
\label{PreviousWork}

Automatically assigning version numbers is not a new idea and projects
exist that already do that to some extent.
This chapter discusses existing tools that claim to do that. It also
examines the various ways they approach it, why they are not as
successful as it would be expected and draw ideas for how this project
might implement a similar tool.

\section{semantic-release}

\textit{semantic-release} \cite{SemanticRelease} is a Javascript tool
for ``fully automated package publishing''. It can do a range of
functions related to automated publishing of packages in the
Javascript ecosystem through the Node Package Manager, but of interest
are the two ways it can deal with proposing a new version number.

One way is that it tries to interpret Git commit messages and from
that determines what the next version number should be. A minor
(feature) release for example would be indicated by a commit message
of the following format:

\begin{center}
\texttt{feat(pencil): add 'graphiteWidth' option}
\end{center}

This approach suffers from the same drawback as incrementing the
version number manually does -- it has to be done consistently.
Not conforming to the format even once would mean that the tool would
fail to identify that the version should be bumped.

Another way is by employing an additional tool called
\textit{cracks} \cite{Cracks}. According to the official description
``this module can automatically detect breaking changes by running the
test suite of your last-release against the current codebase.''
This is generally a good approach -- ideally
the test suite of the previous version should pass if the new one is
not meant to be a major release. However, it raises some issues.

\begin{itemize}
\item It could be that the library in question has whitebox unit
tests. If they start failing, that does not necessarily constitute a
breaking change, because while the internal implementation of those
methods may have changed, the public API could still be the same. Cracks has
no way of differentiating blackbox from whitebox tests unless manually
specified.
\item The unit tests may not cover the public API in its entirety.
Modifying the signature of a public method, for example, that's not
covered by the tests may go completely unnoticed.
\item The library may be in a stage of development such that it has no
unit tests, or they cover a very small portion of the public API, yet
the authors still want to use semantic versioning.
\end{itemize}

\section{GitVersion}

\textit{GitVersion} \cite{GitVersion} ``looks at your Git history and
works out the semantic version of the commit being built.'' By knowing
in advance what workflow contributors to the repository are using, it
can determine what component of the version number ought to be bumped.
Previous versions of GitVersion have implicit knowledge of the
workflows \textit{GitFlow} \cite{GitFlow} and \textit{GitHubFlow}
\cite{GitHubFlow}, but the current one (\textit{3.0}) at the time of
writing can be configured to work with more custom workflows.

Consider an example usage of GitVersion on a history of commits.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/dot/gitversionexample}
\caption{Example Git commit history}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\textbf{Commit} & \textbf{Version} & \textbf{Explanation}                              \\
0      & 1.0.0                 & First commit that has a version.                      \\
1      & 1.0.1+1               & Patch was introduced, and one commit since then.      \\
2      & 1.0.1-feature-foo.1+1 & Feature was introduced, and one commit since then.    \\
3      & 1.0.1+3               & Three commits since the last patch.                   \\
4      & 1.1.0-rc.1            & First release candidate for the new feature.          \\
5      & 1.1.0                 & Feature branch was merged.                            \\
6      & 1.1.1+1               & Patch was introduced, and one commit since then.      \\
7      & 1.1.1+2               & Patch was introduced, and two commits since then.     \\
8      & 2.0.0-rc.1            & First release candidate for the new breaking changes. \\
9      & 2.0.0                 & Breaking release branch was merged.
\end{tabular}
\caption{Proposed semantic version numbers by GitVersion, if every
commit were to be released}
\end{table}

Notice that similarly to semantic-release, GitVersion relies on user
input -- in this case, branch names. This results in the same set of
drawbacks, with the addition that GitVersion is heavily dependent on
Git as a version control system and cannot trivially be ported to
another one. The naming scheme has to be applied consistently, and
forces contributors to all use the same one\footnote{Arguably, that
can be a good thing. However, such requirements should not be imposed
by software like this.} . The choice of naming scheme can be somewhat
alleviated, as the workflow GitVersion expects is highly configurable
\cite{GitVersionConfiguration}.

\section{Endjin.Assembly.ChangeDetection}

\textit{Endjin.Assembly.ChangeDetection} \cite{Endjin} is a
proof-of-concept project that takes a very different and less
error-prone approach. It can identify breaking changes in .NET
assemblies by looking inside the assembly itself. It does this
by examining any changes made to public types in the assembly, such as
removing them or changing their signature. It essentially determines
the binary compatibility of two .NET assemblies.

This way of proposing a version number is preferred, because it
imposes no additional requirements on contributors to the repository
and does not rely on user input to relay changes to the public API.
Instead, it inspects it itself.

However, Endjin.Assembly.ChangeDetection is not really a fully-fledged
tool, but an experiment applicable to only one particular ecosystem.

\section{elm-package}

\textit{elm-package} \cite{ElmPackage} is the official package manager
of the language Elm -- a purely functional, domain-specific
language for creating browser-based user interfaces. Elm is heavily
influenced by Haskell, and elm-package itself is written entirely in
Haskell.

As part of the package publishing process, elm-package automatically
bumps the version number of the package. It never claims outright that
it follows semantic versioning, but the rules it specifies are very
reminiscent of it \cite{ElmPackageVR}:

\begin{itemize}
\item Versions all have exactly three parts: \codeinline{MAJOR.MINOR.PATCH}
\item All packages start with initial version \textit{1.0.0}
\item Versions are incremented based on how the API changes:
\begin{itemize}
\item \codeinline{PATCH} -- the API is the same, no risk of breaking code
\item \codeinline{MINOR} -- values have been added, existing values are unchanged
\item \codeinline{MAJOR} -- existing values have been changed or removed
\end{itemize}
\end{itemize}

The most notable change from semantic versioning is that elm-package
disallows versions smaller than \textit{1.0.0}. Semver claims that \textit{1.0.0}
defines the public API, so it would make sense for a tool like this to
disallow versions smaller than that. If the previously published
version of the package is \textit{0.9.0} and the new changeset is breaking,
elm-package would have no way of knowing whether the next one should
be \textit{0.9.1}, \textit{0.10.0} or \textit{1.0.0}.

\section{Java-specific tooling}

\subsection{JDiff}

\textit{JDiff} \cite{JDiff} is a tool that does not assign version
numbers, but instead documents changes between two variants of a Java API.
It then goes on to generate an HTML report of the differences, to act
as a supplement to the changelog. It not only looks at the API itself,
but also inspects and notes differences in accompanying documentation
strings. Those may indicate a change in the behaviour of some functionality,
even if its interface remains the same.

JDiff is intended to be used in conjuction with the build system Ant
\cite{Ant}, where an \textit{old} and a \textit{new} project have to
be defined so that JDiff knows what to compare. It requires that the
project is compiled (twice) and that its dependencies are available in
Java's \codeinline{CLASSPATH}, because it uses the Java reflection API
which allows for compiled code to be loaded and inspected at runtime.

At the time of writing, the tool seems to be somewhat outdated with
the last release being JDiff \textit{1.1.1} from November 2008.

\subsection{Clirr}

\textit{Clirr} \cite{Clirr} is another tool that does not explicitly
assign a version number, but instead only lists changes and warns
about loss of backward compatibility. It was used in the study of the
Maven repository \cite{SemverMaven} to determine compatibility between
different versions of published Jar files, which are bundles of
compiled code. It also relies on reflection.

It can act as a plugin for the Maven build system and break builds
when incompatible changes are introduced unintentionally.
Additionally, it can be used to generate a report similar to what
JDiff produces.

Just like JDiff, Clirr seems to be relatively out of date as well, the
last release being  \textit{0.6} from September 2005.

\subsection{japicmp}

\textit{japicmp} \cite{Japicmp} measures compatibility between
provided Jar files. It can tie into the build system Maven and
configure it so that it produces Jar files of two subsequent versions
and breaks builds when incompatibilities are detected, just like Clirr
does. Alternatively it can be used as a command-line application to
find out differences between any two given Jar files.

Unlike the other two Java utilities, this one does not use the Java
reflection API. Instead it relies on the external library
\textit{javaassist} \cite{JavaAssist} which allows for bytecode
manipulation at the point of loading external classes
\cite{BytecodeManip}. This is used to circumvent the requirement that
all dependencies of the project are in the \codeinline{CLASSPATH}.
However, the implication is that differences arising from types
defined in external code cannot be identified. Consider for example
replacing the type of a parameter in an otherwise unchanged method to
a superclass of its original type -- all previous client code is
guaranteed to compile fine, because of Java's inheritance rules, but
there's no way that Japicmp would be aware of this.

\chapter{Requirements}
\label{Requirements}

This chapter formalises the approach that the tool for this project,
Autobump, should use to propose version numbers, and formulates the
set of features that should be expected. It also goes into some brief
discussion of non-functional requirements.

\section{Estimating next semantic version through code analysis}
\label{EstimatingNextSemver}

Much like \textit{elm-package}, Autobump looks at how the code itself
has changed to estimate what the next version should be. This requires
little to no manual intervention from the user, no special naming
schemes, and is as reliable as this kind of software can be. In
contrast to these tools, Autobump aims to be much more general and
not tied to one ecosystem or language. For this reason,
ways of abstractly defining breaking changes and feature additions
need to be examined, as well as considering how those definitions can
be applied to different languages.

\subsection{Breaking changes}
\label{BreakingChanges}

% TODO: where do you draw the line? if we say 2 changes are breaking,
% then... which ones
For the definition of breaking change to be useful, it ought to be
strict -- and that is the way Semver approaches it. It defines
\textbf{any} change to the API which makes it incompatible with the
last version as breaking. Determining semantic incompatibilities is
generally undecidable, so this project takes a more strict definition of
``incompatible'' -- namely changes that make the new code technically
incompatible with the old, i.e. it doesn't compile in the case of
static languages, or throws unexpected runtime errors with dynamic
languages. It does not take into account number of affected users,
difficulty of transitioning to the new version, or changes in
behaviour, because those things cannot be possibly known reliably. It
also does not subjectively impart more significance on one change than
another if both are of the same class, as it can be speculated humans
tend to do.

In the spirit of Semver, this projects adopts a technical and
deterministic list of changes to entities that are considered
to be breaking. There is an important assumption that all
public-facing entities (i.e. those accessible externally in a normal
for the language way) are part of the API. There may be a misalignment
between what the library authors consider to be the public API and
what is technically accessible by client code, which is going to make
the authors' predictions overly liberal. Autobump makes the safer
choice of being potentially overly conservative by default, and only
relaxing when configured explicitly to do so.

This dissertation tries to use abstract enough terms so that this list
of changes is applicable for different target languages. Clirr, as a
Java-specific tool, is able identify 43 different API changes. The
table below contains only 6 entries, and yet it can be argued that
that is enough to represent all deltas that Clirr can find. It is
mainly a reporting tool, so a high resolution of changes is necessary
to tell the difference between, say, removing a method from a class
and removing a class entirely. Autobump is intended to automate the
assignment of version numbers, and both of those changes are breaking
-- they can be safely lumped together under ``removing entities''.

\begin{table}[H]
\centering
\begin{tabular}{|p{0.45\linewidth}|p{0.45\linewidth}|}
\hline
\textbf{Change}                                                                               & \textbf{Rationalisation}                                                 \\
\hline
Removing an entity.                                                                           & User applications may be using it.                                       \\
\hline
Changing the type of an entity to an incompatible type.                                       & User applications may be expecting a different type.                     \\
\hline
Adding a parameter with no default value to a function signature.                             & Calls to that function will immediately fail due to a missing parameter. \\
\hline
Removing a parameter from a function signature, regardless of whether it had a default value. & Calls to that function \textit{may} fail due to a missing parameter.     \\
\hline
Removing the default value of a parameter.                                                    & Calls to that function may be relying on the now missing default value.  \\
\hline
\end{tabular}
\caption{List of changes that are considered breaking}
\end{table}

When constructing this list, a small number of assumptions
regarding the target language are made:

\begin{enumerate}[(i)]
\item The language provides some mechanism of grouping functionality
into entities, be it object-orientation, namespacing, modules and so
on. Note that groupings are absolutely allowed to be nested --
consider a Java library which defines a constant inside a class inside
a class. In this case the constant and both classes are considered to
be ``entities''.
\item The language has some notion of parameterized callable pieces of
code, which for simplicity's sake are referred to only as ``functions'',
even though in reality they may be object methods, macros, procedures etc.
Additionally, some of those functions are callable from outside of the
program where they are defined, comprising the majority of the API.
\label{SecondAssumption}
\item The language has a type system which has the notion of binary
compatibility:
\begin{equation}
\forall(T,Y) \in S, \mathrm{compatible}(T,Y) \in \{\mathrm{True},\mathrm{False}\}
\end{equation}
where $S$ is the set of all admissable types in the language and
program. \\

The notion of type compatibility is defined as the type $T$ being able
to replace $Y$ without violating the rules of the language -- e.g. the
Liskov substitution principle\cite{Liskov} in a typical
object-oriented language. Note that this notion is also compatible
with dynamically typed languages where types can not be reasonably
checked by taking $compatible$ to always be true.
\end{enumerate}

There are languages that do not conform at all to these assumptions
(e.g. minimalistic ones such as Brainfuck\cite{Brainfuck}), but these
are also languages where a tool like Autobump would be utterly
unuseful. There are also languages that partially conform. In those
cases, as long as assumption \ref{SecondAssumption} is valid (i.e.
there is an API), the rest can be ignored -- of course making some functionality
of Autobump obsolete -- namely identifying in which parts of
the program the changes have occurred and checking type compatibility.
This project examines the radically different languages
Python, Java and Clojure (section \ref{LanguageHandlers}) and chapter
\ref{Evaluation} discusses whether the assumptions were found to be
reasonable.

\subsection{Feature additions}

Semver specifies that the minor version number should be incremented
when new or improved functionality is added to the software in a
backwards-compatible manner. If no breaking changes are identified as
specified in the previous subsection, it can be safely assumed that if any
features were indeed added, they would be backwards-compatible.

However, while changes to the API can be used to identify breaking
changes with certainty, it is not so with feature additions. The
version of Semver at the time of writing, Semver 2.0.0, is ambiguous
regarding whether the public API can be tied to the addition of
features. At one point it claims that ``backwards compatible API
additions/changes increment the minor version'', however later on it
stipulates that the minor version number ``MAY be incremented if
substantial new functionality or improvements are introduced within
the private code''.

While it would be possible to inspect changes to the private code,
perhaps in terms of how the abstract syntax tree was modified, the
line between a patch change and a feature addition would be quite
fuzzy.

Revisiting the established list of breaking changes and replacing
the destructive aspect of each item with a constructive one results in
the following list:

\begin{itemize}
\item Adding a new entity.
\item Changing the type of an entity into a compatible type.
\item Adding a parameter with a default value to a function signature.
\end{itemize}

Autobump can monitor changes like these and suggest that those are
indicative of a feature addition. Provided that no breaking changes
are found, but some of these are, then it would be appropriate to bump
the minor version number. This is a valid intepretation of the Semver
specification, and it is the approach that \textit{elm-package} takes
\cite{ElmPackageVR} as it relates to Elm: it defines a minor version
bump as ``values have been added, existing values are unchanged''.

Note that while backwards-compatible additions to the API virtually
always indicate a feature addition (at least for a flexible enough
definition of ``feature''), not all feature additions have to be
accompanied by such a change. For example, a major performance
improvement may be notable enough that the authors consider it a
feature.

While Autobump is guaranteed to disagree with authors in some of the
cases where a feature was added because it had no bearing on the API,
this does not devalue its usefulness. The main point of semantic
versioning is to guard against breaking changes, which can be reliably
done just by looking at the API.

\section{Feature set and usage}

% TODO: check tense usage

Autobump is a a tool which given two revisions in a version
controlled project reliably determines what the version of the later
revision should be\footnote{In some contexts, ``version'' and
``revision'' may be identical. This dissertation refers to the
semantic version of a release as ``version'', and the commit
identifier in a VCS as ``revision''}.

Although technically Autobump could be used with any project, this
dissertation focuses on libraries, frameworks and otherwise projects
whose main usefulness comes from the ability to be integrated in
client applications by exposing an API. It is with those types of
projects that semantic versioning is most practical.

Because identifying the version delta requires first identifying a
list of changes to the public API, a secondary function of Autobump is
the generation of a changelog in preparation for a release.

Autobump can be part of a manual or automated workflow of bumping the
version number with every release, generating a changelog, or both. It
is a command-line application with behaviour driven by passing
options, so it can be easily integrated in an automated deployment
system. Another possibility is using it standalone as a sanity check.
Developers can run it to find out if they have unintentionally broken
backwards compatibility with a recent commit.

Unlike other previous tools that accomplish more or less the same
thing, Autobump has attempted to generalise what changes to the API look
like across different languages. As such, Autobump has a common
framework that makes adding support for additional languages less
expensive than developing a new tool from scratch.

\chapter{Implementation}
\label{Implementation}
% TODO: do I explain handling 0.* version numbers?

This chapter looks at how Autobump works internally in terms of its
architecture, as well as how it exposes its functionality to the end
user. Furthermore, it focuses on how implementation details differ for
the different target languages.

\section{Architecture}
\label{Architecture}

Autobump is written mainly in Python and establishes a common
interface and version comparison framework that form the core of the
program. Support for different languages and version control systems
is added on top of that through the use of handlers, which are also
written in Python when appropriate, or are written in another language
and use Python as a middleware language to link back to Autobump.

\begin{figure}
\label{ArchitectureFig}
\centering
\includegraphics[width=\linewidth]{images/dot/architecture}
\caption{\textit{Autobump} architecture}
\end{figure}

In normal operation, the top-level program performs the
following steps:

\begin{enumerate}
\item Identify which version control system the repository is using
and select the appropriate handler. Operations on the repository are
abstracted away by the handler, so that Autobump's other functions are
VCS-agnostic.
\item Identify which two revisions should be compared -- either from
command-line arguments or through an educated guess. An educated guess
consists of comparing the last tagged release of the project with the
latest commit. The version number of the previous release is
determined from the name of tag.
\item Invoke a language handler a total of two times for both
revisions to construct common\footnote{Common between different
source languages, not the two representations.} representations of the
codebase out of the public APIs. If the language handler needs
introspection, this may require building the two revisions of the project.
The nature of this common representation is discussed in \ref{CAPIR}
and the exact mechanism that language handlers use for the conversion
in \ref{LanguageHandlers}.
\item Run the core comparison logic on both representations to find
out how the public API has changed in the later one compared to the
earlier one. This step can generate a changelog and will identify what
the version bump should be. This is discussed in \ref{Logic}.
\item Output the new version number by applying the bump to the old
one. At this point, a changelog may be output as well if requested.
\end{enumerate}

\section{Configuration}

The configuration module lies at the very bottom of Autobump (as seen
in figure \ref{ArchitectureFig}). It made sense that the behaviour of
some components is user-configurable. The exact options that are
available are discussed in later sections, but for a rundown of every
tweakable see appendix \ref{RundownConfigurables}.

When a configurable value of the form \codeinline{Component/Option} is
requested, the module tries to fetch it from three sources and stops
at the first that succeeds:

\begin{enumerate}[(i)]
\item An environment variable named \codeinline{AB_OPTION}. Note that the
component part of the option is missing. This is for brevity, since
environment variables are meant to be used for quickly overriding
options.
\item A configuration file \codeinline{autobump.ini} present in the
working directory, which has a section \codeinline{[Component]} with a
setting of the form \codeinline{[option=...]} nested inside. This matches
the informal standard \codeinline{INI} format for configuration files. The reason
this format is preferred is that Python comes with the module
\pythoninline{configparser} for reading those types of files out of
the box.
\item A default value, hardcoded in the configuration module itself.
\end{enumerate}

To make Autobump configuration sets truly portable (and to assist with
debugging), the command-line option \codeinline{--export-config} is
provided which prints a configuration file corresponding to Autobump's
current configuration.

Additionally, the configuration module provides standard Python
mechanisms for overriding configuration programmatically -- a context
manager and a function decorator, as shown in figures
\ref{ConfigOverrideCM} and \ref{ConfigOverrideD}. Those can be useful
if using Autobump as a library from an external application. For
example, Autobump's own unit tests override configuration options to
make their success or failure not dependent on Autobump's current
configuration.

\begin{figure}[H]
\label{ConfigOverrideCM}
\centering
\caption{Overriding with a context manager}
\begin{tabular}{c}
\begin{python}
from autobump.config import get, config_overrides

with config_overrides({"category": {"option": "value"}}):
    get("category", "option") == "value"
\end{python}
\end{tabular}
\end{figure}

\begin{figure}[H]
\label{ConfigOverrideD}
\centering
\caption{Overriding with a function decorator}
\begin{tabular}{c}
\begin{python}
from autobump.config import get, config_override

@config_override("category", "option", "value")
def this_test_requires_a_particular_option_to_be_set():
    get("category", "option") == "value"
\end{python}
\end{tabular}
\end{figure}

\section{Common representation for APIs}
\label{CAPIR}

Before proceeding with the issue of representing APIs, it is important
to note that for the purposes of this project there is no interest in
any internal APIs software packages might use -- they should not
influence external applications that depend on the packages in question.
There is interest only in the public APIs which are meant for external
use, as changing these indicates an incrementation of the semantic version.

% TODO: public only in java
Most general-purpose languages provide ways to restrict visibility of
entities in the source code (such as classes, functions and so on).
Technically all externally visible entities, usually indicated with the keyword
\javainline{public}, are accessible from client applications. In practice,
however, it may be that not all of these form the intended, official
and documented public API. Faced with this problem, Autobump takes the
safer approach and only considers visibility when determining whether
an entity is part of the public API. The reason for that is that there
is nothing technically restricting client applications from using
visible entities that are otherwise undocumented or not formally
intended for outside use.

By relying on language-specific handlers to extract the public API of
a codebase and convert it into a common representation, the same
comparison logic can be reused. Beside reducing repetitiveness in the
code, this has the additional effect of allowing handlers for new
languages to be ``plugged in'' with virtually no changes to the
program itself.

Choosing an appropriate representation was an important, and
difficult, part of this project. The problem of representing an API
regardless of language is solved for some contexts by
\textit{interface descriptor languages} (IDLs), such as
WSDL\cite{WSDL}, SWIG\cite{SWIG} or Apache Thrift\cite{Thrift}. All of
these are meant for a specific purpose, usually interoperability between
different technologies. While adapting one of these to use in Autobump
was certainly possible, this project opted to use its own model instead for
several reasons.

\begin{itemize}
\item Barely any features are needed except for API representation.
Using an established IDL may be considered overkill.
\item Using an established IDL would mean either importing an external
library as a dependency, or writing a new one just for this
dissertation, and both are undesirable.
\item Should extensions be needed, it would be trivial to extend an
ad-hoc model compared to a third-party one.
\item Autobump's representation of APIs is internal -- the user should
never care about it, so the choice of IDL can ignore any user
experience issues.
\end{itemize}

The representation that Autobump uses (fig. \ref{RepresentationUML})
consists of several entities that are common across public APIs
exposed by different languages. From now on, this model will be
referred to as \textbf{CAPIR} (standing for ``common API
representation'') for the sake of brevity.

\pythoninline{Entity} is a base class representing an entity of
interest found in source code. Python does not enforce
object-orientation so this class is not crucial. It is only present to
complete the taxonomy.

\pythoninline{Type} is a base class that is the basis of the type
systems used by the language handlers. This is discussed in further
detail in \ref{TypeSystems}.

\pythoninline{Unit}s are a recursive data structure that indicate a
general grouping of entities. They can represent a range of grouping
techniques found in different languages -- translation units, modules,
classes, interfaces, mixins, traits etc. In fact, Autobump uses a
single top-level Unit to represent an entire codebase.

\pythoninline{Field}s and \pythoninline{Function}s are found in units.
Fields have a type and typically denote the setting of a variable or a
constant. Functions represent callable pieces of code -- functions,
methods, routines, or whatever the source language calls them. The
names given to these two entities are not ideal, because it reality
they can be used for more general concepts. For example, Clojure
records (described in detail in appendix \ref{ClojurePrimer}) are
intuitively seen as \pythoninline{Unit}s because they indicate a
grouping of entities. However, Clojure is a functional programming
language and they are internally represented with just a few
functions. In this case CAPIR follows the language's representation
and treats those as \pythoninline{Function}s as well.

Every function has one or more \pythoninline{Signature}s which in turn
are lists of \pythoninline{Parameter}s. Parameters have a type and can
have a default value. Note that the \pythoninline{Function}s themselves
don't have return values; instead they can be put as part of the
signature if necessary.

% TODO: shoehorning languages into CAPIR, drawbacks for changelog generation

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/dot/repr}
\caption{Class diagram -- Autobump's public API representation (CAPIR)}
\label{RepresentationUML}
\end{figure}

\clearpage
As an example of what CAPIR looks like in practice, fig.
\ref{RepresentationExample} shows the end result of transforming a
simple Python codebase consisting of two modules. Note how a tree is
formed akin to simplified abstract syntax tree -- this is instrumental
in allowing the comparison logic to be recursively defined later
(\ref{Logic}).

\begin{halfmini}{moduleA.py}
\begin{python}
def accept_foo_bar(foo, bar):
    pass

class ThingDoer(object):
    THING_QUOTA = 5

    def do_thing():
        pass
\end{python}
\end{halfmini}
\begin{halfmini}{moduleB.py}
\begin{python}
class Discombobulator(object):
    def discombobulate(thingimajig):
        raise self.Discombobulated()

    class Discombobulated(Exception):
        pass
\end{python}
\end{halfmini}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/dot/reprexample}
\caption{CAPIR of a codebase consisting of two modules}
\label{RepresentationExample}
\end{figure}
\clearpage

\section{Handlers for programming languages}
\label{LanguageHandlers}

This section discusses the overall idea behind how this project goes
about implementing support for multiple programming languages in the
same tool using the ideas described in section \ref{EstimatingSemver},
and then moves on to the specifics for each supported language.

The conversion of source code into CAPIR is delegated to separate
modules that expose a single function to the rest of the program:

\begin{center}
\begin{tabular}{c}
\begin{python}
def codebase_to_units(location):
    ...
\end{python}
\end{tabular}
\end{center}

This function takes in a location on disk where some version of a
repository is checked out, and returns a list of units that represent
every top-level entity found in the codebase -- e.g. for Python, that
would be individual \codeinline{.py} files.

Generally, all current implementations of that function follow a
similar pattern in how they work:

\begin{enumerate}
\item Recursively walk the file system tree at \pythoninline{location} and
filter file names of interest with some regular expression -- e.g.
only \codeinline{.py} files are considered, but not if they begin with \codeinline{test-}.
\item For every file of interest, convert it to a CAPIR unit in some
way appropriate for the language.
\item Return the list of units.
\end{enumerate}

This workflow is not enforced and it is completely up to the handler
to decide how to implement the function and the CAPIR conversion.
Sometimes it is viable to have more than one handler per language --
see \ref{JavaHandlers}.

Autobump comes with handlers for three languages -- Python,
Java and Clojure. The rationale for picking those is that they are
different from one another by characteristics that matter when trying
to convert source code into the internal API representation. Having a
handler for a dynamically typed, interpreted language like Python
would make it easier to write one for a similar language, like Ruby,
in the future.

% TODO: Table of handlers, build required, so on
% TODO: what language are the handlers in?

\subsection{Transforming source code into CAPIR}

Each language handler, as part of its implementation, essentially has
to have a function that maps a source file to a CAPIR unit. In
general, there are two convenient ways to achieve this: through
introspection, and through inspecting the abstract syntax tree.
Introspection would usually yield better results, but it requires
first compiling the library. In dynamic languages, there is virtually no
difference between using introspection and the AST.

% TODO: Dynamic?
% TODO: Not true, what about method generation?

\subsection{Type systems}
\label{TypeSystems}

Section \ref{BreakingChanges} showed that changing the type of a public
entity to a non-compatible one should be considered a breaking change.
However, what it means for two types to be compatible varies. Python,
Java and Clojure were chosen as candidate target languages for this
tool partly because of their different type systems.

In CAPIR, the only thing required of a \pythoninline{Type} is to be able to
identify whether it is compatible or not with another type. It does
not matter whether or not they are the same type, or what those
types actually are\footnote{The point is that what the types actually
are is irrelevant to determining the next semantic version. It may be
of interest only to Autobump's secondary function, that is generating
changelogs.}.

In the following sections on the individual language handlers, we
discuss how the three languages determine compatibility of types.

\subsection{Python}
% TODO: use CPython, but should work with anything

Python is a general-purpose, dynamically typed, interpreted language.
That makes it an interesting target and a good first choice,
especially compounded with the fact that Python is a good middleware
language and Autobump itself is written in Python.
% TODO: is it though?

In 2008 Python 3 was released, which broke backwards compatibility
with the widely spread Python 2. Adoption has been sluggish, but at
the time of writing virtually all libraries that matter fully support
Python 3 along with 2, or are exclusive to Python 3. Seeing as how
Python 2 is deprecated and will not receive any further development,
this project is concerned solely with Python 3. Moreover,
some interesting features of the language that matter for Autobump
(like type hinting) are only available in Python 3.

The Python handler uses the AST and is itself written in Python. It
takes advantage of Python's ``batteries included'' approach which
bundles all sorts of useful libraries along with the core Python
distribution, and that includes a Python parser. Converting to CAPIR
is a simple matter of opening each source file as text, parsing it and
then walking the resulting tree to produce the appropriate CAPIR
structures for each node.

Early iterations of the Python handler used introspection. It used to
import source files dynamically, which causes them to be compiled and
made available to the whole program. Then, the individual items in the
module were visited to generate their CAPIR counterparts. This led to
several problems. It's not guaranteed that Autobump runs in the same
environment as the library, so missing dependencies would halt the
whole process. But probably the biggest problem is that this approach
essentially evaluates the code it comes across -- after all, in order
to perform introspection on a function for example, a \pythoninline{def} has
to be evaluated first. If there is top-level code that has side
effects it will get executed along with the function definitions.

Python, as a dynamic language, can modify the structure of objects at
runtime by using functions such as \pythoninline{setattr}. This makes
it possible for a library to construct its functionality at runtime
instead of defining outright. This is fairly unlikely to happen in the
real-world libraries that Autobump would be useful for, so that possibility
is discounted.

\subsubsection{Types}

Python, by design and philosophy, encourages the use of duck typing.
When an operation is performed on an object, only the compatiblity of
that object with the operation is considered, not the actual type of
the object. This is a form of runtime structural typing (as opposed to
nominal typing) -- the structure of the object is what's important,
not what it is called.

Consider the following trivial example:

\begin{center}
\begin{tabular}{c}
\begin{python}
def operate(obj):
    obj.operation()
\end{python}
\end{tabular}
\end{center}

The contract that Python enforces is that \pythoninline{obj} has an
implementation of the method \pythoninline{operation}. It makes no
difference what the type of \pythoninline{obj} is -- the program will not
fail as long as the parameter passed into \pythoninline{operate} implements
\pythoninline{operation}.

If the only thing that matters are the accessed properties and methods
of an object to determine whether the program fails or not, this makes
the task of figuring out how the types of parameters have changed more
difficult. It would be unnecessary and even misleading to attempt to
look up usage of a function inside the project itself, for example in
tests. Even though it may be found that all parameters passed into the
function are instances of a particular class, client code absolutely
could pass in objects of a different type that happen to have the same
properties and methods as needed by the function.

% TODO: What about types of entities? Need to implement AST scanning
% as well

A better choice is to consider two sets $S_a$ and $S_b$ -- the sets of
all properties and methods requested from the object in the two
versions to compare, $S_a$ being the earlier version chronologically.
If $S_a \supseteq S_b$, then it can be safely claimed that there was no
breaking change as far as the type of the object is concerned. After
all, if client code was passing an object successfully into the
earlier version of the function, then it should continue to do so
because either the set hasn't changed, or some of the expected
properties and methods are no longer expected.

However, if $S_a \subset S_b$, then there are additional properties
and methods that the later version expects. It's not necessary that
client code breaks in this case, but it is definitely a possibility.
Likewise in all other less likely scenarios, like $S_a$ and $S_b$
being neither super nor subsets of one another.

Consider the following example:

\begin{halfmini}{Variant A}
\begin{python}
def func(obj):
    obj.method1()
    obj.method2()
\end{python}
\end{halfmini}
\begin{halfmini}{Variant B}
\begin{python}
def func(obj):
    obj.method1()
\end{python}
\end{halfmini}

In this case, because \pythoninline{method2} is no longer expected in
Variant B, $S_a \supset S_b$ and it is safe to claim that there is no
breaking change. If it were the other way around, i.e. Variant A came
after Variant B, then that is not guaranteed. Client code previously
passing in objects that implement \pythoninline{method1} was not obliged to
have them implement \pythoninline{method2} as well, so it may break once the
new revision of the library starts needing it.

Depending on whether the configuration option
\codeinline{Python/Structural Typing} is set, Autobump can examine the use
of a parameter inside a function and determine those two sets. This is
accomplished by walking the AST of the function definition and
checking what methods are called on that object in all possible
branches of execution. Afterwards, it takes the pessimistic view that
unless $S_a \supseteq S_b$ there was definitely a breaking change and
bumps the version number accordingly.

Depending on the nature of the project, this behaviour can be a
hinderance and yield lots of false positivies. That is why it is
configurable. If the option is disabled, then the Python handlers
never considers the types of entities and assumes them to be compatible.

\subsubsection{Type hinting}

As of Python 3.5 there is support for type hinting in the language
itself. It is purely syntactic and only affects the metadata of
objects when evaluated in the interpreter. No actual type checking is
done and this is done to static analysis tools, mostly type checkers.
The Python handler functions as another static analysis tool, but with
a somewhat different goal -- checking compatibility between different
versions of the same code, not validating invocations.

In Python 3.5 and 3.6, which are the only versions of Python with
support for type hinting at the time of writing, there is a small
oversight. When given code, the only way to see what the type hints
are is to actually evaluate the code and check the metadata of the
relevant objects.
This is undesirable in the case of Autobump where the AST ought to be
used instead. A third-party fork of Python's built in \codeinline{ast}
module exists, named \codeinline{typed_ast}\cite{TypedAST}. It gives
access to type hinting information as part of AST itself. It's
expected that this fork will get merged back into mainline Python
eventually, but for the time being Autobump uses it as an external
library.

If the option \codeinline{Python/Type Hinting} is enabled, whenever there
is type hinting information for a function or field it's actual usage
in the AST is ignored regardless of whether \codeinline{Python/Use
Structural Typing} is enabled. Given the obvious incompatibility of
having one version be parsed with structural typing, and another one
having a type hint, Autobump always pessimistically assumes that the two are
not going to be compatible. If both versions have type hints, they are
compared using Python's native facilities.
% TODO: no they're not

\begin{halfmini}{Variant A}
\begin{python}
def func(a: str):
    a.split()
\end{python}
\end{halfmini}
\begin{halfmini}{Variant B}
\begin{python}
def func(a: str):
    a.split()
    a.strip()
\end{python}
\end{halfmini}

In the above example, if both type hinting and structural typing are
enabled, Autobump won't find a breaking change because the type hint
has remained the same. If the type hint were missing in both, it would
pessimistically claim there is a breaking change due to a new method
being called on the parameter \pythoninline{a}, unaware that it is
actually a string.

\subsection{Java}
\label{JavaHandlers}

Java is a compiled, statically-typed language which makes it very
different from Python. Nevertheless, the two approaches -- AST
and introspection -- are still reasonable options for transforming a
Java source tree into CAPIR. Java is the only language supported by
Autobump that actually has two separate handlers.

\subsubsection{Using the abstract syntax tree}

\codeinline{java_ast} is written entirely in Python and
values speed over corectness. It is a quick-and-dirty handler that
visits all Java source files in the repository and generates their
ASTs via the third-party package \codeinline{javalang}\cite{Javalang}.
Transforming the AST into CAPIR is not quite as straightforward as it
is in Python, because Java is statically typed.

Consider this trivial example:

\begin{halfmini}{Variant A}
\begin{java}
public void method(A arg) { ... }
\end{java}
\end{halfmini}
\begin{halfmini}{Variant B}
\begin{java}
public void method(B arg) { ... }
\end{java}
\end{halfmini}

Note that the only thing that has changed is type of the parameter.
Whether or not there was a breaking change in this case depends on
two things.
\begin{enumerate}
\item What the relative names \javainline{A} and \javainline{B} get resolved
to. This is controlled by a \javainline{package} statement at the top of
the file and subsequent \javainline{import} statements. If the code
compiles fine, and the names are not explicitly imported, then it is
certain that they are from the same package as the source file. In the
case of a wildcard import, i.e. something like \javainline{import
org.apache.util.*}, there's no way to know whether the names are
coming from the wildcard or the current package. The problem
exacerbates when there are multiple wildcard imports. It would be
impossible to fully qualify the type names without asking Java itself
with introspection.
\item The relationship between the two types. Java supports class
inheritance and the conceptually similar implementation of interfaces.
If \codeinline{B} is more concrete than \codeinline{A}, i.e. it is further
down the inheritance tree, then there was definitely a breaking change
-- client code still passing in objects of type \codeinline{A} cannot
implicitly cast to \codeinline{B}. If \codeinline{B} is more abstract, then
the parameter itself was generalised and client code will still work
fine passing in more concrete variants. While the AST of the relevant
classes can be inspected to see where they derive from or which
interfaces they implement, if the types are found in third-party code
or the AST is absent for whatever reason, it is again impossible to
decide whether there was a breaking change without using introspection.
\end{enumerate}

\codeinline{java_ast}, as it is constructing the ASTs of all source files,
does its best to fully qualify type names based on the surrounding
statements and keeps track of the inheritance tree for the project. It
does this by doing two passes on all source files -- once to establish
and fully qualify all types found used or declared in the project, and
then a second pass to actually build CAPIR. To illustrate why two
passes are necessary, consider a Java file in the codebase which has
these two variants:

\begin{halfmini}{Variant A}
\begin{java}[basicstyle=\ttms,keywordstyle=\ttbs\color{keyword},emphstyle=\ttbs\color{name}]
package somePackage;

public class MyClass {
    public void method(OtherClass p) {}
}
\end{java}
\end{halfmini}
\begin{halfmini}{Variant B}
\begin{java}[basicstyle=\ttms,keywordstyle=\ttbs\color{keyword},emphstyle=\ttbs\color{name}]
package somePackage;

public class MyClass {
    public void method(YetAnotherClass p) {}
}
\end{java}
\end{halfmini}

\javainline{OtherClass} and \javainline{YetAnotherClass} may both be
declared in the same package as this source file. However, it's not
guaranteed that the parser will have visited those in advance. We
cannot adequately build CAPIR of a type just by looking at the
reference type \javainline{OtherClass}, because it can't be qualified
before having visited all other files. If it is indeed defined in the
same package, there is the opportunity to check whether its
descendants include \javainline{YetAnotherClass} and identify whether
this particular change is going to be breaking. Note that even if the handler
was aware that \javainline{OtherClass} was declared in the same
package and it could be fully qualified on the first pass, it is still
possible that \javainline{YetAnotherClass} hasn't been visited and the
relationship between the two is unknown.

The handler has obvious shortcomings in that it simply misses cases
that can be determined only by introspection. Considering the same
example as befor, it could be that both parameters types are after all
declared in some external dependency and not in the same package. In
that case there would be no way of knowing the relationship between
the two using this handler. The optional configuration option
\codeinline{Java/Error On External Types} forces the handler to give up
when the AST of a source file is needed, but not present in the
project.

\subsubsection{Using reflection}

The alternative is \codeinline{java_native}. It consists of two utilities
written in Java, along with some Python middleware that interacts with
the rest of Autobump. It relies solely on introspection of the
\codeinline{.class} files generated after a full compilation of the
project. One tool, the \codeinline{Inspector}, loads a list of class files
and then prints to standard out an XML representation of all classes,
including fields, methods, inner classes and so on. This is invoked
once, immediately after compilation. Note that the XML representation
is semantically absolutely indentical to CAPIR. The Python middleware then
translates this XML representation into a list of CAPIR units, which is
almost trivial.

There is one small modification that the Python middleware does when
translating the XML. Note how CAPIR allows for a function to have
multiple signatures. This is the preferred way of representing
overloaded methods in Java, the alternative being having multiple
functions of the same name. Autobump would yield the same proposal for
a semantic version at the end, but the changelog would be less intelligible.
That is why it takes the return value of every method and makes it
part of the signature. The signature of the method \javainline{void
m(int a)} is considered to be \javainline{(void, int)} instead of
just \javainline{(int)}. This extra ``argument'' takes the special
name \pythoninline{$AUTOBUMP_RETURN$} as can be seen in figure
\ref{CAPIRXMLExample}. The return type of the function in CAPIR is set
to some dummy type that is compatible with everything, so that the
comparison algorithm detects only changes in the signatures.

\begin{figure}[H]
\caption{Java source and the resultant CAPIR in intermediary XML}
\begin{halfmini}{Java Source}
\begin{java}[basicstyle=\ttms,keywordstyle=\ttbs\color{keyword},emphstyle=\ttbs\color{name}]
package com.autobump.somepackage;

public class SomeClass {
    public int field;

    public void method(String a, Integer b) {}

    public class InnerClass {
    }
}
\end{java}
\end{halfmini}
\begin{halfmini}{CAPIR (XML)}
% TODO: remove extra $
\begin{xml}[basicstyle=\ttmt,keywordstyle=\ttbt\color{keyword},emphstyle=\ttbt\color{name}]
<class name="com.autobump.somepackage.SomeClass">
 <field name="field" type="int"/>
 <method name="method" returns="void">
   <signature>
     <parameter name="arg0" type="java.lang.String"/>
     <parameter name="arg1" type="java.lang.Integer"/>
   </signature>
 </method>
 ...
 <class name="com.autobump.somepackage.SomeClass$$InnerClass$$">
   ...
 </class>
</class>
\end{xml}
\end{halfmini}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[height=0.5\textheight]{images/dot/xmlcapir}
\caption{CAPIR of Java source code produced from intermediary XML}
\label{CAPIRXMLExample}
\end{figure}

Type comparison is handled through another tool --
\codeinline{TypeCompatiblityChecker}. Given two types, it simply prints
whether or not the first one is compatible with the second one. This
again works by loading the relevant class files and simply asking Java
itself. CAPIR types set by the Python middleware will automatically
invoke this utility when \pythoninline{is_compatible} is called
later on in the comparison.

Having to compile the project first and then run these tools
introduced substantial complexity to the handler. It would be naÃ¯ve to
hardcode any commands that compile the project. Instead, it is better to
delegate to the build system that comes with the project to properly
handle things like dependencies and \codeinline{CLASSPATH} issues.
\codeinline{java_ast} requires that a build command and a build root are
passed in to Autobump as command-line arguments.
% TODO: why arguments and not in config?

The build command is a shell command that runs before using the
\codeinline{Inspector} to introspect all classes, for example
\codeinline{mvn compile} if the project is using Maven as its build
system. The build root is a path relative to the root directory where
the classes get placed after compilation. It is important that the
handler knows that in order to correctly fully qualify class names. In
Java, the package hierarchy corresponds to directory structure, and
the wrong build root would result in the \codeinline{Inspector} not
finding the needed classes.

The native handler produces much more accurate results, but is also
much slower -- it necessitates compiling the project, twice. There's
value in using the AST handler for self-contained libraries where it
would give identical results to the native one. Alternatively, if all
the user is interested in is the new version number and not the changelog,
running the AST handler once can be a shortcut. If it reports a major
bump, then there's no reason to run the native one. After all, the AST
handler can detect a subset of changes compared to the native one. If
not, then \codeinline{java_native} can be run to make sure.

\subsubsection{Handling arrays}

When comparing types, Java takes into account dimensions as well. It
is not sufficient that the types themselves are compatible, but also
that the dimensions are equal. For example, if the type $A$ is deemed
compatible with $B$, it is not the case that $A[]$ is compatible with
$B$. Both handlers satisfy this by keeping track of dimension in their
CAPIR type representations and modifying the
\codeinline{is_compatible} function accordingly.

\subsection{Clojure}
\label{Clojure}

Clojure is interpreted, general-purpose variant of the Lisp
programming language, with an emphasis on functional programming. It
has several properties that make it noticeably different from the
previous two languages considered:

\begin{itemize}
\item While not purely functional, it heavily emphasises programming
in a functional style and discourages stateful programming both through the
language itself and the culture surrounding the language.
\item It is homoiconic, that is programs are written in terms of the
language's own data structures (``code is data''). In essence the
programmer is always creating and editing the AST directly. This
allows for powerful metaprogramming features.
\item It is a dynamically typed language running on the statically
typed Java Virtual Machine.
\item It has support for gradual typing --
some variables and expressions may be given static types that can be
checked at compile time, while others remain dynamically typed.
\end{itemize}

See appendix \ref{ClojurePrimer} for a primer on Clojure that
elaborates on these features and gives concrete examples.

The Clojure handler follows a model very similar to \codeinline{java_native}
-- the handler itself being written in Clojure and linking to Autobump
using some Python middleware. The bulk of the work done in Clojure is
just mapping a program to its public API. As was with Java and XML,
the most convenient structures to generate with Clojure are
S-expressions, so those are used as an intermediary before the
middleware can translate that into Autobump's internal CAPIR. The
handler takes advantage of the language's homoiconicity and powerful
reflection capabilities, which makes it easy to inspect programs using
the language itself.

Early versions of the Clojure handler simply took advantage of
homoiconicity and directly mapped the AST to CAPIR. However, that is
not sufficient when external libraries are present and types defined
in them need to be type checked, or when optional static typing is being used.
Superficially, because Clojure is dynamic and interpreted, it seems as
if there is the same problem as with Python -- having to
rely on just the AST, because evaluating code and then trying to do
introspection risks executing side effects. It is not so. The Clojure
handler can use a mixture of both approaches with no penalties,
because it is functional and the risk of side effects can be disregarded.

% TODO: here we discuss.... ???

\subsubsection{Mapping AST to CAPIR}

At the core of every Lisp sits the \textit{reader}. This is the
function that converts Lisp objects from their textual form into
their internal representation. Clojure provides us with the
\clojureinline{read-string} function that reads in the string
representation of a form and returns it as a proper Lisp object.
Combining this with the \clojureinline{slurp} function that returns
the contents of a file as a string, a function can be built that
given a file name, returns the Clojure program stored inside as a list
of forms (other lists):

\begin{center}
\begin{tabular}{c}
\begin{clojure}
(defn read-source
  "Read in a Clojure source file and return a list of forms."
  [file-name]
  (read-string (str "(" (slurp file-name) ")")))
\end{clojure}
\end{tabular}
\end{center}

From there, forms of interest can be filtered out to describe the part
of the API they expose using an S-expression. The S-expressions can be
nested together, forming a tree identical in structure to
CAPIR.

\begin{center}
\begin{tabular}{c}
\begin{clojure}
(defn describe-unit
  "Describe the public API of a file as an S-expression."
  [file-name]
  (let* [forms         (read-source file-name)
         fields-defs   (filter public-field-def? forms)
         function-defs (filter public-fn-def? forms)]
    (list 'unit
          (get-ns forms)
          (map describe-field fields-defs)
          (map describe-function function-defs))))
\end{clojure}
\end{tabular}
\end{center}

The functions prefixed with \clojureinline{describe-} return an
S-expression representing a CAPIR entity. They may in turn use other
functions to describe inner entities, e.g.
\clojureinline{describe-function} would invoke
\clojureinline{describe-signature}. The Python middleware's only task
from there is to translate those S-expressions into Autobump's
internal representation, which is done by implementing a minimal Lisp
reader in Python.

To illustrate the process, consider the following example:

\begin{figure}[H]
\caption{Clojure source and the resultant CAPIR by mapping AST
directly to intermediary S-expressions}
\begin{halfmini}{Clojure Source}
\begin{clojure}[otherkeywords={def,defn,defn-,ns}]
(ns lib.core)

(def cst 5)
(def ^{:private true} private-field)
(defn- private-func [])
(defn func
  ([x] (+ cst x))
  ([x ^Integer y] (* (func x) y)))
(defn get-nil [])
\end{clojure}
\end{halfmini}
\begin{halfmini}{CAPIR (S-expressions)}
\begin{clojure}[otherkeywords={file,field,function,signature}]
((file lib.core
  ((field cst nil))
  ((function func
     ((signature ((x nil)))
      (signature ((x nil) (y Integer)))))
   (function get-nil
      ((signature ()))))))
\end{clojure}
\end{halfmini}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[height=0.5\textheight]{images/dot/sexpcapir}
\caption{CAPIR of Clojure source code produced from intermediary S-expressions}
\label{CAPIRSexpExample}
\end{figure}

Note how the type of \clojureinline{cst} is \clojureinline{nil} and not
\clojureinline{java.lang.Double} as Clojure would report if the type
of the value were introspected. This is a limitation of just using the
AST and not evaluating forms. However, finding out about type hints
can be done, because they interpreted by the reader and don't need
evaluation.

\subsubsection{Macro expansion}

Clojure is a macro-heavy language. It has a relatively small core, and
a lot of functionality is built by using macros that expand to special
forms. For example, the macro for defining a function
\clojureinline{defn} eventually expands to a \clojureinlinedef with an
anonymous function. This makes it tricky to extract functions and
fields from namespaces without actually evaluating the code. Clojure
provides a special form \clojureinline{macroexpand} which takes in a
form and returns the form with one step of macro expansion.
\clojureinline{macroexpand-all} may be more appropriate, because it
recursively expands all macros until there are no more.

It may be the case that a library is entirely written with the
top-level forms being macros that when evaluated populate the
namespace with functions and fields. It may also be the case that
built-in macros are overriden by some library to behave slightly
differently -- for example, the library for optional static typing
\clojureinline{core.typed} overrides \clojureinline{defn} so that
programs can specify type signatures as part of the functional
definiton instead of an additional annotation.

It would be unreasonable to attempt to handle such cases when mapping
AST to directly, even when using facilities like
\clojureinline{macroexpand}.

\subsubsection{Using both AST and reflection}

Several reasons have compounded why relying on the AST alone is not
preferrable, especially when evaluating the code doesn't impose any
penalties except for making the handler slightly slower:

\begin{itemize}
\item It's impossible to get the types of fields without evaluating
their values.
\item It's unreasonably difficult to guard against all sorts of macro
trickery, and it cannot be claimed for certain what a form does without
evaluating it.
\item On the same note, optional static typing cannot be properly
handled because it relies on macro trickery.
\item Type checks cannot be run against external libraries. If the type
hint of a parameter was changed, there's no way to verify whether the
new one is compatible with the old one.
\end{itemize}

The vast majority of Clojure projects use the build system Leiningen
\cite{Leiningen}. A typical development workflow that it exposes (as
do other similar build systems) is starting a REPL that evaluates all
files in the project and makes the namespaces and functions available
for manual probing. This feature is usually not used directly, but
further abstracted into some integrated development environment, e.g.
CIDER \cite{CIDER}.

The Clojure handler can be modified in such a way that it keeps its
\clojureinline{describe-...} architecture, but sources its data from
the runtime instead of the AST. This necessitates running it in the
same environment as the project itself, which is identical to what a
REPL offers. For this reason, the Clojure executable that Autobump
invokes can be configured to be one such that it presents this
environment for execution. A convenient solution that is specific for
Leiningen is the plugin \codeinline{lein-exec} \cite{LeinExec}, which
allows running scripts as if they were piped into a REPL.

The accuracy of functions such as \codeinline{public-fn-def?} can now
be greatly improved by simply invoking Clojure built-ins instead of
interpreting macro invokations:

\begin{halfmini}{Using AST}
\begin{clojure}
(defn public-fn-def? [lst]
  (and (not (:private (meta (second lst))))
       (= (first lst) "def")))
\end{clojure}
\end{halfmini}
\begin{halfmini}{Using reflection}
\begin{clojure}
(defn public-fn-def? [v]
  (fn? v))
\end{clojure}
\end{halfmini}

Note that is not necessary to check that the definition is private
when using reflection; it would not be present when examining the
namespace anyway.

\section{Handlers for version control systems}
\label{VCSHandlers}

Just like Autobump attempts to abstract away details of the languages
it handles, it is also transparent with regard to the underlying
version control system the target project uses. Abstracting away
operations on a VCS is a much easier task, since many of them follow
similar ideas.

Handlers define a very small API. The central function they are
expected to implement is

\begin{center}
\begin{tabular}{c}
\begin{python}
def get_commit(repository, commit):
    ...
\end{python}
\end{tabular}
\end{center}

which returns a directory location, where the state of the repository
at that particular commit identifier can be found. How the handler
obtains that is an implementation detail. Both version control systems
that Autobump supports out of the box (Git and Mercurial) are
distributed, so their preferred approach is to clone the repository to
a temporary created location (not necessarily on disk, but in the file
system), and then issue the appropriate command, e.g.
\codeinline{checkout}), to get the working tree at a particular
revision. Appropriate Python features are used so that the temporary
directories get cleaned up after Autobump is done using them.

The function \pythoninline{get_commit} is used in conjuction with the
language handlers' \pythoninline{codebase_to_units}, which takes in a
location in the file system and converts the code there to CAPIR.

Autobump's main program essentially simple function composition to
obtain the CAPIR of code as it was at some revision:

\begin{center}
\begin{tabular}{c}
% TODO: real composition
\begin{python}
capir_at_commit = codebase_to_units . get_commit
\end{python}
\end{tabular}
\end{center}

\section{Comparison logic}
\label{Logic}

After the language handler has run twice and generated two CAPIR
trees, the comparison algorithm can be invoked to find the differences
between the two variants. It itself is fairly straightforward and
defined recursively:

\begin{algorithm}
\caption{CAPIR Entity Comparison}
\begin{algorithmic}[1]
\Procedure{Compare}{A, B}
\State $assert(type(A)$ is $type(B))$
\State $comparators \gets \text{map of the form }(member \rightarrow function)$
\For{$member, comparator$ in $comparators$}
\If{$member$ in $A$}
$report(comparator(A[member], B[member]))$
\EndIf
\EndFor
\State $all \gets members(A) \cup members(B)$
\For{$member$ in $all$}
\If {$member$ not in $A$ and $member$ in $B$}
$report(\detokenize{ENTITY_INTRODUCED})$
\EndIf
\If {$member$ in $A$ and $member$ not in $B$}
$report(\detokenize{ENTITY_REMOVED})$
\EndIf
\EndFor
\State $common \gets members(A) \cap members(B)$
\For{$member$ in $common$}
$compare(A[member], B[member])$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

In short, the algorithm compares two entities by examining their list
of attributes and runs specific comparator functions on known
bottom-level members. After that is done, it checks for members that
are missing from either variant and recurses into members that are
present in the intersection of both entities.

Note that this procedure technically compares entities, not trees. To
kick off the comparison of two trees it's necessary to wrap both in a
CAPIR unit, so that the first level of recursion actually looks at the
top-level units in both codebases.

Autobump's core objective, suggesting the new version number, is
essentially formulated functionally in the main program as follows:

\begin{center}
\begin{tabular}{c}
% TODO: real composition
\begin{python}
capir_at_commit = codebase_to_units . get_commit
change = compare(capir_at_commit(previous), capir_at_commit(current))
new_version = old_version.bump(change)
\end{python}
\end{tabular}
\end{center}

$report$ is a simple function that takes care of adding a change to
the changelog and checking whether the latest reported change is more
notable than the most notable so far. For example, reporting a feature
addition after several patches would cause a feature addition to be
the most notable change. Note that nothing can top a breaking change.
Had Autobump only concerned itself with bumping the version number,
the algorithm could stop at finding a breaking change. However,
because a changelog needs to be generated as well, the algorithm
carries on looking for changes even though the version number itself
is clear at this point.

A map of the form $member \rightarrow function$ is purposefully used
instead of invoking the comparators directly. It shows the general
principle of comparing members of CAPIR entities, even if eventually
CAPIR is modified or extended and the comparators change. The only
comparators used so far ar for type checking (on the member $type$)
and signature checking (on the member $signatures$).

% TODO: talk about individual comparators

\section{User interface}

Autobump is a command-line application. Direct instructions to it are
provided by passing in options.

The anticipated most frequent manual use case is comparing the last
released version and the current commit that is meant for release.
That way the user can see what the version of that new release should
be. In this case, the only thing required of the user is to pass in
the language handler. The output consists of a single line, indicating
the new version.

\begin{figure}[H]
\centering
\caption{Most frequent manual use case}
% TODO: git history of most frequent use case
\begin{BVerbatim}
> git tag | tail -n1
1.2.0
> autobump python
1.3.0
>
\end{BVerbatim}
\end{figure}

Alternatively, a changelog can be generated in addition to just
providing the version number. The option \codeinline{--changelog
[filename]} saves the changelog to a file, and
\codeinline{--changelog-stdout} just prints it. The very last line of
output is still guaranteed to be new version number, so it can be
easily extracted using tools like \codeinline{tail}.

\begin{figure}[H]
\centering
\caption{Changelog generation}
\begin{BVerbatim}
> autobump python --changelog-stdout
moduleA.some_function: Entity was introduced.
moduleB.other_function: Parameter(s) with default value was added.
1.3.0
>
\end{BVerbatim}
\end{figure}

To alter the default revisions to compare, the options
\codeinline{--from} and \codeinline{--to} can be used. They accept a
valid identifier as recognised by the underlying version control
system, e.g. a commit hash or a tag name. Changing the range can be
used to check for previous mistakes or when changing the release
history. It is also used for running Autobump in evaluation mode,
where it is automatically run multiple times against chronologically
adjacent revisions, as discussed in chapter \ref{Evaluation}.
% TODO: wtf is "changing the release history"?

\begin{figure}[H]
\centering
\caption{Changelog generation}
\begin{BVerbatim}
> autobump python --from 1.0.0 --to 1.1.0
1.1.0
>
\end{BVerbatim}
\end{figure}

Autobump is also able to provide debugging information in case the
user suspects that something is not working quite right. The level of
information can be controlled and by default Autobump doesn't provide
any except for warnings. The following options control the level of
information, in order of application (i.e. options further down the
list trump everything above).

\begin{enumerate}[(i)]
\item No option: only warnings are shown.
\item \codeinline{--info}: provides general information about Autobump
is doing, as well as warnings.
\item \codeinline{--debug}: provides very detailed information about
the current execution path, as well as warnings.
\item \codeinline{--silence}: absolutely ignores any debugging
information.
\end{enumerate}

All debugging information is printed to \codeinline{stderr} so as not
to interfere with the regular output of the program. This was very
important during development, as the debugging information could
easily be piped to a separate file for later analysis.

Errors are always shown and cannot be silenced. That is desirable,
because Autobump treats all errors as fatal and immediately terminates
when one is encountered. The return code of the program is set to
non-zero to indicate that it failed.

\begin{figure}[H]
\centering
\caption{Fatal error in Autobump}
\begin{BVerbatim}
> autobump cpp
autobump: Invalid handler specified!
> echo $$?
1
>
\end{BVerbatim}
\end{figure}

A full rundown of all options is given in appendix \ref{RundownOptions}.

\subsection{Integration with other development tools}

% TODO: fill this in

\chapter{Testing}
\label{Testing}

This chapter looks at how it can be demonstrated that Autobump indeed
works and what testing techniques were used throughout development.
Autobump, because of the nature of what it does, has an exteremely
large range of valid inputs. For that reason testing was central to
Autobump's development and was done from very early on.

\section{Unit testing}

Non-trivial modules from the implementation, namely the comparison
logic and the language handlers, are tested in isolation by using
Python's built-in \pythoninline{unittest} framework. This has been very
useful for identifying edge cases where the modules don't quite do the
correct thing. Also, adding new features to the language handlers has
proven to be very error-prone, so having the unit tests in place
serves well for identifying regressions.

All tests treat the module they're testing as a black box. This is
very easy to do, because virtually all modules have a single
entrypoint that abstracts all implementation detail. Different inputs
are given to that function and then the results are compared against
the expected ones.

\subsection{Comparison logic}

% TODO: What about VCS handlers?
\subsection{Language handlers}

The entrypoint of every language handler is the function
\pythoninline{codebase_to_units} that has a single input parameter, the
location of that codebase on disk. Typically that directory would be
produced by Autobump in advance of calling the handler by cloning and
checking out some revision of a repository.

To test this function, the presense of a directory needs to be mocked,
containing some fixture instead of an actual repository. This is best
done with Python's built-in \pythoninline{tempfile} module which can create
temporary files and directories, not necessarily physically on the
disk. The fixtures are found in the tests themselves and are written
to temporary files as part of the set up process. By holding onto the
handle obtained by calling functions from \codeinline{tempfile}, it can
be ensured that everything is cleaned up during the tear down phase of
testing.

A fixture consists of a list of tuples, each tuple representing the
path to a file and its content. A minimal Clojure fixture looks like
this:

\begin{center}
\begin{tabular}{c}
\begin{python}
sources = [
    ("lib/core.clj",
    "
    (ns lib.core)
    (def constant 1)
    ")
]
\end{python}
\end{tabular}
\end{center}

Having the fixture actually present in the filesystem is necessary
because the handlers rely on external utilities, which need to load
and parse those files. The notable exception is the Python handler,
which is the only one written entirely in Python. For that reason we
can take a shortcut when writing tests for it. Instead of calling the
entrypoint \pythoninline{codebase_to_units} which scans a location for
relevant files and only then converts them to CAPIR units,
\pythoninline{_source_to_unit} can be called directly. That is an
implementation-specific function found only in the Python handler. It
takes in Python source code as a string and returns a unit. Even
though this is technically white box testing, this function is still
treated as if it were the entrypoint to the module. This has two
advantages: it greatly speeds up the tests because there's no need to
create temporary files and directories, and it simplifies the fixtures
-- they can be the source code of a single file with no metadata.

\section{Acceptance testing}
% TODO: 1.0.0 -> 2.0.0 should have been, but 1.0.1 -> 1.0.2 is fine

Even if the unit tests all pass and reasonably cover edge cases for
the individial modules, Autobump itself can still misbehave and
produce wrong results when used in practice. Early on in the project,
Autobump was tested against some real-life and some specially
constructed repositories manually. This was horribly inefficient and
error-prone. For that reason it was crucial to develop acceptance, or
end-to-end, tests that can quickly show that Autobump works correctly
when applied to a full-blown repository.

The entirety of Autobump can be considered as a single entity, with the
input being a repository\footnote{There are of course additional
inputs, like what handler to use or which revision to compare.
However, those are trivial to mock.} and the output the proposed
version number. Having a repository as an input is non-trivial.

Three approaches were considered.
\begin{enumerate}[(i)]
\item Maintain a Git \textit{submodule}, a kind of nested repository,
inside Autobump's own repository. The submodule would contain the
history of a mock project that changes its API over time. Specific
commits would be tagged with version numbers, representing the version
number that Autobump is expected to produce when comparing that
revision to the previously tagged one. This approach was quickly
ruled out, because it makes editing the progression of the project
unnecessarily difficult.
\item Reconstruct a Git repository containing the history of a project
by using an acceptance test framework like \textit{Robot
Framework}\cite{Robot}. Frameworks like these generally let you
specify pre and postconditions either in freeform or
\textit{given-when-then} form, so an example test may look something
like this:

\begin{center}
\begin{BVerbatim}
Given a public method in a class
When the method is removed
Then version bump should be major
\end{BVerbatim}
\end{center}

This would have produced very clear and easy to read tests, at the
expense of having to spend much development time translating
instructions like these into an actual repository that can be fed into Autobump.
\item Reconstruct a Git repository from a series of patches kept in
text form as part of test itself. Every patch would have metadata that
indicates the expected version of the project at that commit. This is
the option that Autobump uses, as it achieves a convenient
middleground between the other two: it allows for the history of the
project to be edited relatively easily, and it has much smaller
boilerplate needs. The only accompanying code (i.e. the test runner)
is one that creates a Git repository in a temporary directory from
that series of patches, and then runs Autobump against every pair of
consecutive revisions.
\end{enumerate}

% TODO: clarify test vs suite
However, it's not quite enough to just have a series of patches to
construct the acceptance test. At the bare minimum, the test also
needs to say which handler should be used when running Autobump. Tests
can also optionally define \pythoninline{setUp} and
\pythoninline{tearDown} functions that run respectively before and
after the suite. For example, the \codeinline{java_maven} test, which
goes through the history of a Maven project by using the native Java
handler, defines those two functions so that the \codeinline{CLASSPATH}
is properly set up before running Autobump. In normal operation, the
\codeinline{CLASSPATH} would be set up in Autobump's configuration file
or by setting the environment variable, but that is not an option when
automatically running multiple acceptance tests.

\begin{figure}[H]
\label{SampleAcceptanceTest}
\caption{Example acceptance test for the Python handler}
\begin{minipage}[t]{0.7\textwidth}
\begin{python}
handler = "python"

def setUp(repo):
    pass

def tearDown(repo):
    pass

commit_history = [

    ("1.0.0",
     "Initial commit",
     """
     diff --git a/module_a.py b/module_a.py
     new file mode 100644
     index 0000000..ab92b1d
     --- /dev/null
     +++ b/module_a.py
     @@ -0,0 +1,2 @@
     +def some_function():
     +    pass
     --
     """),

    ("1.1.0",
     "Add parameter with default value to function",
     """
     diff --git a/module_a.py b/module_a.py
     index ab92b1d..f700cc9 100644
     --- a/module_a.py
     +++ b/module_a.py
     @@ -1,2 +1,2 @@
     -def some_function():
     +def some_function(a=True):
     pass
     --
     """),

    ("2.0.0",
     "Remove parameter to function",
     """
     diff --git a/module_a.py b/module_a.py
     index ab92b1d..f700cc9 100644
     --- a/module_a.py
     +++ b/module_a.py
     @@ -1,2 +1,2 @@
     -def some_function(a):
     +def some_function():
     pass
     --
     """)
]
\end{python}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\noindent A symbol called exactly \pythoninline{handler} needs to be
set specifying the handler to use as a string. This is effectively the
same as setting the command-line option.
\\
\noindent Python lacks any special requirements because it does not use a build
system, so those functions can either be left empty or even left undefined.
\\
\noindent The scenario this acceptance test describes begins with a
single module which has a single empty function that does nothing and
accepts no parameters.
\\
\noindent Then, an optional parameter is added to the function
signature. At this point both the previous signature (with no
parameters) and the current one can be used, so that constitutes a
feature additing and the version number is bumped to
\codeinline{1.1.0} accordingly.
\\
\noindent In the last diff, the optional parameter is removed. Client
code calling the function and passing that parameter is expected to
fail, so that constitutes a breaking change and the version is bumped
to \codeinline{2.0.0}.
\end{minipage}
\end{figure}

\chapter{Evaluation}
\label{Evaluation}
% TODO: in essence, conflate semantic versioning with
% how-big-the-change-was versioning

This chapter applies Autobump retroactively to real-world libraries
written in the different target languages. It compares the version
numbers output of Autobump to what the developers of these libraries
assigned manually and interprets the findings. Looking at the data, it
is discussed whether the authors and users of those libraries would
have benefited had they used a tool such as Autobump. Additionally,
some of Autobump's design decisions are put to the test.

Two rounds of evaluation were performed. In the first round, Autobump
was manually ran against the history of ten projects across the
three languages. The reasons for the mismatches are examined manually
and the differences in the findings for the individual languages are
discussed.

In the second round, a large-scale, mostly automated evaluation was
performed on 632 Python projects. Overall trends are discussed, as
well as a comparison with the findings from the manual evaluation of
Python projects.

\section{Running Autobump against existing libraries}
% TODO: rename this section if doing a large-scale Python study

Retroactively running Autobump against a specific library was done
much in the same way as running an acceptance test. In testing, a
repository is recreated in the filesystem from a set of patches, with
each commit tagged as some version. During evaluation, no repository
is recreated but an existing one is used and only chronologically
adjacent tags are compared, not necessarily adjacent commits. These
similarities allowed for a large part of the testing mechanism to be
incorporated into Autobump itself. By passing the option
\codeinline{-e} or \codeinline{--evaluate} when invoking the program,
it will take the revisions specified by \codeinline{--from} and
\codeinline{--to} as a range instead of them being an ealier and a
latter commit. In essence, running an acceptance test is evaluating an
artificially constructed repository.
% TODO: what if --to is before --from?

Autobump expects that the tag names correspond to versions, and it
attempts to extract the semantic version. For example, the tag
\codeinline{gson-release-2.8} gets recognised as version
\codeinline{2.8.0}. If the two chronologically adjacent tags are
\codeinline{gson-release-2.8} and \codeinline{gson-release-2.9}, the
tool compares the two, figures out that the earlier one is
\codeinline{2.8.0} and issues a warning if the latter one is not
\codeinline{2.9.0}.

While running in evaluation mode, Autobump prints to
\codeinline{stdout} all the pairs it compares. If the option
\codeinline{--changelog-stdout} is passed as well, a changelog is
inserted in the output for each pair. The output itself is a simple
text format that allows for quick filtering and other text
manipulations:

\begin{center}
\begin{BVerbatim}
!EVAL Start diffing 0.4.0 and 0.4.1
docopt.Argument.match: Type was changed to an incompatible type
docopt.Command.match: Type was changed to an incompatible type
!EVAL End   diffing 0.4.0 and 0.4.1
!EVAL MISMATCH: 0.4.0 -- 0.4.1 should have been 0.4.0 -- 0.5.0
\end{BVerbatim}
\end{center}

This allows to measure the portion of mismatches, and whether Autobump
suggested a major version bump when the authors didn't. In combination
with metadata from the repository itself, the change in those metrics
over time can be shown. Appendix \ref{EvaluationScripts} shows the
various scripts that operate on this format to assist with evaluation.

Ten libraries in total across all languages were evaluated. They were
chosen based on factors like popularity, size and to what degree their
authors claim they adhere to semantic versioning. Prior to any
empirical data, it can be speculated how those factors could impact
the projects' actual adherence to semantic versioning.

Popularity is measured differently for the different languages, because
different statistics are available from the respective central
repositories. The Python Package Index (PyPI) \cite{PyPI} only measures
the total number of downloads of a particular package. This is not
ideal, because it is impossible to distinguish whether it is a direct
or indirect dependency of other projects. For example, the package
\codeinline{six} which provides Python 2 and 3 compatibility
utilities, is the third-most popular package on PyPI at the time of
writing with 54 million downloads, but it is unlikely that many end
users are actually using it directly. Most uses are probably coming
from other libraries on PyPI that are compatible with both Python 2
and 3.

On the other hand, Maven, which is the central repository for Java and
Clojure projects, does not provide downloads counts but the number of
dependant artifacts also found on Maven. This is a better statistic,
but still not ideal because it does not indicate the number of
applications that rely on a library which are not published on Maven
themselves.

Popularity, albeit a crude measurement, is proportional to the number
of users a library has. Depending on the nature of the library and the
development culture around it, a larger number of users can imply more
pressure on authors to better apply versioning schemes or at the very
least, provide detailed changelogs.

The relationship between library size and the number of accidental
breaking changes can be easily speculated upon: if no automated
tooling is used, the larger the library, the larger the risk of
accidentally introducing a breaking change because of an oversight.
This dissertation uses lines of code (LOC) in the most recent revision
of the library to indicate size. A more accurate measurement would be
the number of CAPIR entities, but LOC is good enough to indicate the
size relative to other projects of the same language. Of course it
would be less fruitful to compare against projects in a different
language -- for example, Python libraries on average tend to be much
smaller than Java libraries.

Claimed adherence to semantic versioning is found by looking at
project \codeinline{README} and \codeinline{CONTRIBUTING} files, or
other developer communication, such as mailing lists. Adherence is not
binary, but gradual -- this dissertation makes the distinction between
officially following Semver, apparently following Semver but never
saying that outright (i.e. the version numbers very much seem like
Semver), and anything inbetween. No projects were chosen that don't
even seem like they follow Semver, because no meaningful results can
be derived from evaluating those.

\section{Manual evaluation}

Tables \ref{PythonProjectsForEvaluation},
\ref{JavaProjectsForEvaluation} and \ref{ClojureProjectsForEvaluation}
outline the chosen projects for the first, manual, round of
evaluation. Subsequent sections discuss each library and give a
summary of general findings for the different languages.

The sample size in this round is very small and cannot be used to
extrapolate overarching themes. However, because it is reasonably
small manual inspection of the mismatches can be done, which in turn
allows for examining the mindset and motivations of developers.
% TODO: mindset and motivations?

The process of evaluation was also important for identifying
previously unknown errors in Autobump's behaviour, which were covered
by neither unit nor acceptance tests. Manual inspection of the
mismatches allowed for those bugs to be corrected. The following
results were obtained only after correcting Autobump's behaviour, so
all mismatches are because of a genuine disagreement between what the
developers claim and what semantic versioning (or at least Autobump's
interpretation of it) claims. Note that only mismatches are examined
-- versions where the two parties agree are not looked at, because it
is fairly unlikely that both are wrong.

% Which version of Autobump was used?

\subsection{Python}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|p{10cm}|}
\hline
\textbf{Project} & \textbf{Size} & \textbf{Downloads} & \textbf{Semantically versioned} \\
\hline
\codeinline{requests} & 4500 LOC & 53 million & Yes, officially. \\
\codeinline{simplejson} & 2000 LOC & 182 million & Seems like it, but not officially. \\
\codeinline{docopt} & 600 LOC & 9 million & Seems like it, but not
officially. \\
\codeinline{hashids} & 300 LOC & 154 000 & \codeinline{README} claims
that ``[they] try to follow Semver, but check the changelog''. \\
\hline
\end{tabular}
\caption{Python projects chosen for evaluation}
\label{PythonProjectsForEvaluation}
\end{table}

When evaluating Python projects, not all released versions were taken
into account. Even though only projects with Python 3 support were
selected, some of them are old enough that they have versions from
before they added support. If that is the case, it is noted and the
version and mismatch counts and ratios are adusted appropriately.

Although the Python language does have information hiding mechanisms,
they are relatively weak and seem like more of an afterthought.
Autobump does take into account Python's official policy of
considering symbol names prefixed with an underscore private and
ignores them. However, this project found that this policy is not
universally applied in the Python ecosystem. Almost all examined
libraries contained code that is obviously not meant for public use
but is nevertheless publically accessible. Strictly speaking this is
allowed by Semver -- it does allow the public API to be ``public'' by
documentation and intention, not only language mechanisms. To reduce
the number of false positives, custom configuration files were created
for each project that ignore certain files and directories that are
obviously not meant to be part of the public API. Initial attempts at
evaluating Python libraries did not do this and legitimate data was
being drowned out by changes to these private-by-convention entities.
% TODO; config files

Given that structural typing is a configurable option for the Python
handler, each project was evaluated twice -- with the option disabled
and then enabled.

\subsubsection{requests}

% TODO: put links to libraries, footnote or bibliography?
\textit{requests} is a large-ish (for Python standards) library for
making HTTP requests that officially claims to follow Semver, as
specified in their contributing guidelines. The evaluation range was
from version \codeinline{1.0.0} to \codeinline{2.13.0}, which was the
most recent version at the time of writing. The lower bound happens to
be both when Semver claims that the API is first published, and when
Python 3 support was added.

Tables \ref{RequestsNonStructural} and \ref{RequestsStructural} show
the number and ratios of mismatches with structural typing disabled
and enabled respectively. In both cases the proportion of total
mismatches hovers around 30\%, but the proportion of versions that
according to Autobump should have been breaking increases dramatically
when structural typing is enabled. Examining those patches where
breaking changes are found by Autobump with structural typing enabled,
it can be seen that calling new methods on function parameters is not
an infrequent occurrence. It is ambiguous whether those should really
be regarded as breaking changes -- in all of these cases reading the
code manually reveals that it's not unreasonable that the new methods
are available since the intention in using a parameter is clear.
However, technically, it could be that client code is passing objects
that implement the previously needed methods and not the new ones.
That would work fine with the previous version but would break with
the new one. It is not necessarily a mistake that project authors
didn't consider those cases to be a breaking change.

\noindent
\begin{minipage}[t]{0.5\textwidth}
\begin{table}[H]
\centering
\begin{tabular}{|lr|}
All versions & 43 \\
Mismatches & 13 \\
Mismatches (breaking) & 5 \\
Proportion of mismatches & 30\% \\
Proportion of mismatches (breaking) & 11\%
\end{tabular}
\caption{\textit{requests} statistics (w/o structural typing)}
\label{RequestsNonStructural}
\end{table}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{table}[H]
\centering
\begin{tabular}{|lr|}
All versions & 43 \\
Mismatches & 15 \\
Mismatches (breaking) & 11 \\
Proportion of mismatches & 34\% \\
Proportion of mismatches (breaking) & 25\%
\end{tabular}
\caption{\textit{requests} statistics (w/ structural typing)}
\label{RequestsStructural}
\end{table}
\end{minipage}

The 5 mismatches where Autobump claims a breaking change with
structural typing disabled are all due to author mistakes. For
example, in the patch from \codeinline{1.1.0} to \codeinline{1.2.0},
which should have been feature additions only, a single property of an
exception class was removed along with some legitimate feature
additions. This change is literally one line of code, but is
indisputably breaking. The other 4 are also because of minor
modifications that are nevertheless breaking -- removing a function or constant.

It is important to note that not a single mismatch, breaking or not,
was found for the file \codeinline{api.py}. This file is the main
entrypoint for the library and methods defined it are expected to be
the most frequently used.

Mismatches that were not breaking are generally very similar to the 5
that were. Adding a single new parameter with a default value to a
function, or one or two new methods in a class is frequently
designated as a patch change by the authors, while Autobump claims it
is a feature addition. Strictly speaking Autobump is correct in those
cases as Semver claims that ``minor version Y [...] MUST be
incremented if new, backwards compatible functionality is introduced
to the public API''. Not adhering to this rule is not as bad as
introducing breaking changes without bumping the major version, but is
still a deviation from semantic versioning.

In a minority of cases Autobump proposes a patch change when the
authors have claimed a feature addition. Examining those cases
manually it can be seen that all of them are because of genuine
feature additions or improvements that do not impact the public API,
for example optimizing a piece of code. Semver specifies that ``[the
minor version] MAY be incremented if substantial new functionality or
improvements are introduced within the private code''. In that case
both the authors' and Autobump's suggestions are correct.

An interesting trend emerges when the release dates of different
versions are plotted against the number of mismatches accumulated thus
far (fig. \ref{RequestsCumulativeMismatches}). It seems that releases
that are done soon after the previous one are much less likely to
introduce an unnoted breaking change as opposed to releases that have
a larger time interval between them. This is better illustrated by a
histogram showing the total number of versions released in a time
interval (measured in days) compared to the ones which should have
been marked as breaking (fig. \ref{RequestsHistogram}). The time
intervals that are responsible for the majority of omitted major
version bumps are in the triple digits.

\begin{figure}[]
\centering
\includegraphics[height=0.4\textheight]{images/evaluation/requests_cumulative_mismatches}
\caption{Cumulative mismatches over time (\textit{requests})}
\label{RequestsCumulativeMismatches}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[height=0.4\textheight]{images/evaluation/requests_introduced_changes}
\caption{Histogram of total versions vs. breaking mismatches in a time
interval (\textit{requests})}
\label{RequestsHistogram}
\end{figure}

\subsubsection{simplejson}

\textit{simplejson} is a JSON parsing library which has the highest
number of downloads on PyPI across all projects. It is also bundled
with a standard CPython installation as part of the standard library.
Version \codeinline{3.0.0} is the lower bound for evaluation as that
is first one that supports Python 3. The upper bound is
\codeinline{3.10.0}, the latest one at the time of writing.

The reported number of mismatches with and without structural typing
is identical (tables \ref{SimplejsonNonStructural} and
\ref{SimplejsonStructural}). Evidently the method set used on a
parameter changes, either the authors consider some aspect of that
worthy of a version bump, or it happened to coincide with another
change that warrants the same bump. Given that only 39 versions were
evaluated, it is not unreasonable to say that the latter is possible.

\noindent
\begin{minipage}[t]{0.5\textwidth}
\begin{table}[H]
\centering
\begin{tabular}{|lr|}
All versions & 39 \\
Mismatches & 6 \\
Mismatches (breaking) & 1 \\
Proportion of mismatches & 15\% \\
Proportion of mismatches (breaking) & 2\%
\end{tabular}
\caption{\textit{simplejson} statistics (w/o structural typing)}
\label{SimplejsonNonStructural}
\end{table}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{table}[H]
\centering
\begin{tabular}{|lr|}
All versions & 39 \\
Mismatches & 6 \\
Mismatches (breaking) & 1 \\
Proportion of mismatches & 15\% \\
Proportion of mismatches (breaking) & 2\%
\end{tabular}
\caption{\textit{simplejson} statistics (w/ structural typing)}
\label{SimplejsonStructural}
\end{table}
\end{minipage}

The sole unnoted breaking change is at the patch from
\codeinline{3.0.9} to \codeinline{3.1.0}. One exceptions and two
functions were moved from one module to another. That is noted in the
changelog, but evidently the authors did not consider it significant
enough to warrant a major version bump. However, \textit{simplejson}
never officially claims that it follows semantic versioning.

The remaining 5 mismatches are very similar to what was observed with
\textit{requests}. Very small additive changes to the API of some
functions was deemed a patch change, when Semver claims it should be a
feature, and improvements that don't impact the API bumped the minor
version (which is a valid interpretation).

Compared to \textit{requests}, the time between releases does not seem
to impact breaking changes (noted or unnoted) at all. Comparable
numbers of versions have been released at different intervals, but
only a single one resulted in an unidentified breaking change.
Moreover, that version was released only a few days after the previous
one (see fig. \ref{SimplejsonHistogram}).

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth,height=0.25\textheight]{images/evaluation/simplejson_introduced_changes}
\caption{Histogram of total versions vs. breaking mismatches in a time
interval (\textit{simplejson})}
\label{SimplejsonHistogram}
\end{figure}

\subsubsection{docopt}

\textit{docopt} is ``Pythonic argument parser'' for creating
command-line user interfaces. It is different from all other evaluated
projects because at the time of writing it does not yet have a
\codeinline{1.0.0} release. The API is not stable according to Semver,
so this project was mainly used to evaluate Autobump's handling of
projects with major version \codeinline{0}.

Only one mismatch is found when not using structural typing. A
function called \codeinline{parse_args} was renamed to
\codeinline{parse_argv} in version \codeinline{0.4.2}. Autobump
claimed that the version shoud have been \codeinline{0.5.0} because
that change was seen as breaking, but suppressed to only a minor bump
because the API is not yet stable.

\noindent
\begin{minipage}[t]{0.5\textwidth}
\begin{table}[H]
\centering
\begin{tabular}{|lr|}
All versions & 11 \\
Mismatches & 1 \\
Proportion of mismatches & 9\% \\
\end{tabular}
\caption{\textit{docopt} statistics (w/o structural typing)}
\label{DocoptNonStructural}
\end{table}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{table}[H]
\centering
\begin{tabular}{|lr|}
All versions & 11 \\
Mismatches & 2 \\
Proportion of mismatches & 18\% \\
\end{tabular}
\caption{\textit{docopt} statistics (w/ structural typing)}
\label{DocoptStructural}
\end{table}
\end{minipage}

The one additional change found with structural typing enabled is
because of an additional method \codeinline{remove} being called on a
parameter. The surrounding usage implies that the argument is a list
and it is reasonable to expect that it would provide a
\codeinline{remove} method. Similarly to some cases in
\textit{requests}, Autobump is being overzealous.

\subsubsection{hashids}

\textit{hashids} is a very small (one class with four methods) library
for generating unique string identifiers from integers. It is a fairly
recent projects, so all 10 released versions support Python 3 and were
evaluated. 4 out of them have major version \codeinline{0} similarly
to \textit{docopt}, whereas the 5\textsuperscript{th} one is
\codeinline{1.0.0}. This is useful to check whether Autobump handles
the boundary between unstable and stable API sensibly.

Similarly to \textit{simplejson} there is no difference in the
mismatches with structural typing enabled and disabled.

\noindent
\begin{minipage}[t]{0.5\textwidth}
\begin{table}[H]
\centering
\begin{tabular}{|lr|}
All versions & 10 \\
Mismatches & 4 \\
Mismatches (breaking) & 1 \\
Proportion of mismatches & 40\% \\
Proportion of mismatches (breaking) & 10\% \\
\end{tabular}
\caption{\textit{hashids} statistics (w/o structural typing)}
\label{HashidsNonStructural}
\end{table}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{table}[H]
\centering
\begin{tabular}{|lr|}
All versions & 10 \\
Mismatches & 4 \\
Mismatches (breaking) & 1 \\
Proportion of mismatches & 40\% \\
Proportion of mismatches (breaking) & 10\% \\
\end{tabular}
\caption{\textit{hashids} statistics (w/ structural typing)}
\label{HashidsStructural}
\end{table}
\end{minipage}

One of the mismatches stems from a simple mistake -- a version was
omitted. In the release history \codeinline{0.8.3} follows
\codeinline{0.8.1}. Autobump correctly suggests that
\codeinline{0.8.2} should have been the appropriate number.

Version \codeinline{0.8.4} is the last version with an unstable API,
the following one being \codeinline{1.0.0}. At that point Autobump
suggests that it should have been \codeinline{0.9.0}, reluctant to
bump the major version making the API official. This is valid
behaviour and this is definitely a case when Autobump's suggestion can
be discarded because it is up to the authors to decide when to publish
an API.

The other non-breaking mismatch comes from a performance optimization
done in \codeinline{1.2.0} where Autobump suggests it should have been
\codeinline{1.1.1}, a patch change. This is an identical situation to
ones found in the previous libraries.

The only unnoted breaking change is renaming the methods
\codeinline{encrypt} and \codeinline{decrypt} to \codeinline{encode}
and \codeinline{decode} respectively. Those methods provide the core
functionality of the library, yet the authors deemed that (rather
dangerously) a patch change.

\subsection{Java}

\begin{table}[H]
\label{JavaProjectsForEvaluation}
\centering
\begin{tabular}{|l|l|l|p{10cm}|}
\hline
\textbf{Project} & \textbf{Size} & \textbf{Dependants} & \textbf{Semantically versioned} \\
\hline
\codeinline{guava} & 150 000 LOC & 10700 & A contributor remarks ``we
actually do use semantic versioning with
Guava''\footnote{https://github.com/google/guava/issues/1682}. \\
\codeinline{mockito} & 11000 LOC & 8415 & \codeinline{CONTRIBUTING}
claims that ``semantic versioning is rigorously maintained'', but is
then implied that this applies only for after major version
\codeinline{2}. \\
\codeinline{gson} & 8000 LOC & 3400 & Seems like it, not officially. \\
\hline
\end{tabular}
\caption{Java projects chosen for evaluation}
\end{table}

Evaluation of Java projects using the native handler proved to be difficult.
Two out the three selected projects happened to have quite a long
history during which they have changed their build system several
times. This makes it impossible to specify a single build command and
evaluate the entire repository in one run. Instead, evaluations had to
be done one range at a time, and then the results merged together.

Furthermore, early versions of some projects (most notably Guava)
require ancient versions of Java. Given the impracticality of
maintaining several versions of the JDK, along with respective
versions of build systems and so on, those versions were just skipped
when using the native handler.

A final hurdle was that compiling complex Java projects like these
ones is slow -- when running the \codeinline{java_native} handler it
was noticeably an order of magnitude slower than \codeinline{java_ast}.

\subsubsection{guava}

\textit{Guava} \cite{Guava} is a set of core libraries that extend the
Java standard library. It is one of the most widely used Java
projects.

\subsubsection{mockito}

\textit{Mockito} \cite{Mockito} is a mocking library. It is one of few
libraries that outright say they follow Semver, although that only
applies after version \codeinline{2.0.0}.

\subsubsection{gson}

\subsection{Clojure}

\begin{table}[H]
\label{ClojureProjectsForEvaluation}
\centering
\begin{tabular}{|l|l|l|p{10cm}|}
\hline
\textbf{Project} & \textbf{Size} & \textbf{Dependants} & \textbf{Semantically versioned} \\
\hline
\codeinline{cheshire} & 1200 LOC & 961 & Seems like it, not
officially. \\
\codeinline{nippy} & 2200 LOC & 105 & Seems like it, not officially. \\
\hline
% \\
% TODO: another Clojure library
\end{tabular}
\caption{Clojure projects chosen for evaluation}
\end{table}

\subsubsection{cheshire}

\textit{cheshire} is a ``fast JSON encoder and decoder''. It is a
relatively mature Clojure project, being at major version
\codeinline{5} at the time of writing and being one of the most
popular Clojure libraries. Although type hinting is optional, it is
used for almost every function in the library.

The very first pair of versions was ignored during evaluation, because
the first version \codeinline{1.1.0} has misconfigured paths and the
namespaces it defines are inaccessible, not even when working from a
manually created REPL.

\begin{table}[H]
\centering
\begin{tabular}{|lr|}
All versions & 38 \\
Dropped versions & 1 \\
Mismatches & 19 \\
Mismatches (breaking) & 7 \\
Proportion of mismatches & 51\% \\
Proportion of mismatches (breaking) & 18\% \\
\end{tabular}
\caption{\textit{cheshire} statistics}
\label{CheshireStats}
\end{table}

There seems to be a clear indication that the project has stabilised
over time (fig. \ref{CheshireCumulativeMismatches}), seeing frequent,
unnoted in the version number, breaking changes for the first 2 years
of development, and then not for the next 4 years. It's noteworthy
that this stabilisation happened only at major version \codeinline{5}
and that the frequency of releases has gone down since then. There
were 3.96 releases per 100 days in that 2-year interval, compared to
only 1.70 for the entirety of the project's history.

Breaking mismatches are evenly divided between two causes:

\begin{enumerate}[(i)]
\item Refactoring one function into several other, similarly-named
functions. All of these changes bump the minor version number, even
though the original function is removed. For example, version
\codeinline{2.1.0} refactors \clojureinline{factory} into
\clojureinline{make-json-factory} and
\clojureinline{make-smile-factory}.
\item Introducing type hints where they were previously none, or
changing them to an incompatible type.
\end{enumerate}

This project also shows a large number of non-breaking mismatches,
accounting for about 30\%. The vast majority of those are because of
one or two small (public) functions being introduced and only bumping
the patch version number. Only two (\codeinline{5.7.0} and
\codeinline{5.5.1}) are because of making legitimate improvements that
don't impact the API.

\begin{figure}[H]
\centering
\includegraphics[height=0.4\textheight]{images/evaluation/cheshire_cumulative_mismatches}
\caption{Cumulative mismatches over time (\textit{cheshire})}
\label{CheshireCumulativeMismatches}
\end{figure}

\subsubsection{nippy}

\textit{nippy} is a ``high-performance serialization library''. It has
70 tagged releases, but nearly half of them carry a label after the
version number. For example version \codeinline{v2.5.0} has 6 variants
-- the final one, two release candidates and three betas. Only the
actual final version is compared to its successor in evaluation, which
brings down the number of releases to 32.

\begin{table}[]
\centering
\begin{tabular}{|lr|}
All versions & 32 \\
Mismatches & 14 \\
Mismatches (breaking) & 6 \\
Proportion of mismatches & 43\% \\
Proportion of mismatches (breaking) & 18\% \\
\end{tabular}
\caption{\textit{nippy} statistics}
\label{NippyStats}
\end{table}

\textit{nippy} shows a trend of stabilisation similar to
\textit{cheshire} (fig. \ref{NippyCumulativeMismatches}). For a period
of about 2 years between versions \codeinline{1.2.0} and
\codeinline{2.7.0} there is a large number unnoted breaking changes,
which then plateaus. The frequency of releases in that interval is
also larger than the one for the entirety of the project: 5.60 per 100
days, compared to only 3.74 per 100 days.

\begin{figure}[H]
\centering
\includegraphics[height=0.4\textheight]{images/evaluation/nippy_cumulative_mismatches}
\caption{Cumulative mismatches over time (\textit{nippy})}
\label{NippyCumulativeMismatches}
\end{figure}

All 6 breaking mismatches see at most 2 entities being removed or
renamed. In one case renaming happens not because of refactoring, but
because of a typo: \codeinline{2.6.1} corrects the function
\clojureinline{freezeable?} to \clojureinline{freezable?} which was
introduced in the previous version \codeinline{2.6.0}. Fixing a typo
was considered a patch-level change by the authors, even though that
typo was at that point part of the public API.

The distribution and reasoning behind non-breaking mismatches is
nearly identical to what was observed in \textit{cheshire} -- nearly
all such changes are because of a small number of public functions
being introduced and not bumping the minor version number.

\subsection{Summary}

% TODO: histogram of number of releases, number of breaking
% TODO: cumulative mismatches/breaking changes over time

\section{Automated evaluation}

In addition to the previous round, a large scale evaluation was
attempted on 632 Python projects. It was almost completely automated,
with scripts responsible for each step -- from suggesting projects, to
gathering metrics.

\subsection{Methodology}

The number 632 is not arbitrary. To gather Python projects with
varying popularity, the websites \textit{http://pypi-ranking.info} and
\textit{http://pypi.org} were scraped (see listing
\ref{ScraperScript}). The first one was used to get a listing of all
published projects in decreasing order of popularity and check for
Python 3 compatibility, and the second one was used to visit the
official page of each project and search via a regular expression for
a GitHub\footnote{Earlier iterations of the scraper script tried to
use a more general regular expression, but that proved to be very
erroneous, so it was limited to the most popular hosting for
repositories.} repository. As projects get less popular, their support
for Python 3 decreases rapidly, as they have not seen a release in
some time. After reaching zero (number of downloads) the ones that
support it get extremely sparse, so the scraping was cut off soon
after that point. That is not to say that there are only 632 Python
projects that support Python 3 and have a download count larger than
zero -- these are only the ones for which a GitHub link was found
reliably on the module's PyPI page.

Note that not all of these projects are guaranteed to be libraries;
there is no good way to filter out end user applications. Instead, we
just assume the modules the project defines can be imported and used
by other applications (this is technically true for all Python
projects). Although Autobump itself is an end-user application and not
a library, it still adheres to Python naming conventions to
distinguish public from private code. If all projects do the same then
there will be no difference whether they are applications or
libraries.

After the list of repositories was obtained, each one was cloned in a
separate directory (listing \ref{CloneScript}). Of 632 repositories,
605 were cloned successfully, the rest failing because they were
private or removed.

A generic configuration file (\ref{GenericConfigFile}) was constructed
that ignores frequently encountered directories and files such as
\codeinline{tests} and \codeinline{setup.py}. After that, all projects
were evaluated from their very first release to the very last, all
using the same configuration file.

There is an obvious problem with Python 3 compatibility -- although
PyPI can rightfully claim that a project supports Python 3 and it ends
up in this list of 632 repositories, it is not guaranteed that all
versions support Python 3. There is no good automatic way of
determining the first version that does so. Initially, a heuristic was
used. When the Python handler encounters a file it cannot parse, it
either warns the user or outright fails with an error depending on how
it is configured. The generic configuration file directs Autobump to
steamroll through the entire projects only warning about unparseable
files. Unparseable files most likely indicate Python 2 code, so the
first version that doesn't raise such a warning is taken to be the
first version as far as evaluation is concerned.

However, this approach led to too many versions, sometimes entire
projects, being dropped. It is not uncommon for projects to have both
Python 2 and Python 3 code in their repository. Instead, while
unparseable files are still omitted, Autobump still considers others.
This potentially leads to introducing false negatives, but the impact
of that on the final data ought to be smaller than dropping too many
versions -- library code written in Python 2 is frequently compatible
with 3 without any modifications. User-facing parts of the projects
using a \pythoninline{print} statements without parantheses are more
likely to fail.

\subsection{Results}

Out of the 605 successfully cloned projects, only 403 were
successfully evaluated. The remaining were discounted because they had
less than 2 released versions\footnote{This means less than 2 tags in
the repository. It is entirely possible that the project has releases
which are not tagged, but the script can't find that out.}, so no
adjacent pairs of versions could be compared.

To get some context about the state of the projects prior to any
evaluation, figure \ref{DistributionAllVersions} shows the
distribution of the number of released versions. It is clear that
nearly all projects have less than 50 versions, with the majority
having less than 10.

\begin{figure}[]
\centering
\includegraphics[height=0.4\textheight]{images/evaluation/distribution_all_versions}
\caption{Distribution of the number of releases}
\label{DistributionAllVersions}
\end{figure}

Moreover, looking at the largest major version ever found in the projects
(figure \ref{DistributionLargestMajor}) we can see that the data has a
disproportionate number of projects that have never seen a
\textit{1.0.0} release.

\begin{figure}[]
\centering
\includegraphics[height=0.4\textheight]{images/evaluation/distribution_major_versions}
\caption{Distribution of largest major versions}
\label{DistributionLargestMajor}
\end{figure}

Having this information in mind, three data sets were constructed to
avoid skewing the statistical results because of outliers: one
comprising all 403 projects, a second one containing only projects
which reach version \textit{1.0.0} and a third one with projects that
have at least 10 released versions. This partioning is shown on fig.
\ref{Datasets}, while table \ref{DatasetStatistics} present some
statistics for each data set.

\begin{figure}[H]
\centering
\includegraphics[height=0.3\textheight]{images/evaluation/datasets}
\caption{Data sets}
\label{Datasets}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
& \textbf{All projects} & \textbf{Published API} & \textbf{At least 10 releases} \\
\hline
Total projects & 403 & 172 & 185 \\
\hline
Smallest number of releases & 2 & 2 & 10 \\
\hline
Largest number of releases & 203 & 203 & 203 \\
\hline
Min. \% of all mismatches & 0\% & 0\% & 0\% \\
\hline
Max. \% of all mismatches & 88.8\% & 88.8\% & 88.2\% \\
\hline
Projects with the minimal number of total mismatches & 42 & 13 & 1 \\
\hline
Min. \% of breaking mismatches & 0\% & 0\% & 0\% \\
\hline
Max. \% of breaking mismatches & 75\% & 75\% & 70.6\% \\
\hline
Projects with the minimal number of breaking mismatches & 284 & 53 & 98 \\
\hline
\textbf{Avg. \% of all mismatches} & 40\% & 46.3\% & 44.7\% \\
\hline
\textbf{Avg. \% of breaking mismatches} & 6.8\% & 15.9\% & 9.3\% \\
\hline
\end{tabular}
\caption{Statistics for each data set}
\label{DatasetStatistics}
\end{table}

\chapter{Conclusion}
\label{Conclusion}

% TODO: handler for binaries using debugging information, supports
% multiple languages

% TODO: better automated versioning scheme

\begin{appendices}

\chapter{Running Autobump in practice}
\label{AutobumpInPractice}

Autobump requires at least Python 3.5. The easiest way to run it is to
install the published version from PyPI by running \codeinline{pip
install autobump}. This is going to pull in all necessary dependencies
and check whether the version of Python is appropriate. Autobump
should then be available under the executable in \codeinline{PATH}
named \codeinline{autobump}.

Alternatively, Autobump can be used from within a virtual environment
that has all dependencies preinstalled (see
\codeinline{requirements.txt}) by setting the \codeinline{PYTHONPATH}
environment variable appropriately. An executable won't be available
because \codeinline{setuptools} takes care of creating it when making
the distribution package, but Autobump can be called as a module by
running \codeinline{python -m autobump}. This approach is not recommended.

\section{Rundown of options}
\label{RundownOptions}

\section{Rundown of configurables}
\label{RundownConfigurables}

\section{Task management}

Autobump's source code comes with a regular \textit{GNU Make} Makefile
that is used for task management. The different targets are:

% TODO: fullstops after items
\begin{itemize}
\item \codeinline{lint}: runs a linter against all source code (except
for tests) and fails the target if there are formatting issues
\item \codeinline{unit_test, acceptance_test, test}: run the unit
tests suite, the acceptance test suite or both respectively
\item \codeinline{.coverage.unit, .coverage.acceptance, .coverage}:
run the appropriate tests and generate \textit{Coverage.py}
\cite{Coverage} test coverage data
\item \codeinline{libexec}: compiles non-Python code, e.g. the Java handler
\item \codeinline{dist}: creates a distribution package, suitable for
publishing or local installation
\item \codeinline{install}: installs the distribution package locally
\end{itemize}

\chapter{Primer on Clojure}
\label{ClojurePrimer}

This appendix gives a simplified view of the language and briefly
covers aspects of it that relate to Autobump. The goal is to give a
reader unfamiliar with Clojure enough knowledge to make sense of
subsection \ref{Clojure}.

Clojure is a functional, dynamically typed language from the Lisp
family of languages that runs on top of the Java Virtual Machine. It
has a small set of immutable data structures that are reused
throughout the standard and external libraries instead of defining
ad-hoc ones (``It is better to have 100 functions operate on one data
structure than to have 10 functions operate on 10 data
structures.''\cite{ClojureRationale}).

\subsubsection{Data structures}

The most frequently used data structures are lists, vectors, maps and
sets:

\begin{center}
\begin{tabular}{c}
\begin{clojure}
(1 2 3 4)    ;; list
[1 2 3 4]    ;; vector
{:a 1 :b 2}  ;; map
#{1 2 3 4}   ;; set
\end{clojure}
\end{tabular}
\end{center}

Programs written in Clojure are themselves expressed in those data
structures, mostly lists and vectors. The source code of a Clojure
program is nearly identical to its abstract syntax tree, with the
parser having to do very little work. Every node in the AST is denoted
by an \textit{S-expression}, which is classically defined as being either an
atom, or an ordered pair of two other S-expressions. S-expressions are
also known as \textit{forms}.

\subsubsection{Functions and macros}

Function calling is done by constructing a list where the first
element is the function and the rest are arguments (i.e. prefix
notation):

\begin{center}
\begin{tabular}{c}
\begin{clojure}
(+ 1 2)      ;; => 3
\end{clojure}
\end{tabular}
\end{center}

Functions are defined by invoking the macro \clojureinline{defn},
where the first argument is the name of function, the second is a
vector comprising the arguments and the rest are the body of the
function. Macro forms, when evaluated, produce other forms.

\begin{center}
\begin{tabular}{c}
\begin{clojure}[emph=add]
(defn add [a b]
  (+ a b))
\end{clojure}
\end{tabular}
\end{center}

Functions can have multiple signatures by wrapping argument-body pairs
in lists:

\begin{center}
\begin{tabular}{c}
\begin{clojure}[emph=add]
(defn add
  ([a b]
   (+ a b))
  ([a b c]
   (+ a b c)))
\end{clojure}
\end{tabular}
\end{center}

\subsubsection{Special forms}

Special forms are built-in baseline forms. The special form
\clojureinlinedef is used to define symbols that
match to values:

\begin{center}
\begin{tabular}{c}
\begin{clojure}[emph=pi,otherkeywords=def]
(def pi 3.14)
\end{clojure}
\end{tabular}
\end{center}

In fact, the \clojureinline{defn} macro expands as such:

\begin{center}
\begin{tabular}{c}
\begin{clojure}[emph=add]
(macroexpand '(defn add [a b] (+ a b)))

;; => (def add (fn ([a b] (+ a b))))
\end{clojure}
\end{tabular}
\end{center}

Evaluating a list can be avoided by prefixing it with
\clojureinline{'} as done in the above example. This is shorthand for
the special form \clojureinline{quote}. \clojureinline{'(a b c)} and
\clojureinline{(quote (a b c))} are equivalent. Quoting a list returns
it literally.

\subsubsection{Symbol metadata and type hinting}

Defined symbols in Clojure can have metadata attached to them. This
feature is most frequently used for docstrings, type hinting or hiding
implementation details. Metadata is handled by the Lisp reader
(parser) which converts the textual representation of objects into the
internal one.

\begin{center}
\begin{tabular}{c}
\begin{clojure}[otherkeywords=def]
(defn add [^Integer a ^Integer b] ;; Both parameters are hinted to be integers.
  (+ a b))

(def ^{:private true} pi 3.14)    ;; Field is now private.
\end{clojure}
\end{tabular}
\end{center}

To make functions private, there is a built-in macro
\clojureinline{defn-}. It expands to exactly the same form as
\clojureinline{defn} but sets the \clojureinline{:private} tag to be
\clojureinline{true} just like defining a private field.

\subsubsection{Namespaces}

Clojure code is divided into namespaces, similarly to Java packages.
Namespace declarations are put at the top of the file.

\begin{center}
\begin{tabular}{c}
\begin{clojure}[otherkeywords=ns]
(ns lib.core)
\end{clojure}
\end{tabular}
\end{center}

The same special form can be used to make other namespaces available
in the current one, similarly to Java's \javainline{import} statement.

\begin{center}
\begin{tabular}{c}
\begin{clojure}[otherkeywords=ns]
(ns lib.core
  (:require [clojure.test :as test]
            [lib.other :refer [function1 function2]]))
\end{clojure}
\end{tabular}
\end{center}

% TODO: records, protocols

\subsubsection{Optional static typing}

\end{appendices}

%% Bibliography
\bibliographystyle{ieeetr}
\bibliography{dissertation}

\end{document}

% The below list of file-local variables enables
% auto filling of text and completely nukes all indentation
% except for code blocks.

% Local Variables:
% eval: (auto-fill-mode 1)
% sentence-end-double-space: nil
% LaTeX-indent-environment-list: (("verbatim" current-indentation) ("lstlisting" current-indentation))
% TeX-brace-indent-level: 0
% LaTeX-indent-level: 0
% LaTeX-item-indent: 0
% tex-indent-basic: 0
% tex-indent-item: 0
% tex-indent-arg: 0
% eval: (highlight-regexp "TODO")
% End:
